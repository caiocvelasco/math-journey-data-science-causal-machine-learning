---
title: "Why I'm Interested in Causal ML?"
---

My interest in Causal Machine Learning arises from a long-standing divide in the way different worlds think about data.  

## The 2 Worlds

In his influential essay *Statistical Modeling: The Two Cultures (2001)*, **Leo Breiman** described two very different traditions. 

* **Data Modeling** culture (the dominant one in Statistics at the time): assumes data are generated by a stochastic model with explicit assumptions (e.g., linear regression, logistic regression). The focus is on inference and interpretability.
* **Algorithmic Modeling** culture (rising in Machine Learning): does not assume a specific stochastic DGP, but uses flexible algorithms (e.g., random forests, neural networks) to directly optimize predictive accuracy.

### The Statistics (and Econometrics) World  

Did you know that when you run a regression, you are implicitly making a very strong claim about how the world works?  

In this tradition, everything begins with the **data-generating process (DGP)** — what **Aris Spanos** in his book *Probability Theory and Statistical Inference: Econometric Modeling with Observational Data* calls a *stochastic mechanism*. Imagine it as a hidden machine tossing probabilistic dice, producing the data we observe. A statistical model is not just a tool; it is a **story about that hidden mechanism** that is producing the outcomes we observe.

Examples:  
- A linear regression assumes wages, test scores, or prices are generated as a straight-line combination of inputs plus random noise.  
- A Poisson model assumes counts come from a specific probabilistic law.  
- A time-series AR(1) model assumes today’s value depends on yesterday’s value with some random shock added in.  

If the **DGP** is hidden, what do you think we want to do? **To uncover its truth**.  

That is why introductory statistics courses begin with the notion of a **Population** and then introduce **Samples**. Since we cannot observe the entire population, we study samples and rely on probabilistic laws — the Law of Large Numbers (LLN), the Central Limit Theorem (CLT) — to argue that *repeated samples will “stabilize” around the truth* about some population parameters (mean, median, variance…).  

From there comes **Inference**:   
> If I trust my assumptions about the DGP, then I can use my sample to make **representative** claims about the population.  

This is the heart of statistics and econometrics: *model* the **hidden process**, *estimate* its parameters, and *test* whether your story holds up against data. But — and this is crucial — *the DGP is always an assumption*. If it is wrong, the inference may be misleading.  

### The Machine Learning World  

Now comes the other world. The **algorithmic modeling culture**. Here, **the hidden machine is ignored**. No one asks whether wages are “truly linear” in education, or whether prices follow a Poisson law, or whether the dice is probabilistic. Instead, the question is brutally pragmatic: *Can I predict well?*  

In this world, the central drama is not about whether the stochastic story is correct, but about whether *a model trained on one dataset* (*training set*) **generalizes** to unseen data (*the test set*). This is the **training vs. testing problem**.  

The classical core risks are:  
- **Underfitting**: the model is too simple, missing patterns (high bias).  
- **Overfitting**: the model is too complex, capturing noise instead of signal (high variance).  

Success here is measured by **predictive accuracy**, not by **unbiasedness** or **efficiency** of estimators, like in the Statistics world. Interpretability is optional (if you don’t know what is inside a black box, you cannot “read” it and “interpret” it); performance is everything. Predictions may work well in stable environments but fail when the underlying data distribution shifts.  

---

A concrete example makes the distinction clearer — and shows how each culture fails where the other succeeds:  

- **Estimating the causal effect of education on wages.**  
  In econometrics and statistics, I would model wages as generated by a stochastic mechanism involving education, experience, and other covariates. My goal is to isolate the effect of education from confounders. *If I only used a machine learning model, it might predict wages very well — but it would not tell me the causal role of education* (maybe it is not education itself that raises wages, but family wealth that enables both schooling and better jobs), because prediction does not separate correlation from causation. ML excels at *prediction*, but fails to deliver interpretability and identification here.  

- **Predicting house prices.**  
  Econometrics might specify a linear model with a few chosen variables and assumptions about errors. But this DGP is likely too simplistic: house prices depend on thousands of complex, nonlinear factors (location, amenities, school ratings, neighborhood trends). *An econometric model here may fail badly in predictive accuracy*. By contrast, ML models like gradient boosting can capture these nonlinearities and achieve much higher predictive power — even though they may not explain *why* prices are high.  

So each culture shines in one domain and struggles in the other. Together, they suggest the need for a synthesis.  

> **Causal ML in Action**  
> A famous example comes from **Online Advertising**.  
> - A standard supervised ML model might predict that showing more ads *increases* purchases, because ads and purchases are positively correlated in the data.  
> - But when economists ran randomized experiments, they found the causal effect was often much smaller — sometimes **zero**. Why? Because people who were already likely to buy were also the ones most likely to be targeted with ads.  
>  
> This is a case where prediction looked strong, but causality revealed the true relationship and new decisions had to be made.  

---

### Why Causal ML Is a Synthesis  

✨ This is why Causal Machine Learning excites me.  

By combining machine learning with the discipline of causal inference, researchers such as *Victor Chernozhukov, Christian Hansen, Nathan Kallus, Martin Spindler, and Vasilis Syrgkanis* — co-authors of *Causal ML: Applied Causal Inference Powered by ML and AI* — show that we can take the best of both worlds.  

The basic idea is simple:  
- From **machine learning**, we borrow flexible algorithms that can handle many variables and complex patterns.  
- From **statistics and econometrics**, we borrow tools that check whether we are uncovering **cause-and-effect** rather than just correlations.  

Techniques like **sample splitting** and **cross-fitting** might sound technical, but the intuition is clear: we let the algorithm “learn” patterns on one part of the data, and we **keep another part untouched** to see whether those patterns still hold when drawing **causal** conclusions. This helps us avoid fooling ourselves with spurious results.  

Causal thinking also pushes us beyond the tidy assumption that all observations are **IID** (independent and identically distributed). Real life changes: policies shift, markets move, and people adapt. Causal inference gives us language to reason about such changes — about **interventions** (“what if we changed X?”), **counterfactuals** (“what would have happened otherwise?”), and **distribution shifts** (“what if tomorrow doesn’t look like yesterday?”).  

**Causal ML suggests a synthesis**:  
- From statistics and econometrics, we inherit the concern with stochastic mechanisms, identification, and rigorous inference.  
- From machine learning, we gain flexibility, robustness, and predictive accuracy.  
- From causality, we add the language to generalize beyond observation, to interventions and explanations.  

## Focus

My main focus in this project is to **learn — and become comfortable with — the mathematical foundations that underpin Causal Machine Learning**.

While Causal ML is an exciting and active field (and may well be my future research direction), the broader point is that **mathematical depth makes a difference**. Too often, methods are applied skillfully but without a full grasp of what is happening underneath.

By working through the logical and mathematical foundations step by step, **I aim to develop the clarity that allows not only the application of tools, but a genuine understanding of them**. The goal is to bridge the cultures of explanation and prediction — and, in some small way, contribute to the ongoing effort to unify them.

**What “foundations” means here**  
- **Logic & Set Theory** — the language of proofs and structures.  
- **Real Analysis** — rigor of limits, continuity, convergence, integration.  
- **Linear Algebra** — vector spaces, eigen-structure, decompositions.  
- **Functional Analysis & Hilbert Spaces** — norms, projections, RKHS.  
- **Topology & Measure Theory** — σ-algebras, Lebesgue integration, convergence theorems.  
- **Probability** — Kolmogorov framework, LLN/CLT, conditional expectation.  
- **Mathematical Statistics** — estimation, tests, asymptotics.  
- **Causality** — SCMs, potential outcomes, identifiability, modern Causal ML.


 
