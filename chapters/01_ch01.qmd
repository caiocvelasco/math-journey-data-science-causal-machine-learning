---
title: "Phase 1: Logic & Set Theory"
---

# 0. Mathematics, Axioms, and Science {.unnumbered}

Mathematics begins with **axioms** â€” *assumptions we agree to accept without proof* â€” and builds everything else from them using precise rules of logic.  
This process of **axiomatization** is what gives mathematics its unique clarity: starting from simple, explicit principles, we construct entire theories.  

Why is this important?  
- Axioms are not â€œtrueâ€ in an absolute sense. They are starting points.  
- What matters is whether the consequences drawn from them are **logically consistent** and **useful** for understanding the world.  
- In practice, this makes mathematics a *language* in which science can express and test ideas.  

Mathematics provides the rigorous framework for turning assumptions into predictions. 

For example, in **Euclidean geometry** we assume as an **axiom**:  

> *Through two distinct points, there is exactly one straight line.*  

But this depends on the â€œspaceâ€ we are working in.  

- On a flat plane (Euclidean space), the axiom works perfectly: two points uniquely determine a straight line.  
- On the surface of a sphere (non-Euclidean geometry), the analogue of a â€œstraight lineâ€ is a **geodesic** â€” the shortest path between two points on the surface.  
  On a sphere, geodesics turn out to be **arcs of great circles** (circles centered at the sphereâ€™s center, like the equator or any meridian).  

Intuitively: 
- if you fly from *New York* to *Paris*, the shortest route is not a line of latitude on a flat map, but rather a curved arc of a great circle â€” airplanes follow these routes because they minimize distance. 
- the airplane could in principle fly in a perfectly straight line â€” but only if it were allowed to go through the Earthâ€™s interior. Since itâ€™s constrained to stay on the surface of the sphere, the â€œstraight lineâ€ within that space becomes a geodesic (the great circle).

Now consider two points on the equator that are exactly opposite each other (say *Rio de Janeiro* and *Jakarta*).  
- There isnâ€™t a **unique** great circle through them â€” in fact, infinitely many great circles (all meridians) pass through both points.  
- So the Euclidean axiom **fails** in this curved space.  

The key idea is that â€œstraight lineâ€ really means **shortest possible path** given the geometry of the space.  

On a plane, thatâ€™s the familiar straight line; on a sphere, itâ€™s a great circle.  
The notion of a geodesic generalizes this idea: whatever the space, it tells us the natural way to connect two points as efficiently as possible.

This illustrates a key point: 

> axioms are not *â€œuniversally trueâ€* in some absolute sense.  

They are **conventions** that **define the system we are working in**.  
Within their proper context (e.g., Euclidean geometry), their consequences are **logically consistent** (no contradictions arise) and **useful** (for designing buildings, maps, and bridges). But in other contexts (e.g., spherical geometry), we need a different set of axioms.    

In the same way, in **probability** we may assume:  

> *Outcomes of an experiment are independent and identically distributed (i.i.d.).*  

Real-world data often violates this (dependence in time series, non-identical distributions in heterogeneous populations).  
But within the i.i.d. framework, the consequences are **logically consistent** and extremely **useful**: they give us fundamental results like the **Law of Large Numbers** and the **Central Limit Theorem**.  

The analogy with geometry is this:  
- In flat Euclidean space, the â€œstraight lineâ€ is the natural geodesic.  
- On a sphere, the â€œstraight lineâ€ is reinterpreted as a great circle.  

Similarly, in probability:  
- For many problems, assuming i.i.d. is the **natural starting point** (the â€œstraight lineâ€ of probability).  
- In more complex contexts (time series, causal inference), we need to **redefine the structure** (like choosing geodesics on a sphere) with weaker or different axioms â€” e.g., mixing conditions, stationarity, or potential outcomes.  


**Why we begin with Logic and Set Theory**  

1. **Logic** gives us the rules of the game:  
   how to combine statements, reason consistently, and structure proofs.  
   Without logic, axioms and theorems would collapse into ambiguity.  

2. **Set Theory** provides the building blocks:  
   almost every object in modern mathematics (numbers, functions, probability spaces, datasets) can be described in terms of sets.  

Together, Logic and Set Theory form the **grammar and vocabulary of mathematics**.  
They are not yet â€œabout the real worldâ€ â€” but they give us the precise tools to state assumptions and derive consequences.  

**Intuition for research**  

When you see theoretical work in physics, economics, or causal machine learning, it often starts by **axiomatizing** the problem:  
- *Define* the objects (e.g. random variables, causal spaces).  
- *Specify* assumptions as axioms (e.g. independence, stability, interventions).  
- *Derive* results rigorously from there.  

By starting here, we are learning how to *speak the language of mathematics* before applying it to inference, probability, and causality.

---

Thus, before diving into analysis and probability, we establish a foundation in logic and set theory. This is important to **formalize assumptions, express mathematical objects precisely, and build proofs with rigor**.

Logic gives us the language **to connect premises and conclusions**, while Set Theory gives us **the structure to define universes of discourse, events, functions, probability spaces, etc**.

Therefore: **Yes, it would be essential to learn Logic and Set Theory!**

# 1. Sentential Logic {.unnumbered}

## 1.1 Statements, Propositions, Predicates, and Connectives

**ðŸ’¡Motivation**\
Learning statements and connectives is like learning the **alphabet of mathematics**.\
- If you cannot distinguish valid statements, you cannot even *start* a proof.

*Example in causal inference:*\
- $p$: â€œTreatment is randomized.â€\
- $q$: â€œIgnorability holds.â€\
- Then â€œIf treatment is randomized, then ignorability holdsâ€ is $p \to q$.

Without connectives, weâ€™d stay in informal language. With them, **we can formalize statements and reason rigorously about consequences**, such as proving that a set of assumptions implies consistency of an estimator, showing that ignorability implies identification of a treatment effect, or demonstrating that conditional independence leads to factorization of a probability distribution into simpler components.

A **Statement** (or **Proposition**) is a declarative sentence that is either *true* or *false*.

-   Example: â€œ3 is evenâ€ (false), â€œBarcelona is in Spainâ€ (true).

A **Predicate** is like a â€œtemplateâ€ for a statement: it depends on a variable and becomes a statement once you specify the value. For example:\
$P(x): x > 0$\
- $P(2)$ â†’ â€œ2 \> 0â€ (true).\
- $P(-1)$ â†’ â€œ-1 \> 0â€ (false).

**Connectives** let us combine assumptions systematically. Logical connectives let us build compound statements:

-   Negation: $\lnot p$ (â€œnot $p$â€)\
-   Conjunction: $p \land q$ (â€œ$p$ and $q$â€)\
-   Disjunction: $p \lor q$ (â€œ$p$ or $q$â€)\
-   Conditional: $p \to q$ (â€œif $p$ then $q$â€)\
-   Biconditional: $p \leftrightarrow q$ (â€œ$p$ if and only if $q$â€)

Thus:\
-   **Predicate**: *general template* (open sentence, truth depends on a variable) -\> becomes true or false only when a variable is given a value.\
-   **Proposition/statement**: *instance of that template* (closed sentence, definite truth) -\> something that is already true or false.\
-   **Connectives**: *operators* that take simple propositions and form compound propositions.

## 1.2 Truth Tables

**ðŸ’¡Motivation** Truth tables are the **grammar checker** of logic.\
- They allow us to test whether two statements are equivalent (so we can swap one for another in a proof, probably to make life easier).\
- They reveal tautologies (always true) and contradictions (always false).\
- They give a mechanical way to check validity of deductive arguments.

*Example in statistics:*\
- â€œIf the data behave nicely (i.i.d. + finite variance), then the sample mean is reliable (it will converge to the true mean)â€ ($p \to q$).\
- Equivalent contrapositive: â€œIf effect is not identifiable, then ignorability does not holdâ€ ($\lnot q \to \lnot p$).\
- Truth tables prove these are the same, so you can flip perspectives safely in a paper.

A **truth table** shows how the truth value of a compound statement depends on its parts.

**Example:** Prove that an implication ($p \to q$) is equivalent ($\equiv$) to its contrapositive ($\lnot q \to \lnot p$).

We want to show: $p \to q \;\equiv\; \lnot q \to \lnot p$

| $p$ | $q$ | $p \to q$ | $\lnot q$ | $\lnot p$ | $\lnot q \to \lnot p$ |
|-----|-----|-----------|-----------|-----------|-----------------------|
| T   | T   | T         | F         | F         | T                     |
| T   | F   | F         | T         | F         | F                     |
| F   | T   | T         | F         | T         | T                     |
| F   | F   | T         | T         | T         | T                     |

Since the last two columns match, the implication is equivalent to its contrapositive.

**The Basic Connectives**

Logical connectives are rules for combining simpler statements into compound ones.  
Here are the five most common and some intuition:

---

**Negation: $\lnot p$ (â€œnot $p$â€)**

- **Rule:** Negation flips the truth value.  
  - If $p$ is true, $\lnot p$ is false.  
  - If $p$ is false, $\lnot p$ is true.  

- **Example:**  
  $p$: â€œIt is raining.â€  
  $\lnot p$: â€œIt is not raining.â€  

---

**Conjunction: $p \land q$ (â€œ$p$ and $q$â€)**

- **Rule:** $p \land q$ is true only if **both** $p$ and $q$ are true.  

- **Example:**  
  $p$: â€œIt is raining.â€  
  $q$: â€œI am carrying an umbrella.â€  
  $p \land q$: â€œIt is raining **and** I am carrying an umbrella.â€  

- **Truth check:** If either part fails, the whole conjunction is false.  

---

**Disjunction: $p \lor q$ (â€œ$p$ or $q$â€)**

- **Rule:** $p \lor q$ is true if **at least one** of $p, q$ is true.  
  (This is the **inclusive or** used in logic.)  

- **Example:**  
  $p$: â€œI will drink coffee.â€  
  $q$: â€œI will drink tea.â€  
  $p \lor q$: â€œI will drink coffee or tea (or both).â€  

- **Note:** In everyday language, â€œorâ€ can be exclusive. Logic defaults to inclusive.  

---

**Conditional: $p \to q$ (â€œif $p$ then $q$â€)**

- **Rule:** An implication is false **only** when $p$ is true and $q$ is false.  
  **In all other cases, it is true.**  
  Equivalent form:  
  $$
  p \to q \;\equiv\; \lnot p \lor q
  $$  

- **Example:**  
  $p$: â€œIt rains.â€  
  $q$: â€œThe ground is wet.â€  
  $p \to q$: â€œIf it rains, then the ground is wet.â€  

- **Case analysis:**\  
  - If it rains **and** the ground is wet â†’ the statement â€œIf it rains, then the ground is wetâ€ has been **kept**. Both the condition and the consequence hold, so the implication is **true**.  
  - If it rains **but** the ground is not wet â†’ the statement has been **broken**. This is the **only case** where an implication is false: the condition was met but the promised result failed.  
  - If it doesnâ€™t rain â†’ the statement never gets a chance to be tested. We cannot accuse it of being false, because the condition (â€œit rainsâ€) never happened. By definition, logic treats this as **vacuously true**: the promise has not been broken, since there was nothing to check.  
  - If it doesnâ€™t rain **and** the ground is wet â†’ still vacuously true. The implication didnâ€™t say what should happen when it doesnâ€™t rain; the ground being wet for other reasons (sprinklers, a bucket of water, etc.) doesnâ€™t violate the promise.

**Intuition and goal of the conditional**

The statement $p \to q$ is read "if $p$ *holds* then $q$ *holds*" or even "if $p$ is *true* then $q$ is *true*."\  
At first glance, this seems strange because we must also handle cases when $p$ or $q$ are false.\  
Why not just say it means â€œboth $p$ and $q$ are trueâ€?  

The key is that an implication is really a **promise** or **rule**:  
- â€œWhenever $p$ happens, $q$ must also happen.â€  

So we only judge the statement in the situations where the promise could actually be tested: **when $p$ is true**.  

- If $p$ is true and $q$ is true  â†’ the promise is kept   â†’ the implication is true.  
- If $p$ is true and $q$ is false â†’ the promise is broken â†’ the implication is false.  

But if $p$ is false, the situation that was promised **never arises**. In those cases, the rule is not violated. By convention (and to make logical systems consistent), we treat the implication as **vacuously true** whenever $p$ is false.  

This explains why the truth table looks the way it does:  

| $p$ | $q$ | $p \to q$ | Explanation |
|-----|-----|-----------|-------------|
| T   | T   | T         | promise kept |
| T   | F   | F         | promise broken |
| F   | T   | T         | vacuously true (*condition never triggered*) |
| F   | F   | T         | vacuously true (*condition never triggered*) |

**Why is this important?**

Understanding the conditional matters because:  

1. It allows us to **formalize logical rules** like the contrapositive:  
   $$
   p \to q \;\equiv\; \lnot q \to \lnot p
   $$
   which is *central* in proofs.  

2. It prevents confusion when reading theorems:  
   - â€œIf a sequence converges, then it is boundedâ€ ($p \to q$).  
   - This is **not** claiming that all bounded sequences converge; the truth table guarantees the direction of the promise is clear.  

3. It highlights **vacuous truth**, which appears everywhere in math:  
   - â€œAll unicorns have hornsâ€ is technically true, because there are no unicorns to provide a counterexample.  
   - Similarly, in probability, if an event has probability zero, conditional statements given that event can be vacuously true.  

By appreciating this structure, the reader sees why the conditional is defined with its somewhat surprising truth table: it captures the idea of a promise that can only be broken in one very specific case.

---

**Biconditional: $p \leftrightarrow q$ (â€œ$p$ if and only if $q$â€)**

- **Rule:** $p \leftrightarrow q$ is true exactly when $p$ and $q$ have the **same truth value**  
  (both true or both false).  
  
  Equivalent form:  
  $$
  p \leftrightarrow q \;\equiv\; (p \to q) \land (q \to p)
  $$  

- **Example:**  
  $p$: â€œToday is Saturday.â€  
  $q$: â€œTomorrow is Sunday.â€  
  $p \leftrightarrow q$: â€œToday is Saturday **if and only if** tomorrow is Sunday.â€  

- **Case analysis:**  
  - If both $p$ and $q$ are true â†’ the biconditional is true (both directions of the promise hold).  
  - If $p$ is true but $q$ is false â†’ false, because one direction of the â€œif and only ifâ€ fails.  
  - If $p$ is false but $q$ is true â†’ false, for the same reason.  
  - If both $p$ and $q$ are false â†’ true, because they match in value (both false).  

This explains why the truth table looks like this:  

| $p$ | $q$ | $p \leftrightarrow q$ | Explanation |
|-----|-----|-----------------------|-------------|
| T   | T   | T                     | both true â†’ promise kept |
| T   | F   | F                     | mismatch â†’ one direction fails |
| F   | T   | F                     | mismatch â†’ one direction fails |
| F   | F   | T                     | both false â†’ they match |


**Intuition and goal of the biconditional**

The biconditional expresses **equivalence**: $p$ and $q$ â€œ*stand or fall together*.â€  
It is stronger than a one-way implication: both $p \to q$ *and* $q \to p$ must hold.  

- If you read $p \leftrightarrow q$ aloud, it means:  
  *â€œ$p$ is true exactly when $q$ is true.â€* or *â€œ$p$ holds exactly when $q$ holds.â€*

This is why mathematicians often use â€œ**iff**â€ (*â€œif and only ifâ€*) in definitions and theorems:  
- It guarantees not only that $p$ implies $q$, but also that $q$ implies $p$.  

---

**Why is this important?**

1. It formalizes **definitions** in mathematics.  
   - Example: â€œA number $n$ is even **iff** $n = 2k$ for some integer $k$.â€  
   - This captures both directions: every even number has that form, and every number of that form is even.  

2. It allows us to state **equivalence theorems**.  
   - Example: â€œA sequence is Cauchy **iff** it is convergent (in $\mathbb{R}$).â€  
   - The biconditional captures the deep connection: each property implies the other.  

3. It makes reasoning reversible.  
   - With an implication, you can only go forward ($p \to q$).  
   - With a biconditional, you can go forward and backward: knowing either $p$ or $q$ tells you the other.  

By mastering the biconditional, the reader understands why mathematicians love the phrase â€œif and only ifâ€: itâ€™s the precise way of stating **true equivalence** between concepts.

**Summary Table of Connectives**

| Connective | Symbol | Rule (when true) |
|------------|--------|------------------|
| Negation   | $\lnot p$ | when $p$ is false |
| Conjunction | $p \land q$ | when $p$ and $q$ are true |
| Disjunction | $p \lor q$ | when at least one of $p, q$ is true |
| Conditional | $p \to q$ | false only if $p$ true and $q$ false |
| Biconditional | $p \leftrightarrow q$ | when $p$ and $q$ have same truth value |

## 1.3 Tautologies, Contradictions, and Logical Equivalence

**Tautology**

**Definition:**  
A **tautology** is a statement that is true in *all possible cases*.  

**Why it matters:**  
- Tautologies act like **universal truths**: they donâ€™t depend on data or assumptions.  
- They are often the â€œglueâ€ of proofs, showing that certain forms are always valid.  
- Many rules of inference (like *modus ponens*) are based on tautologies.  

**Example (logic):**  

$$
(p \land q) \to p
$$  

This means: *If both \(p\) and \(q\) are true, then \(p\) is true.*  
- Always true, regardless of whether \(p\) or \(q\) are true or false.  

**Example (statistics):**  
The **Law of Total Probability** is tautological:  

$$
P(A) = P(A \cap B) + P(A \cap \lnot B).
$$  

This identity always holds by construction, no matter what events \(A\) and \(B\) are.  

---

**Contradiction**

**Definition:**  
A **contradiction** is a statement that is false in *all possible cases*.  

**Why it matters:**  
- Contradictions are the **engine** of *proof by contradiction*.  
- If assuming something leads to a contradiction, then the assumption must be false.  
- They represent â€œimpossible situationsâ€ in logic.  

**Example (logic):**  

$$
p \land \lnot p
$$  

This means: *\(p\) is true and \(p\) is false at the same time.*  
- Always false, no matter what truth value \(p\) has.  

**Example (statistics):**  
Suppose we assume:  
1. â€œThe variance of this distribution is finite.â€  
2. â€œThe variance of this distribution is infinite.â€  

Together, these form a contradiction, so at least one assumption must be wrong.  

---

**Logical Equivalence**

**Definition:**  
Two statements are **logically equivalent** if they have the same truth value in *all possible cases*.  

**Why it matters:**  
- Logical equivalence lets us **replace one statement with another** in a proof.  
- Many powerful proof strategies rely on equivalence (contrapositive law, De Morganâ€™s laws, distributive laws).  
- Often the equivalent form is much easier to work with.  

**Example (logic):**  

$$
p \to q \;\equiv\; \lnot p \lor q
$$  

This means: *â€œIf \(p\), then \(q\)â€ is the same as â€œEither not \(p\), or \(q\).â€*  
- This equivalence makes it **easier** to manipulate conditionals in proofs.  

**Example (causal inference):**  

$$
\text{Ignorability} \to \text{Identifiability}
$$  

is **logically equivalent** to  

$$
\lnot \text{Identifiability} \to \lnot \text{Ignorability}.
$$  

Switching to the contrapositive often makes a proof or argument **simpler**.  

---

**Summary:**  
- **Tautologies** give us universal truths to rely on.  
- **Contradictions** allow us to eliminate false assumptions through contradiction proofs.  
- **Logical equivalence** lets us restate problems in easier forms without changing meaning.  

------------------------------------------------------------------------

# 2. Quantificational Logic {.unnumbered}

In propositional (sentential) logic, we treated statements as indivisible units: each one was either true or false.  
But in mathematics and statistics, we often want to say things about *all numbers* in a set, or claim that *at least one* number has a certain property.  
This is where **quantifiers** come in.

---

## 2.1 Predicates and Quantifiers

As we saw above, a **predicate** is like a sentence with a â€œblankâ€ â€” it becomes a full statement only once you plug in a value.  

- Example: $P(x): x > 0$.  
  - If $x = 2$, then $P(2)$ is the proposition â€œ2 > 0â€ (true).  
  - If $x = -3$, then $P(-3)$ is the proposition â€œ-3 > 0â€ (false).  

We use **quantifiers** to talk about how many elements **satisfy** a predicate:  

- **Universal quantifier ($\forall$):**  
  $\forall x\; P(x)$ means â€œfor all $x$, $P(x)$ is true.â€  

- **Existential quantifier ($\exists$):**  
  $\exists x\; P(x)$ means â€œthere exists at least one $x$ such that $P(x)$ is true.â€  

**Examples:**  
- $\forall x \in \mathbb{Z},\; x^2 \geq 0$. (*Every integer squared is nonnegative.*)  
- $\exists x \in \mathbb{Z},\; x^2 = 9$. (*There exists an integer whose square is 9.*)  

## 2.2 Universe of Discourse

The **universe of discourse** is the set of objects we allow $x$ to vary over.  
The truth of a statement depends on it!  

**Example:**  

- $\forall x \in \mathbb{R},\; x^2 \geq 0$ is true.  
- $\forall x \in \mathbb{Z},\; x^2 = 2$ is false (no integer squared equals 2).  

If we didnâ€™t specify whether $x$ ranges over $\mathbb{R}$ or $\mathbb{Z}$ (both interpreted as the **universe of discourse** of each **statement**), the meaning would be ambiguous.  

## 2.3 Truth of Quantified Statements

How to evaluate quantified statements:  

- $\forall x\; P(x)$ is true if *every* $x$ in the universe makes $P(x)$ true.  
- $\exists x\; P(x)$ is true if *at least one* $x$ makes $P(x)$ true.  

**Negations of Quantifiers**

Negating quantified statements flips the quantifier:  

$$
\lnot (\forall x\, P(x)) \equiv \exists x\, \lnot P(x)
$$

$$
\lnot (\exists x\, P(x)) \equiv \forall x\, \lnot P(x)
$$

**Examples:**  

- â€œNot all students passedâ€ means â€œThere exists a student who did not pass.â€  
- â€œThere does not exist a unicornâ€ means â€œFor all $x$, $x$ is not a unicorn.â€  

## 2.4 Multiple Quantifiers

Often statements involve more than one quantifier.  
**The order matters!**

- $\forall x \in \mathbb{R},\; \exists y \in \mathbb{R}: y > x$  
  â†’ True, because for every real number $x$, we can pick $y = x+1$.  

- $\exists y \in \mathbb{R},\; \forall x \in \mathbb{R}: y > x$  
  â†’ False, because no single real number is greater than *all* real numbers.  

**Tip:** Think of quantifiers as a kind of **game**:  
- For $\forall x$, your opponent chooses the worst possible $x$.  
- For $\exists y$, you get to respond by picking a suitable $y$.  

The order decides who gets to â€œmoveâ€ first, and the outcome can change completely.  

## 2.5 Why This Matters

Quantifiers appear in almost every mathematical theorem.  

- **Analysis (limits):**  
  $$
  \forall \epsilon > 0,\; \exists \delta > 0:\; |x - a| < \delta \;\to\; |f(x) - L| < \epsilon
  $$  
  (â€œFor every tolerance $\epsilon$, there exists a closeness $\delta$ that guarantees the function stays within that tolerance.â€)  

- **Statistics:**  
  - $\forall n,\; \exists \hat{\theta}_n:\; \hat{\theta}_n \to \theta$ (*There exists an estimator consistent for $\theta$.*)  
  - $\exists$ an unbiased estimator of $\mu$ (the sample mean).  

- **Causal Inference:**  
  - $\forall$ randomized experiments, $\exists$ an unbiased estimator of the treatment effect.  

Quantifiers are the way mathematics formalizes sweeping claims like â€œalwaysâ€ and â€œsometimes,â€ which are **at the heart** of proofs and assumptions in Causal ML.

---

## 2.6 Mathematical Interpretation

Quantifiers can look intimidating at first, but the real skill is **learning how to read them**.  
Every quantified statement has two parts:  
1. the **quantifier** ($\forall$ or $\exists$), and  
2. the **predicate** (a property of $x$ that is claimed to hold).  

---

**2.6.1 Single Quantifier Examples**

- **Universal** ($\forall$):  

  $$
  \forall x \in \mathbb{R},\; x^2 \geq 0
  $$  

  Read: *â€œFor every real number $x$, the square of $x$ is nonnegative.â€*  
  Interpretation: This is true, because no real number squared gives a negative result.  

  **Statistics example:**  

  $$
  \forall n \in \mathbb{N},\; \operatorname{Var}(\bar{X}_n) \geq 0
  $$  

  Read: *â€œFor every sample size $n$, the variance of the sample mean is nonnegative.â€*  
  Interpretation: Always true, because variances can never be negative.  

---

- **Existential** ($\exists$):  

  $$
  \exists x \in \mathbb{Z},\; x^2 = 4
  $$  

  Read: *â€œThere exists an integer whose square is 4.â€*  
  Interpretation: True, since $x = 2$ and $x = -2$ work.  

  **Statistics example:**  

  $$
  \exists \;\text{an estimator } \hat{\theta}\; : \; \mathbb{E}[\hat{\theta}] = \theta
  $$  

  Read: *â€œThere exists an estimator whose expected value equals the true parameter.â€*  
  Interpretation: This is the definition of an unbiased estimator (e.g., the sample mean for $\mu$).  

---

**2.6.2 Multiple Quantifier Examples**

When quantifiers are combined, **order matters**.  

- Example 1:  

  $$
  \forall x \in \mathbb{R},\; \exists y \in \mathbb{R}:\; y > x
  $$  

  Read: *â€œFor every real number $x$, there exists a real number $y$ that is greater than $x$.â€*  
  True, because if someone hands you any $x$, you can always respond with $y = x+1$.  

  **Statistics example:**  

  $$
  \forall \epsilon > 0,\; \exists N \in \mathbb{N}:\; n > N \;\to\; |\bar{X}_n - \mu| < \epsilon
  $$  

  Read: *â€œFor every tolerance $\epsilon$, there exists a large enough sample size $N$ such that if $n > N$, the sample mean is within $\epsilon$ of $\mu$.â€*  
  Interpretation: This is the definition of consistency (Law of Large Numbers).  

---

- Example 2:  

  $$
  \exists y \in \mathbb{R},\; \forall x \in \mathbb{R}:\; y > x
  $$  

  Read: *â€œThere exists a real number $y$ such that $y$ is greater than every real number $x$.â€*  
  False, because no single real number is larger than all others.  

  **Statistics example (false statement):**  

  $$
  \exists N \in \mathbb{N},\; \forall n > N:\; \bar{X}_n = \mu
  $$  

  Read: *â€œThere exists a finite sample size $N$ such that for all $n > N$, the sample mean equals the population mean exactly.â€*  
  False, because sampling variation never completely disappears â€” the sample mean only converges in probability, not with exact equality at some $N$.  

---

**2.6.3 How to Think About Multiple Quantifiers**

A useful way to think is as a **game**:  

- $\forall x$ = your opponent picks a value of $x$, possibly trying to make you fail.  
- $\exists y$ = you get to respond by picking $y$ to satisfy the condition.  

So the statement  

$$
\forall x \in \mathbb{R},\; \exists y \in \mathbb{R}:\; y > x
$$  

means: *No matter what $x$ your opponent picks, you can always respond with a suitable $y$.*  

But the reverse order  

$$
\exists y \in \mathbb{R},\; \forall x \in \mathbb{R}:\; y > x
$$  

means: *You must pick one $y$ that beats all possible $x$.* This is impossible, so the statement is false.  

---

**Why this section is important**  

Quantifiers are everywhere in math, stats, and causal ML.  

- *Universal* quantifiers express generality:  
  - â€œFor all sample sizes $n$, $\operatorname{Var}(\bar{X}_n) \geq 0$.â€  
  - â€œFor all $\epsilon > 0$, there exists an $N$ such that â€¦â€ (limits, consistency).  

- *Existential* quantifiers express possibility:  
  - â€œThere exists an unbiased estimator of $\mu$.â€  
  - â€œThere exists a consistent estimator for every parameter.â€  

- With multiple quantifiers, the **order of â€˜who chooses firstâ€™** changes the meaning dramatically.  
This interpretative skill is essential for reading theorems correctly and avoiding misinterpretation.  

---

## 2.7 Exercises {.unnumbered}

**Goal of these exercises:**  
- Practice **evaluating truth values**.  
- Practice **negating** quantified statements.  
- Practice **reading and interpreting** symbolic logic in plain English.  
At this stage, we are not proving statements â€” only learning to understand and translate them correctly.

1. **Truth values (universe of discourse: $\mathbb{Z}$):**  
   For each statement, decide whether it is true or false and explain why in words.  

   - $\forall x,\; x^2 \geq 0$  
   - $\exists x,\; x^2 = 2$  

   *Hint:* In the first, think: â€œIs there any integer whose square is negative?â€  
   In the second, think: â€œIs there an integer whose square equals 2?â€  

---

2. **Negation practice:**  
   Write the logical negation of each statement and simplify.  

   - $\forall x \in \mathbb{R},\; x^2 \geq 0$  
   - $\exists x \in \mathbb{N},\; x^2 = 2$  

   *Hint:* Use the rules:  
   $$
   \lnot (\forall x\, P(x)) \equiv \exists x\, \lnot P(x), \qquad
   \lnot (\exists x\, P(x)) \equiv \forall x\, \lnot P(x).
   $$  

---

3. **Quantifier order:**  
   Carefully interpret the following statements in plain English.  
   Are they true or false?  

   - $\forall x \in \mathbb{R},\; \exists y \in \mathbb{R}: y > x$  
   - $\exists y \in \mathbb{R},\; \forall x \in \mathbb{R}: y > x$  

---

4. **Translate into symbols:**  

   Express the following in logical notation.  

   - â€œEvery dataset has at least one outlier.â€  
   - â€œThere exists a consistent estimator for every parameter.â€  

---

5. **Interpret the following statistical statements (no proof needed):**  

   - $\forall n \in \mathbb{N},\; \operatorname{Var}(\bar{X}_n) \geq 0$  
     (*For every sample size $n$, the variance of the sample mean is nonnegative.*)  

   - $\exists n \in \mathbb{N},\; \forall \epsilon > 0:\; |\bar{X}_n - \mu| < \epsilon$  
     (*There exists a fixed sample size $n$ such that the sample mean is always arbitrarily close to $\mu$.* Is this realistic?)  

   - $\forall \epsilon > 0,\; \exists N \in \mathbb{N}:\; n > N \;\to\; |\bar{X}_n - \mu| < \epsilon$  
     (*Interpretation:* This is the formal definition of **consistency** / the Law of Large Numbers).  

---


## References

-   Velleman, D. J. (2006). *How to Prove It: A Structured Approach*.\
-   Rosen, K. H. (2011). *Discrete Mathematics and Its Applications*.\
-   Spanos, A. (1999, 2010). *Probability Theory and Statistical Inference*.