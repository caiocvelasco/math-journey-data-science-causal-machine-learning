---
title: "Phase 1: Logic & Set Theory"
---

# 0. Mathematics, Axioms, and Science {.unnumbered}

Mathematics begins with **axioms** ‚Äî *assumptions we agree to accept without proof* ‚Äî and builds everything else from them using precise rules of logic.  
This process of **axiomatization** is what gives mathematics its unique clarity: starting from simple, explicit principles, we construct entire theories.  

Why is this important?  
- Axioms are not ‚Äútrue‚Äù in an absolute sense. They are starting points.  
- What matters is whether the consequences drawn from them are **logically consistent** and **useful** for understanding the world.  
- In practice, this makes mathematics a *language* in which science can express and test ideas.  

Mathematics provides the rigorous framework for turning assumptions into predictions. 

For example, in **Euclidean geometry** we assume as an **axiom**:  

> *Through two distinct points, there is exactly one straight line.*  

But this depends on the ‚Äúspace‚Äù we are working in.  

- On a flat plane (Euclidean space), the axiom works perfectly: two points uniquely determine a straight line.  
- On the surface of a sphere (non-Euclidean geometry), the analogue of a ‚Äústraight line‚Äù is a **geodesic** ‚Äî the shortest path between two points on the surface.  
  On a sphere, geodesics turn out to be **arcs of great circles** (circles centered at the sphere‚Äôs center, like the equator or any meridian).  

Intuitively: 
- if you fly from *New York* to *Paris*, the shortest route is not a line of latitude on a flat map, but rather a curved arc of a great circle ‚Äî airplanes follow these routes because they minimize distance. 
- the airplane could in principle fly in a perfectly straight line ‚Äî but only if it were allowed to go through the Earth‚Äôs interior. Since it‚Äôs constrained to stay on the surface of the sphere, the ‚Äústraight line‚Äù within that space becomes a geodesic (the great circle).

Now consider two points on the equator that are exactly opposite each other (say *Rio de Janeiro* and *Jakarta*).  
- There isn‚Äôt a **unique** great circle through them ‚Äî in fact, infinitely many great circles (all meridians) pass through both points.  
- So the Euclidean axiom **fails** in this curved space.  

The key idea is that ‚Äústraight line‚Äù really means **shortest possible path** given the geometry of the space.  

On a plane, that‚Äôs the familiar straight line; on a sphere, it‚Äôs a great circle.  
The notion of a geodesic generalizes this idea: whatever the space, it tells us the natural way to connect two points as efficiently as possible.

This illustrates a key point: 

> axioms are not *‚Äúuniversally true‚Äù* in some absolute sense.  

They are **conventions** that **define the system we are working in**.  
Within their proper context (e.g., Euclidean geometry), their consequences are **logically consistent** (no contradictions arise) and **useful** (for designing buildings, maps, and bridges). But in other contexts (e.g., spherical geometry), we need a different set of axioms.    

In the same way, in **probability** we may assume:  

> *Outcomes of an experiment are independent and identically distributed (i.i.d.).*  

Real-world data often violates this (dependence in time series, non-identical distributions in heterogeneous populations).  
But within the i.i.d. framework, the consequences are **logically consistent** and extremely **useful**: they give us fundamental results like the **Law of Large Numbers** and the **Central Limit Theorem**.  

The analogy with geometry is this:  
- In flat Euclidean space, the ‚Äústraight line‚Äù is the natural geodesic.  
- On a sphere, the ‚Äústraight line‚Äù is reinterpreted as a great circle.  

Similarly, in probability:  
- For many problems, assuming i.i.d. is the **natural starting point** (the ‚Äústraight line‚Äù of probability).  
- In more complex contexts (time series, causal inference), we need to **redefine the structure** (like choosing geodesics on a sphere) with weaker or different axioms ‚Äî e.g., mixing conditions, stationarity, or potential outcomes.  


**Why we begin with Logic and Set Theory**  

1. **Logic** gives us the rules of the game:  
   how to combine statements, reason consistently, and structure proofs.  
   Without logic, axioms and theorems would collapse into ambiguity.  

2. **Set Theory** provides the building blocks:  
   almost every object in modern mathematics (numbers, functions, probability spaces, datasets) can be described in terms of sets.  

Together, Logic and Set Theory form the **grammar and vocabulary of mathematics**.  
They are not yet ‚Äúabout the real world‚Äù ‚Äî but they give us the precise tools to state assumptions and derive consequences.  

**Intuition for research**  

When you see theoretical work in physics, economics, or causal machine learning, it often starts by **axiomatizing** the problem:  
- *Define* the objects (e.g. random variables, causal spaces).  
- *Specify* assumptions as axioms (e.g. independence, stability, interventions).  
- *Derive* results rigorously from there.  

By starting here, we are learning how to *speak the language of mathematics* before applying it to inference, probability, and causality.

---

Thus, before diving into analysis and probability, we establish a foundation in logic and set theory. This is important to **formalize assumptions, express mathematical objects precisely, and build proofs with rigor**.

Logic gives us the language **to connect premises and conclusions**, while Set Theory gives us **the structure to define universes of discourse, events, functions, probability spaces, etc**.

Therefore: **Yes, it would be essential to learn Logic and Set Theory!**

# 1. Sentential Logic {.unnumbered}

## 1.1 Statements, Propositions, Predicates, and Connectives

**üí°Motivation**\
Learning statements and connectives is like learning the **alphabet of mathematics**.\
- If you cannot distinguish valid statements, you cannot even *start* a proof.

*Example in causal inference:*\
- $p$: ‚ÄúTreatment is randomized.‚Äù\
- $q$: ‚ÄúIgnorability holds.‚Äù\
- Then ‚ÄúIf treatment is randomized, then ignorability holds‚Äù is $p \to q$.

Without connectives, we‚Äôd stay in informal language. With them, **we can formalize statements and reason rigorously about consequences**, such as proving that a set of assumptions implies consistency of an estimator, showing that ignorability implies identification of a treatment effect, or demonstrating that conditional independence leads to factorization of a probability distribution into simpler components.

A **Statement** (or **Proposition**) is a declarative sentence that is either *true* or *false*.

-   Example: ‚Äú3 is even‚Äù (false), ‚ÄúBarcelona is in Spain‚Äù (true).

A **Predicate** is like a ‚Äútemplate‚Äù for a statement: it depends on a variable and becomes a statement once you specify the value. For example:\
$P(x): x > 0$\
- $P(2)$ ‚Üí ‚Äú2 \> 0‚Äù (true).\
- $P(-1)$ ‚Üí ‚Äú-1 \> 0‚Äù (false).

**Connectives** let us combine assumptions systematically. Logical connectives let us build compound statements:

-   Negation: $\lnot p$ (‚Äúnot $p$‚Äù)\
-   Conjunction: $p \land q$ (‚Äú$p$ and $q$‚Äù)\
-   Disjunction: $p \lor q$ (‚Äú$p$ or $q$‚Äù)\
-   Conditional: $p \to q$ (‚Äúif $p$ then $q$‚Äù)\
-   Biconditional: $p \leftrightarrow q$ (‚Äú$p$ if and only if $q$‚Äù)

Thus:\
-   **Predicate**: *general template* (open sentence, truth depends on a variable) -\> becomes true or false only when a variable is given a value.\
-   **Proposition/statement**: *instance of that template* (closed sentence, definite truth) -\> something that is already true or false.\
-   **Connectives**: *operators* that take simple propositions and form compound propositions.

## 1.2 Truth Tables

**üí°Motivation** Truth tables are the **grammar checker** of logic.\
- They allow us to test whether two statements are equivalent (so we can swap one for another in a proof, probably to make life easier).\
- They reveal tautologies (always true) and contradictions (always false).\
- They give a mechanical way to check validity of deductive arguments.

*Example in statistics:*\
- ‚ÄúIf the data behave nicely (i.i.d. + finite variance), then the sample mean is reliable (it will converge to the true mean)‚Äù ($p \to q$).\
- Equivalent contrapositive: ‚ÄúIf effect is not identifiable, then ignorability does not hold‚Äù ($\lnot q \to \lnot p$).\
- Truth tables prove these are the same, so you can flip perspectives safely in a paper.

A **truth table** shows how the truth value of a compound statement depends on its parts.

**Example:** Prove that an implication ($p \to q$) is equivalent ($\equiv$) to its contrapositive ($\lnot q \to \lnot p$).

We want to show: $p \to q \;\equiv\; \lnot q \to \lnot p$

| $p$ | $q$ | $p \to q$ | $\lnot q$ | $\lnot p$ | $\lnot q \to \lnot p$ |
|-----|-----|-----------|-----------|-----------|-----------------------|
| T   | T   | T         | F         | F         | T                     |
| T   | F   | F         | T         | F         | F                     |
| F   | T   | T         | F         | T         | T                     |
| F   | F   | T         | T         | T         | T                     |

Since the last two columns match, the implication is equivalent to its contrapositive.

**The Basic Connectives**

Logical connectives are rules for combining simpler statements into compound ones.  
Here are the five most common and some intuition:

---

**Negation: $\lnot p$ (‚Äúnot $p$‚Äù)**

- **Rule:** Negation flips the truth value.  
  - If $p$ is true, $\lnot p$ is false.  
  - If $p$ is false, $\lnot p$ is true.  

- **Example:**  
  $p$: ‚ÄúIt is raining.‚Äù  
  $\lnot p$: ‚ÄúIt is not raining.‚Äù  

---

**Conjunction: $p \land q$ (‚Äú$p$ and $q$‚Äù)**

- **Rule:** $p \land q$ is true only if **both** $p$ and $q$ are true.  

- **Example:**  
  $p$: ‚ÄúIt is raining.‚Äù  
  $q$: ‚ÄúI am carrying an umbrella.‚Äù  
  $p \land q$: ‚ÄúIt is raining **and** I am carrying an umbrella.‚Äù  

- **Truth check:** If either part fails, the whole conjunction is false.  

---

**Disjunction: $p \lor q$ (‚Äú$p$ or $q$‚Äù)**

- **Rule:** $p \lor q$ is true if **at least one** of $p, q$ is true.  
  (This is the **inclusive or** used in logic.)  

- **Example:**  
  $p$: ‚ÄúI will drink coffee.‚Äù  
  $q$: ‚ÄúI will drink tea.‚Äù  
  $p \lor q$: ‚ÄúI will drink coffee or tea (or both).‚Äù  

- **Note:** In everyday language, ‚Äúor‚Äù can be exclusive. Logic defaults to inclusive.  

---

**Conditional: $p \to q$ (‚Äúif $p$ then $q$‚Äù)**

- **Rule:** An implication is false **only** when $p$ is true and $q$ is false.  
  **In all other cases, it is true.**  
  Equivalent form:  
  $$
  p \to q \;\equiv\; \lnot p \lor q
  $$  

- **Example:**  
  $p$: ‚ÄúIt rains.‚Äù  
  $q$: ‚ÄúThe ground is wet.‚Äù  
  $p \to q$: ‚ÄúIf it rains, then the ground is wet.‚Äù  

- **Case analysis:**\  
  - If it rains **and** the ground is wet ‚Üí the statement ‚ÄúIf it rains, then the ground is wet‚Äù has been **kept**. Both the condition and the consequence hold, so the implication is **true**.  
  - If it rains **but** the ground is not wet ‚Üí the statement has been **broken**. This is the **only case** where an implication is false: the condition was met but the promised result failed.  
  - If it doesn‚Äôt rain ‚Üí the statement never gets a chance to be tested. We cannot accuse it of being false, because the condition (‚Äúit rains‚Äù) never happened. By definition, logic treats this as **vacuously true**: the promise has not been broken, since there was nothing to check.  
  - If it doesn‚Äôt rain **and** the ground is wet ‚Üí still vacuously true. The implication didn‚Äôt say what should happen when it doesn‚Äôt rain; the ground being wet for other reasons (sprinklers, a bucket of water, etc.) doesn‚Äôt violate the promise.

**Intuition and goal of the conditional**

The statement $p \to q$ is read "if $p$ *holds* then $q$ *holds*" or even "if $p$ is *true* then $q$ is *true*."\  
At first glance, this seems strange because we must also handle cases when $p$ or $q$ are false.\  
Why not just say it means ‚Äúboth $p$ and $q$ are true‚Äù?  

The key is that an implication is really a **promise** or **rule**:  
- ‚ÄúWhenever $p$ happens, $q$ must also happen.‚Äù  

So we only judge the statement in the situations where the promise could actually be tested: **when $p$ is true**.  

- If $p$ is true and $q$ is true  ‚Üí the promise is kept   ‚Üí the implication is true.  
- If $p$ is true and $q$ is false ‚Üí the promise is broken ‚Üí the implication is false.  

But if $p$ is false, the situation that was promised **never arises**. In those cases, the rule is not violated. By convention (and to make logical systems consistent), we treat the implication as **vacuously true** whenever $p$ is false.  

This explains why the truth table looks the way it does:  

| $p$ | $q$ | $p \to q$ | Explanation |
|-----|-----|-----------|-------------|
| T   | T   | T         | promise kept |
| T   | F   | F         | promise broken |
| F   | T   | T         | vacuously true (*condition never triggered*) |
| F   | F   | T         | vacuously true (*condition never triggered*) |

**Why is this important?**

Understanding the conditional matters because:  

1. It allows us to **formalize logical rules** like the contrapositive:  
   $$
   p \to q \;\equiv\; \lnot q \to \lnot p
   $$
   which is *central* in proofs.  

2. It prevents confusion when reading theorems:  
   - ‚ÄúIf a sequence converges, then it is bounded‚Äù ($p \to q$).  
   - This is **not** claiming that all bounded sequences converge; the truth table guarantees the direction of the promise is clear.  

3. It highlights **vacuous truth**, which appears everywhere in math:  
   - ‚ÄúAll unicorns have horns‚Äù is technically true, because there are no unicorns to provide a counterexample.  
   - Similarly, in probability, if an event has probability zero, conditional statements given that event can be vacuously true.  

By appreciating this structure, the reader sees why the conditional is defined with its somewhat surprising truth table: it captures the idea of a promise that can only be broken in one very specific case.

---

**Biconditional: $p \leftrightarrow q$ (‚Äú$p$ if and only if $q$‚Äù)**

- **Rule:** $p \leftrightarrow q$ is true exactly when $p$ and $q$ have the **same truth value**  
  (both true or both false).  
  
  Equivalent form:  
  $$
  p \leftrightarrow q \;\equiv\; (p \to q) \land (q \to p)
  $$  

- **Example:**  
  $p$: ‚ÄúToday is Saturday.‚Äù  
  $q$: ‚ÄúTomorrow is Sunday.‚Äù  
  $p \leftrightarrow q$: ‚ÄúToday is Saturday **if and only if** tomorrow is Sunday.‚Äù  

- **Case analysis:**  
  - If both $p$ and $q$ are true ‚Üí the biconditional is true (both directions of the promise hold).  
  - If $p$ is true but $q$ is false ‚Üí false, because one direction of the ‚Äúif and only if‚Äù fails.  
  - If $p$ is false but $q$ is true ‚Üí false, for the same reason.  
  - If both $p$ and $q$ are false ‚Üí true, because they match in value (both false).  

This explains why the truth table looks like this:  

| $p$ | $q$ | $p \leftrightarrow q$ | Explanation |
|-----|-----|-----------------------|-------------|
| T   | T   | T                     | both true ‚Üí promise kept |
| T   | F   | F                     | mismatch ‚Üí one direction fails |
| F   | T   | F                     | mismatch ‚Üí one direction fails |
| F   | F   | T                     | both false ‚Üí they match |


**Intuition and goal of the biconditional**

The biconditional expresses **equivalence**: $p$ and $q$ ‚Äú*stand or fall together*.‚Äù  
It is stronger than a one-way implication: both $p \to q$ *and* $q \to p$ must hold.  

- If you read $p \leftrightarrow q$ aloud, it means:  
  *‚Äú$p$ is true exactly when $q$ is true.‚Äù* or *‚Äú$p$ holds exactly when $q$ holds.‚Äù*

This is why mathematicians often use ‚Äú**iff**‚Äù (*‚Äúif and only if‚Äù*) in definitions and theorems:  
- It guarantees not only that $p$ implies $q$, but also that $q$ implies $p$.  

---

**Why is this important?**

1. It formalizes **definitions** in mathematics.  
   - Example: ‚ÄúA number $n$ is even **iff** $n = 2k$ for some integer $k$.‚Äù  
   - This captures both directions: every even number has that form, and every number of that form is even.  

2. It allows us to state **equivalence theorems**.  
   - Example: ‚ÄúA sequence is Cauchy **iff** it is convergent (in $\mathbb{R}$).‚Äù  
   - The biconditional captures the deep connection: each property implies the other.  

3. It makes reasoning reversible.  
   - With an implication, you can only go forward ($p \to q$).  
   - With a biconditional, you can go forward and backward: knowing either $p$ or $q$ tells you the other.  

By mastering the biconditional, the reader understands why mathematicians love the phrase ‚Äúif and only if‚Äù: it‚Äôs the precise way of stating **true equivalence** between concepts.

**Summary Table of Connectives**

| Connective | Symbol | Rule (when true) |
|------------|--------|------------------|
| Negation   | $\lnot p$ | when $p$ is false |
| Conjunction | $p \land q$ | when $p$ and $q$ are true |
| Disjunction | $p \lor q$ | when at least one of $p, q$ is true |
| Conditional | $p \to q$ | false only if $p$ true and $q$ false |
| Biconditional | $p \leftrightarrow q$ | when $p$ and $q$ have same truth value |

## 1.3 Tautologies, Contradictions, and Logical Equivalence

**Tautology**

**Definition:**  
A **tautology** is a statement that is true in *all possible cases*.  

**Why it matters:**  
- Tautologies act like **universal truths**: they don‚Äôt depend on data or assumptions.  
- They are often the ‚Äúglue‚Äù of proofs, showing that certain forms are always valid.  
- Many rules of inference (like *modus ponens*) are based on tautologies.  

**Example (logic):**  

$$
(p \land q) \to p
$$  

This means: *If both \(p\) and \(q\) are true, then \(p\) is true.*  
- Always true, regardless of whether \(p\) or \(q\) are true or false.  

**Example (statistics):**  
The **Law of Total Probability** is tautological:  

$$
P(A) = P(A \cap B) + P(A \cap \lnot B).
$$  

This identity always holds by construction, no matter what events \(A\) and \(B\) are.  

---

**Contradiction**

**Definition:**  
A **contradiction** is a statement that is false in *all possible cases*.  

**Why it matters:**  
- Contradictions are the **engine** of *proof by contradiction*.  
- If assuming something leads to a contradiction, then the assumption must be false.  
- They represent ‚Äúimpossible situations‚Äù in logic.  

**Example (logic):**  

$$
p \land \lnot p
$$  

This means: *\(p\) is true and \(p\) is false at the same time.*  
- Always false, no matter what truth value \(p\) has.  

**Example (statistics):**  
Suppose we assume:  
1. ‚ÄúThe variance of this distribution is finite.‚Äù  
2. ‚ÄúThe variance of this distribution is infinite.‚Äù  

Together, these form a contradiction, so at least one assumption must be wrong.  

---

**Logical Equivalence**

**Definition:**  
Two statements are **logically equivalent** if they have the same truth value in *all possible cases*.  

**Why it matters:**  
- Logical equivalence lets us **replace one statement with another** in a proof.  
- Many powerful proof strategies rely on equivalence (contrapositive law, De Morgan‚Äôs laws, distributive laws).  
- Often the equivalent form is much easier to work with.  

**Example (logic):**  

$$
p \to q \;\equiv\; \lnot p \lor q
$$  

This means: *‚ÄúIf \(p\), then \(q\)‚Äù is the same as ‚ÄúEither not \(p\), or \(q\).‚Äù*  
- This equivalence makes it **easier** to manipulate conditionals in proofs.  

**Example (causal inference):**  

$$
\text{Ignorability} \to \text{Identifiability}
$$  

is **logically equivalent** to  

$$
\lnot \text{Identifiability} \to \lnot \text{Ignorability}.
$$  

Switching to the contrapositive often makes a proof or argument **simpler**.  

---

**Summary:**  
- **Tautologies** give us universal truths to rely on.  
- **Contradictions** allow us to eliminate false assumptions through contradiction proofs.  
- **Logical equivalence** lets us restate problems in easier forms without changing meaning.  

------------------------------------------------------------------------

# 2. Quantificational Logic {.unnumbered}

In propositional (sentential) logic, we treated statements as indivisible units: each one was either true or false.  
But in mathematics and statistics, we often want to say things about *all numbers* in a set, or claim that *at least one* number has a certain property.  
This is where **quantifiers** come in.

---

## 2.1 Predicates and Quantifiers

As we saw above, a **predicate** is like a sentence with a ‚Äúblank‚Äù ‚Äî it becomes a full statement only once you plug in a value.  

- Example: $P(x): x > 0$.  
  - If $x = 2$, then $P(2)$ is the proposition ‚Äú2 > 0‚Äù (true).  
  - If $x = -3$, then $P(-3)$ is the proposition ‚Äú-3 > 0‚Äù (false).  

We use **quantifiers** to talk about how many elements **satisfy** a predicate:  

- **Universal quantifier ($\forall$):**  
  $\forall x\; P(x)$ means ‚Äúfor all $x$, $P(x)$ is true.‚Äù  

- **Existential quantifier ($\exists$):**  
  $\exists x\; P(x)$ means ‚Äúthere exists at least one $x$ such that $P(x)$ is true.‚Äù  

**Examples:**  
- $\forall x \in \mathbb{Z},\; x^2 \geq 0$. (*Every integer squared is nonnegative.*)  
- $\exists x \in \mathbb{Z},\; x^2 = 9$. (*There exists an integer whose square is 9.*)  

## 2.2 Universe of Discourse

The **universe of discourse** is the set of objects we allow $x$ to vary over.  
The truth of a statement depends on it!  

**Example:**  

- $\forall x \in \mathbb{R},\; x^2 \geq 0$ is true.  
- $\forall x \in \mathbb{Z},\; x^2 = 2$ is false (no integer squared equals 2).  

If we didn‚Äôt specify whether $x$ ranges over $\mathbb{R}$ or $\mathbb{Z}$ (both interpreted as the **universe of discourse** of each **statement**), the meaning would be ambiguous.  

## 2.3 Truth of Quantified Statements

How to evaluate quantified statements:  

- $\forall x\; P(x)$ is true if *every* $x$ in the universe makes $P(x)$ true.  
- $\exists x\; P(x)$ is true if *at least one* $x$ makes $P(x)$ true.  

**Negations of Quantifiers**

Negating quantified statements flips the quantifier:  

$$
\lnot (\forall x\, P(x)) \equiv \exists x\, \lnot P(x)
$$

$$
\lnot (\exists x\, P(x)) \equiv \forall x\, \lnot P(x)
$$

**Examples:**  

- ‚ÄúNot all students passed‚Äù means ‚ÄúThere exists a student who did not pass.‚Äù  
- ‚ÄúThere does not exist a unicorn‚Äù means ‚ÄúFor all $x$, $x$ is not a unicorn.‚Äù  

## 2.4 Multiple Quantifiers

Often statements involve more than one quantifier.  
**The order matters!**

- $\forall x \in \mathbb{R},\; \exists y \in \mathbb{R}: y > x$  
  ‚Üí True, because for every real number $x$, we can pick $y = x+1$.  

- $\exists y \in \mathbb{R},\; \forall x \in \mathbb{R}: y > x$  
  ‚Üí False, because no single real number is greater than *all* real numbers.  

**Tip:** Think of quantifiers as a kind of **game**:  
- For $\forall x$, your opponent chooses the worst possible $x$.  
- For $\exists y$, you get to respond by picking a suitable $y$.  

The order decides who gets to ‚Äúmove‚Äù first, and the outcome can change completely.  

## 2.5 Why This Matters

Quantifiers appear in almost every mathematical theorem.  

- **Analysis (limits):**  
  $$
  \forall \epsilon > 0,\; \exists \delta > 0:\; |x - a| < \delta \;\to\; |f(x) - L| < \epsilon
  $$  
  (‚ÄúFor every tolerance $\epsilon$, there exists a closeness $\delta$ that guarantees the function stays within that tolerance.‚Äù)  

- **Statistics:**  
  - $\forall n,\; \exists \hat{\theta}_n:\; \hat{\theta}_n \to \theta$ (*There exists an estimator consistent for $\theta$.*)  
  - $\exists$ an unbiased estimator of $\mu$ (the sample mean).  

- **Causal Inference:**  
  - $\forall$ randomized experiments, $\exists$ an unbiased estimator of the treatment effect.  

Quantifiers are the way mathematics formalizes sweeping claims like ‚Äúalways‚Äù and ‚Äúsometimes,‚Äù which are **at the heart** of proofs and assumptions in Causal ML.

---

## 2.6 Mathematical Interpretation

Quantifiers can look intimidating at first, but the real skill is **learning how to read them**.  
Every quantified statement has two parts:  
1. the **quantifier** ($\forall$ or $\exists$), and  
2. the **predicate** (a property of $x$ that is claimed to hold).  

---

**2.6.1 Single Quantifier Examples**

- **Universal** ($\forall$):  

  $$
  \forall x \in \mathbb{R},\; x^2 \geq 0
  $$  

  Read: *‚ÄúFor every real number $x$, the square of $x$ is nonnegative.‚Äù*  
  Interpretation: This is true, because no real number squared gives a negative result.  

  **Statistics example:**  

  $$
  \forall n \in \mathbb{N},\; \operatorname{Var}(\bar{X}_n) \geq 0
  $$  

  Read: *‚ÄúFor every sample size $n$, the variance of the sample mean is nonnegative.‚Äù*  
  Interpretation: Always true, because variances can never be negative.  

---

- **Existential** ($\exists$):  

  $$
  \exists x \in \mathbb{Z},\; x^2 = 4
  $$  

  Read: *‚ÄúThere exists an integer whose square is 4.‚Äù*  
  Interpretation: True, since $x = 2$ and $x = -2$ work.  

  **Statistics example:**  

  $$
  \exists \;\text{an estimator } \hat{\theta}\; : \; \mathbb{E}[\hat{\theta}] = \theta
  $$  

  Read: *‚ÄúThere exists an estimator whose expected value equals the true parameter.‚Äù*  
  Interpretation: This is the definition of an unbiased estimator (e.g., the sample mean for $\mu$).  

---

**2.6.2 Multiple Quantifier Examples**

When quantifiers are combined, **order matters**.  

- Example 1:  

  $$
  \forall x \in \mathbb{R},\; \exists y \in \mathbb{R}:\; y > x
  $$  

  Read: *‚ÄúFor every real number $x$, there exists a real number $y$ that is greater than $x$.‚Äù*  
  True, because if someone hands you any $x$, you can always respond with $y = x+1$.  

  **Statistics example:**  

  $$
  \forall \epsilon > 0,\; \exists N \in \mathbb{N}:\; n > N \;\to\; |\bar{X}_n - \mu| < \epsilon
  $$  

  Read: *‚ÄúFor every tolerance $\epsilon$, there exists a large enough sample size $N$ such that if $n > N$, the sample mean is within $\epsilon$ of $\mu$.‚Äù*  
  Interpretation: This is the definition of consistency (Law of Large Numbers).  

---

- Example 2:  

  $$
  \exists y \in \mathbb{R},\; \forall x \in \mathbb{R}:\; y > x
  $$  

  Read: *‚ÄúThere exists a real number $y$ such that $y$ is greater than every real number $x$.‚Äù*  
  False, because no single real number is larger than all others.  

  **Statistics example (false statement):**  

  $$
  \exists N \in \mathbb{N},\; \forall n > N:\; \bar{X}_n = \mu
  $$  

  Read: *‚ÄúThere exists a finite sample size $N$ such that for all $n > N$, the sample mean equals the population mean exactly.‚Äù*  
  False, because sampling variation never completely disappears ‚Äî the sample mean only converges in probability, not with exact equality at some $N$.  

---

**2.6.3 How to Think About Multiple Quantifiers**

A useful way to think is as a **game**:  

- $\forall x$ = your opponent picks a value of $x$, possibly trying to make you fail.  
- $\exists y$ = you get to respond by picking $y$ to satisfy the condition.  

So the statement  

$$
\forall x \in \mathbb{R},\; \exists y \in \mathbb{R}:\; y > x
$$  

means: *No matter what $x$ your opponent picks, you can always respond with a suitable $y$.*  

But the reverse order  

$$
\exists y \in \mathbb{R},\; \forall x \in \mathbb{R}:\; y > x
$$  

means: *You must pick one $y$ that beats all possible $x$.* This is impossible, so the statement is false.  

---

**Why this section is important**  

Quantifiers are everywhere in math, stats, and causal ML.  

- *Universal* quantifiers express generality:  
  - ‚ÄúFor all sample sizes $n$, $\operatorname{Var}(\bar{X}_n) \geq 0$.‚Äù  
  - ‚ÄúFor all $\epsilon > 0$, there exists an $N$ such that ‚Ä¶‚Äù (limits, consistency).  

- *Existential* quantifiers express possibility:  
  - ‚ÄúThere exists an unbiased estimator of $\mu$.‚Äù  
  - ‚ÄúThere exists a consistent estimator for every parameter.‚Äù  

- With multiple quantifiers, the **order of ‚Äòwho chooses first‚Äô** changes the meaning dramatically.  
This interpretative skill is essential for reading theorems correctly and avoiding misinterpretation.  

---

## 2.7 Exercises {.unnumbered}

**Goal of these exercises:**  
- Practice **evaluating truth values**.  
- Practice **negating** quantified statements.  
- Practice **reading and interpreting** symbolic logic in plain English.  
At this stage, we are not proving statements ‚Äî only learning to understand and translate them correctly.

1. **Truth values (universe of discourse: $\mathbb{Z}$):**  
   For each statement, decide whether it is true or false and explain why in words.  

   - $\forall x,\; x^2 \geq 0$  
   - $\exists x,\; x^2 = 2$  

   *Hint:* In the first, think: ‚ÄúIs there any integer whose square is negative?‚Äù  
   In the second, think: ‚ÄúIs there an integer whose square equals 2?‚Äù  

---

2. **Negation practice:**  
   Write the logical negation of each statement and simplify.  

   - $\forall x \in \mathbb{R},\; x^2 \geq 0$  
   - $\exists x \in \mathbb{N},\; x^2 = 2$  

   *Hint:* Use the rules:  
   $$
   \lnot (\forall x\, P(x)) \equiv \exists x\, \lnot P(x), \qquad
   \lnot (\exists x\, P(x)) \equiv \forall x\, \lnot P(x).
   $$  

---

3. **Quantifier order:**  
   Carefully interpret the following statements in plain English.  
   Are they true or false?  

   - $\forall x \in \mathbb{R},\; \exists y \in \mathbb{R}: y > x$  
   - $\exists y \in \mathbb{R},\; \forall x \in \mathbb{R}: y > x$  

---

4. **Translate into symbols:**  

   Express the following in logical notation.  

   - ‚ÄúEvery dataset has at least one outlier.‚Äù  
   - ‚ÄúThere exists a consistent estimator for every parameter.‚Äù  

---

5. **Interpret the following statistical statements (no proof needed):**  

   - $\forall n \in \mathbb{N},\; \operatorname{Var}(\bar{X}_n) \geq 0$  
     (*For every sample size $n$, the variance of the sample mean is nonnegative.*)  

   - $\exists n \in \mathbb{N},\; \forall \epsilon > 0:\; |\bar{X}_n - \mu| < \epsilon$  
     (*There exists a fixed sample size $n$ such that the sample mean is always arbitrarily close to $\mu$.* Is this realistic?)  

   - $\forall \epsilon > 0,\; \exists N \in \mathbb{N}:\; n > N \;\to\; |\bar{X}_n - \mu| < \epsilon$  
     (*Interpretation:* This is the formal definition of **consistency** / the Law of Large Numbers).  

---


## References

-   Velleman, D. J. (2006). *How to Prove It: A Structured Approach*.\
-   Rosen, K. H. (2011). *Discrete Mathematics and Its Applications*.\
-   Spanos, A. (1999, 2010). *Probability Theory and Statistical Inference*.