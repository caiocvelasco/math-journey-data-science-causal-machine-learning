[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Foundations of Data Science & Causal ML: A Mathematical Journey",
    "section": "",
    "text": "Journey Phases\nThis is my structured learning roadmap to prepare for research in causal machine learning with mathematical rigor.\nHere you can find both mathematical foundations and applications.",
    "crumbs": [
      "Journey Phases"
    ]
  },
  {
    "objectID": "index.html#journey-phases",
    "href": "index.html#journey-phases",
    "title": "Foundations of Data Science & Causal ML: A Mathematical Journey",
    "section": "",
    "text": "Why I‚Äôm Interested in Causal ML?\nGoal: Show the reader the contrasts between two ‚ÄúData Cultures‚Äù.\n- Statistics and econometrics build models on top of stochastic mechanisms (data-generating processes), aiming for explanation and inference.\n- Machine learning often ignores mechanisms and focuses on prediction accuracy, generalization, and performance.\n- Causal ML is a synthesis: it combines ML‚Äôs flexibility with statistics‚Äô concern for identification, adding a causal lens to reason about interventions and counterfactuals.\n\n\nPhase 1 ‚Äì Logic & Set Theory\nGoal: Build comfort with the language of mathematics.\n- Proof techniques: direct, contrapositive, contradiction, induction.\n- Sets and families of sets, Cartesian products, power sets.\n- Functions: injective, surjective, bijective.\n- Relations: equivalence relations, partial orders.\n- Cardinality: countable vs.¬†uncountable sets.\nTheory Output: - Sentential logic (Velleman Ch. 1) - Predicate logic & quantifiers (Ch. 2) - Proof techniques (direct, contrapositive, contradiction, induction) (Ch. 3) - Sets, relations, functions (Ch. 4‚Äì6) - Countability, infinity (Ch. 9 selected, Tao‚Äôs appendix)\nApplication Project: - SQL/database operations as set theory (joins, unions, intersections). - Prove/discuss equivalences (e.g., idempotency: SELECT DISTINCT twice = once)\nReferences:\n- Velleman ‚Äì How to Prove It\n- Enderton ‚Äì Set Theory\n\n\n\nPhase 2 ‚Äì Real Analysis\nGoal: Rigorous calculus and convergence, revisiting classical calculus concepts with proofs.\n\nSequences, series, limits.\n\nContinuity, compactness, connectedness.\n\nDifferentiation: Mean Value Theorem, Taylor expansion.\n\nRiemann integration (rigorous foundation).\n\nUniform convergence.\n\nApplied Calculus Lens:\n- Multivariable calculus: partial derivatives, gradients, Jacobians, Hessians.\n- Convexity and optimization.\n- Taylor expansions for approximation.\n- Fundamental Theorem of Calculus as link to probability expectations.\nTheory Output: Œµ‚ÄìŒ¥ proofs, compactness in ‚Ñù, uniform convergence examples.\nApplication Project: Gradient descent convergence demo; connect convexity to logistic regression loss.\nReferences:\n- Rudin ‚Äì Principles of Mathematical Analysis (Baby Rudin)\n- Tao ‚Äì Analysis I\n\n\n\nPhase 3 ‚Äì Linear Algebra\nGoal: Move beyond computation to proofs and structure.\n- Vector spaces, subspaces, linear independence, bases, dimension.\n- Linear transformations and matrices.\n- Inner product spaces, orthogonality, Gram‚ÄìSchmidt.\n- Determinants, eigenvalues, eigenvectors, diagonalization.\n- Spectral theorem, singular value decomposition.\n- Matrix norms and conditioning.\nTheory Output: Proofs of rank‚Äìnullity theorem, spectral theorem for symmetric matrices, SVD existence.\nApplication Project: PCA from first principles ‚Äî prove orthogonal diagonalization, then implement PCA via SVD.\nReferences:\n- Axler ‚Äì Linear Algebra Done Right\n- Friedberg, Insel & Spence ‚Äì Linear Algebra\n- Trefethen & Bau ‚Äì Numerical Linear Algebra (for computational aspects)\n\n\n\nPhase 4 ‚Äì Functional Analysis & Hilbert Spaces\nGoal: Develop the tools to handle infinite-dimensional vector spaces, operators, and kernels.\n- Normed vector spaces, Banach spaces.\n- Hilbert spaces, orthogonality, projections.\n- Bounded linear operators.\n- Reproducing Kernel Hilbert Spaces (RKHS).\nTheory Output: Prove projection theorem in Hilbert spaces, examples of bounded/unbounded operators, RKHS construction.\nApplication Project: Kernelized regression and SVMs ‚Äî connect functional analysis with machine learning models.\nReferences:\n- Kreyszig ‚Äì Introductory Functional Analysis with Applications\n- Conway ‚Äì A Course in Functional Analysis\n- Berlinet & Thomas-Agnan ‚Äì Reproducing Kernel Hilbert Spaces in Probability and Statistics\n\n\n\nPhase 5 ‚Äì Topology & Measure Theory\nGoal: Learn the structures that underlie probability theory.\n- Metric spaces, open/closed sets.\n- Compactness and product spaces.\n- œÉ-algebras, measurable functions.\n- Lebesgue measure and integration.\n- Convergence theorems: MCT, DCT.\nTheory Output: Worked examples of œÉ-algebras, Lebesgue integral, and convergence theorems.\nApplication Project: Fraud detection via Monte Carlo ‚Äî rare events and measure-zero sets in anomaly detection.\nReferences:\n- Munkres ‚Äì Topology\n- Schilling ‚Äì Measures, Integrals and Martingales\n\n\n\nPhase 6 ‚Äì Probability\nGoal: Define probability rigorously √† la Kolmogorov.\n- Probability spaces and random variables as measurable functions.\n- Distributions, independence, product measures.\n- Conditional expectation as L¬≤ projection.\n- Laws of large numbers, central limit theorem.\n- Intro to martingales.\nTheory Output: Probability space construction, LLN/CLT proofs, conditional expectation as projection.\nApplication Project: A/B testing simulation ‚Äî CLT and confidence intervals for conversion rates.\nReferences:\n- Durrett ‚Äì Probability: Theory and Examples\n- Klenke ‚Äì Probability Theory\n\n\n\nPhase 7 ‚Äì Mathematical Statistics\nGoal: Connect probability ‚Üí inference.\n- Point estimation: MLE, method of moments.\n- Properties: unbiasedness, consistency, efficiency.\n- Hypothesis testing and likelihood ratio tests.\n- Asymptotic results: convergence in probability/distribution, delta method.\nTheory Output: Consistency of MLE, hypothesis testing framework, asymptotic normality proofs.\nApplication Project: Logistic regression for churn prediction ‚Äî prove Bernoulli MLE consistency, simulate convergence, apply to real dataset.\nReferences:\n- Casella & Berger ‚Äì Statistical Inference\n\n\n\nPhase 8 ‚Äì Causality\nGoal: Enter causal inference with strong mathematical foundations.\n- Pearl‚Äôs Structural Causal Models & do-calculus.\n- Rubin‚Äôs potential outcomes framework.\n- Invariant causal prediction (Peters, Janzing, Sch√∂lkopf).\n- Identifiability proofs.\n- Axiomatic frameworks (Park & Muandet).\nTheory Output: Worked proofs of identifiability, back-door/front-door criteria, do-calculus rules.\nApplication Project: Uplift modeling for churn retention OR reproduction of Chernozhukov‚Äôs Double Machine Learning estimator with Python.\nReferences:\n- Pearl ‚Äì Causality\n- Peters, Janzing, Sch√∂lkopf ‚Äì Elements of Causal Inference\n- Chernozhukov et al.¬†‚Äì Causal Machine Learning papers",
    "crumbs": [
      "Journey Phases"
    ]
  },
  {
    "objectID": "chapters/about.html",
    "href": "chapters/about.html",
    "title": "About Me",
    "section": "",
    "text": "Hi, I‚Äôm Caio Velasco üëã\nMechanical Engineer from Brazil with a Master‚Äôs in Economics & Public Policy (UCLA), thanks to a full scholarship from the Lemann Foundation. In 2021, I paused a PhD in Economics in the Netherlands to care for family during the pandemic and transitioned fully into Data Science.\nI work at the intersection of engineering, economics, and data‚Äîspanning Business Analysis, Consulting, Analytics Engineering, and Data Science across the US, UK, Spain, and Brazil. I enjoy turning messy, fragmented systems into robust end-to-end data platforms and pairing that with mathematical depth (statistics, econometrics, and causal ML) to drive real-world impact and avoid the correlation vs.¬†causation dilemma.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>About Me</span>"
    ]
  },
  {
    "objectID": "chapters/about.html#what-im-doing-now",
    "href": "chapters/about.html#what-im-doing-now",
    "title": "About Me",
    "section": "What I‚Äôm doing now",
    "text": "What I‚Äôm doing now\n\nBuilding an open book: Foundations of Data Science & Causal ML: A Mathematical Journey (mathematical rigor + applications).\nHelping teams modernize their stacks (ingestion ‚Üí modeling with dbt ‚Üí warehouses like Snowflake/Redshift ‚Üí governance & Looker dashboards).\nDeepening foundations in real analysis, probability, statistics, and causal inference to support future research.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>About Me</span>"
    ]
  },
  {
    "objectID": "chapters/about.html#selected-highlights",
    "href": "chapters/about.html#selected-highlights",
    "title": "About Me",
    "section": "Selected highlights",
    "text": "Selected highlights\n\nAwards & scholarships from Yale University, UCLA, GE Foundation, Lemann Foundation, and The Club of Rome.\nEarly career: helped build Stone Payments (NASDAQ: STNE) in Brazil from scratch and founded MePrepara, an online math prep platform with 140+ videos supporting low-income Brazilian students.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>About Me</span>"
    ]
  },
  {
    "objectID": "chapters/about.html#focus-areas",
    "href": "chapters/about.html#focus-areas",
    "title": "About Me",
    "section": "Focus areas",
    "text": "Focus areas\n\nData & Analytics Engineering: Python ¬∑ SQL ¬∑ dbt ¬∑ Snowflake/Redshift ¬∑ AWS ¬∑ Looker ¬∑ best practices for modeling, tests, CI/CD.\nStat/ML & Causality: mathematical statistics, econometrics, causal inference.\nEducation: writing clear, didactic materials; mentoring; building bridges between theory and practice.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>About Me</span>"
    ]
  },
  {
    "objectID": "chapters/about.html#values-approach",
    "href": "chapters/about.html#values-approach",
    "title": "About Me",
    "section": "Values & approach",
    "text": "Values & approach\n\nRigor + practicality. Start from first principles, then ship value.\nClarity. Well-structured code and always documentating the process didacticaly.\nImpact. Data and technology as tools to expand opportunity‚Äîbecause education changed my life.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>About Me</span>"
    ]
  },
  {
    "objectID": "chapters/about.html#contact-links",
    "href": "chapters/about.html#contact-links",
    "title": "About Me",
    "section": "Contact & links",
    "text": "Contact & links\n\nPortfolio: https://caiocvelasco.github.io/\nGitHub: https://github.com/caiocvelasco\nLinkedIn: https://www.linkedin.com/in/caiocvelasco/\nEmail: mailto:caiocvelasco@gmail.com\n\n\n\nColophon\nThis page is rendered with Quarto. The portrait is expected at images/caio.jpg. Adjust the image path/size in the HTML above if your repo uses a different structure.",
    "crumbs": [
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>About Me</span>"
    ]
  },
  {
    "objectID": "chapters/00_ch00.html",
    "href": "chapters/00_ch00.html",
    "title": "Why I‚Äôm Interested in Causal ML?",
    "section": "",
    "text": "The 2 Worlds\nMy interest in Causal Machine Learning arises from a long-standing divide in the way different worlds think about data.\nIn his influential essay Statistical Modeling: The Two Cultures (2001), Leo Breiman described two very different traditions.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Why I'm Interested in Causal ML?</span>"
    ]
  },
  {
    "objectID": "chapters/00_ch00.html#the-2-worlds",
    "href": "chapters/00_ch00.html#the-2-worlds",
    "title": "Why I‚Äôm Interested in Causal ML?",
    "section": "",
    "text": "Data Modeling culture (the dominant one in Statistics at the time): assumes data are generated by a stochastic model with explicit assumptions (e.g., linear regression, logistic regression). The focus is on inference and interpretability.\nAlgorithmic Modeling culture (rising in Machine Learning): does not assume a specific stochastic DGP, but uses flexible algorithms (e.g., random forests, neural networks) to directly optimize predictive accuracy.\n\n\nThe Statistics (and Econometrics) World\nDid you know that when you run a regression, you are implicitly making a very strong claim about how the world works?\nIn this tradition, everything begins with the data-generating process (DGP) ‚Äî what Aris Spanos in his book Probability Theory and Statistical Inference: Econometric Modeling with Observational Data calls a stochastic mechanism. Imagine it as a hidden machine tossing probabilistic dice, producing the data we observe. A statistical model is not just a tool; it is a story about that hidden mechanism that is producing the outcomes we observe.\nExamples:\n- A linear regression assumes wages, test scores, or prices are generated as a straight-line combination of inputs plus random noise.\n- A Poisson model assumes counts come from a specific probabilistic law.\n- A time-series AR(1) model assumes today‚Äôs value depends on yesterday‚Äôs value with some random shock added in.\nIf the DGP is hidden, what do you think we want to do? To uncover its truth.\nThat is why introductory statistics courses begin with the notion of a Population and then introduce Samples. Since we cannot observe the entire population, we study samples and rely on probabilistic laws ‚Äî the Law of Large Numbers (LLN), the Central Limit Theorem (CLT) ‚Äî to argue that repeated samples will ‚Äústabilize‚Äù around the truth about some population parameters (mean, median, variance‚Ä¶).\nFrom there comes Inference:\n&gt; If I trust my assumptions about the DGP, then I can use my sample to make representative claims about the population.\nThis is the heart of statistics and econometrics: model the hidden process, estimate its parameters, and test whether your story holds up against data. But ‚Äî and this is crucial ‚Äî the DGP is always an assumption. If it is wrong, the inference may be misleading.\n\n\nThe Machine Learning World\nNow comes the other world. The algorithmic modeling culture. Here, the hidden machine is ignored. No one asks whether wages are ‚Äútruly linear‚Äù in education, or whether prices follow a Poisson law, or whether the dice is probabilistic. Instead, the question is brutally pragmatic: Can I predict well?\nIn this world, the central drama is not about whether the stochastic story is correct, but about whether a model trained on one dataset (training set) generalizes to unseen data (the test set). This is the training vs.¬†testing problem.\nThe classical core risks are:\n- Underfitting: the model is too simple, missing patterns (high bias).\n- Overfitting: the model is too complex, capturing noise instead of signal (high variance).\nSuccess here is measured by predictive accuracy, not by unbiasedness or efficiency of estimators, like in the Statistics world. Interpretability is optional (if you don‚Äôt know what is inside a black box, you cannot ‚Äúread‚Äù it and ‚Äúinterpret‚Äù it); performance is everything. Predictions may work well in stable environments but fail when the underlying data distribution shifts.\n\nA concrete example makes the distinction clearer ‚Äî and shows how each culture fails where the other succeeds:\n\nEstimating the causal effect of education on wages.\nIn econometrics and statistics, I would model wages as generated by a stochastic mechanism involving education, experience, and other covariates. My goal is to isolate the effect of education from confounders. If I only used a machine learning model, it might predict wages very well ‚Äî but it would not tell me the causal role of education (maybe it is not education itself that raises wages, but family wealth that enables both schooling and better jobs), because prediction does not separate correlation from causation. ML excels at prediction, but fails to deliver interpretability and identification here.\nPredicting house prices.\nEconometrics might specify a linear model with a few chosen variables and assumptions about errors. But this DGP is likely too simplistic: house prices depend on thousands of complex, nonlinear factors (location, amenities, school ratings, neighborhood trends). An econometric model here may fail badly in predictive accuracy. By contrast, ML models like gradient boosting can capture these nonlinearities and achieve much higher predictive power ‚Äî even though they may not explain why prices are high.\n\nSo each culture shines in one domain and struggles in the other. Together, they suggest the need for a synthesis.\n\nCausal ML in Action\nA famous example comes from Online Advertising.\n- A standard supervised ML model might predict that showing more ads increases purchases, because ads and purchases are positively correlated in the data.\n- But when economists ran randomized experiments, they found the causal effect was often much smaller ‚Äî sometimes zero. Why? Because people who were already likely to buy were also the ones most likely to be targeted with ads.\nThis is a case where prediction looked strong, but causality revealed the true relationship and new decisions had to be made.\n\n\n\n\nWhy Causal ML Is a Synthesis\n‚ú® This is why Causal Machine Learning excites me.\nBy combining machine learning with the discipline of causal inference, researchers such as Victor Chernozhukov, Christian Hansen, Nathan Kallus, Martin Spindler, and Vasilis Syrgkanis ‚Äî co-authors of Causal ML: Applied Causal Inference Powered by ML and AI ‚Äî show that we can take the best of both worlds.\nThe basic idea is simple:\n- From machine learning, we borrow flexible algorithms that can handle many variables and complex patterns.\n- From statistics and econometrics, we borrow tools that check whether we are uncovering cause-and-effect rather than just correlations.\nTechniques like sample splitting and cross-fitting might sound technical, but the intuition is clear: we let the algorithm ‚Äúlearn‚Äù patterns on one part of the data, and we keep another part untouched to see whether those patterns still hold when drawing causal conclusions. This helps us avoid fooling ourselves with spurious results.\nCausal thinking also pushes us beyond the tidy assumption that all observations are IID (independent and identically distributed). Real life changes: policies shift, markets move, and people adapt. Causal inference gives us language to reason about such changes ‚Äî about interventions (‚Äúwhat if we changed X?‚Äù), counterfactuals (‚Äúwhat would have happened otherwise?‚Äù), and distribution shifts (‚Äúwhat if tomorrow doesn‚Äôt look like yesterday?‚Äù).\nCausal ML suggests a synthesis:\n- From statistics and econometrics, we inherit the concern with stochastic mechanisms, identification, and rigorous inference.\n- From machine learning, we gain flexibility, robustness, and predictive accuracy.\n- From causality, we add the language to generalize beyond observation, to interventions and explanations.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Why I'm Interested in Causal ML?</span>"
    ]
  },
  {
    "objectID": "chapters/00_ch00.html#focus",
    "href": "chapters/00_ch00.html#focus",
    "title": "Why I‚Äôm Interested in Causal ML?",
    "section": "Focus",
    "text": "Focus\nMy main focus in this project is to learn ‚Äî and become comfortable with ‚Äî the mathematical foundations that underpin Causal Machine Learning.\nWhile Causal ML is an exciting and active field (and may well be my future research direction), the broader point is that mathematical depth makes a difference. Too often, methods are applied skillfully but without a full grasp of what is happening underneath.\nBy working through the logical and mathematical foundations step by step, I aim to develop the clarity that allows not only the application of tools, but a genuine understanding of them. The goal is to bridge the cultures of explanation and prediction ‚Äî and, in some small way, contribute to the ongoing effort to unify them.\nWhat ‚Äúfoundations‚Äù means here\n- Logic & Set Theory ‚Äî the language of proofs and structures.\n- Real Analysis ‚Äî rigor of limits, continuity, convergence, integration.\n- Linear Algebra ‚Äî vector spaces, eigen-structure, decompositions.\n- Functional Analysis & Hilbert Spaces ‚Äî norms, projections, RKHS.\n- Topology & Measure Theory ‚Äî œÉ-algebras, Lebesgue integration, convergence theorems.\n- Probability ‚Äî Kolmogorov framework, LLN/CLT, conditional expectation.\n- Mathematical Statistics ‚Äî estimation, tests, asymptotics.\n- Causality ‚Äî SCMs, potential outcomes, identifiability, modern Causal ML.",
    "crumbs": [
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Why I'm Interested in Causal ML?</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html",
    "href": "chapters/01_ch01.html",
    "title": "Phase 1: Logic & Set Theory",
    "section": "",
    "text": "0. Mathematics, Axioms, and Science\nMathematics begins with axioms ‚Äî assumptions we agree to accept without proof ‚Äî and builds everything else from them using precise rules of logic.\nThis process of axiomatization is what gives mathematics its unique clarity: starting from simple, explicit principles, we construct entire theories.\nWhy is this important?\n- Axioms are not ‚Äútrue‚Äù in an absolute sense. They are starting points.\n- What matters is whether the consequences drawn from them are logically consistent and useful for understanding the world.\n- In practice, this makes mathematics a language in which science can express and test ideas.\nMathematics provides the rigorous framework for turning assumptions into predictions.\nFor example, in Euclidean geometry we assume as an axiom:\nBut this depends on the ‚Äúspace‚Äù we are working in.\nIntuitively:\n- if you fly from New York to Paris, the shortest route is not a line of latitude on a flat map, but rather a curved arc of a great circle ‚Äî airplanes follow these routes because they minimize distance. - the airplane could in principle fly in a perfectly straight line ‚Äî but only if it were allowed to go through the Earth‚Äôs interior. Since it‚Äôs constrained to stay on the surface of the sphere, the ‚Äústraight line‚Äù within that space becomes a geodesic (the great circle).\nNow consider two points on the equator that are exactly opposite each other (say Rio de Janeiro and Jakarta).\n- There isn‚Äôt a unique great circle through them ‚Äî in fact, infinitely many great circles (all meridians) pass through both points.\n- So the Euclidean axiom fails in this curved space.\nThe key idea is that ‚Äústraight line‚Äù really means shortest possible path given the geometry of the space.\nOn a plane, that‚Äôs the familiar straight line; on a sphere, it‚Äôs a great circle.\nThe notion of a geodesic generalizes this idea: whatever the space, it tells us the natural way to connect two points as efficiently as possible.\nThis illustrates a key point:\nThey are conventions that define the system we are working in.\nWithin their proper context (e.g., Euclidean geometry), their consequences are logically consistent (no contradictions arise) and useful (for designing buildings, maps, and bridges). But in other contexts (e.g., spherical geometry), we need a different set of axioms.\nIn the same way, in probability we may assume:\nReal-world data often violates this (dependence in time series, non-identical distributions in heterogeneous populations).\nBut within the i.i.d. framework, the consequences are logically consistent and extremely useful: they give us fundamental results like the Law of Large Numbers and the Central Limit Theorem.\nThe analogy with geometry is this:\n- In flat Euclidean space, the ‚Äústraight line‚Äù is the natural geodesic.\n- On a sphere, the ‚Äústraight line‚Äù is reinterpreted as a great circle.\nSimilarly, in probability:\n- For many problems, assuming i.i.d. is the natural starting point (the ‚Äústraight line‚Äù of probability).\n- In more complex contexts (time series, causal inference), we need to redefine the structure (like choosing geodesics on a sphere) with weaker or different axioms ‚Äî e.g., mixing conditions, stationarity, or potential outcomes.\nWhy we begin with Logic and Set Theory\nTogether, Logic and Set Theory form the grammar and vocabulary of mathematics.\nThey are not yet ‚Äúabout the real world‚Äù ‚Äî but they give us the precise tools to state assumptions and derive consequences.\nIntuition for research\nWhen you see theoretical work in physics, economics, or causal machine learning, it often starts by axiomatizing the problem:\n- Define the objects (e.g.¬†random variables, causal spaces).\n- Specify assumptions as axioms (e.g.¬†independence, stability, interventions).\n- Derive results rigorously from there.\nBy starting here, we are learning how to speak the language of mathematics before applying it to inference, probability, and causality.\nThus, before diving into analysis and probability, we establish a foundation in logic and set theory. This is important to formalize assumptions, express mathematical objects precisely, and build proofs with rigor.\nLogic gives us the language to connect premises and conclusions, while Set Theory gives us the structure to define universes of discourse, events, functions, probability spaces, etc.\nTherefore: Yes, it would be essential to learn Logic and Set Theory!",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html#statements-propositions-predicates-and-connectives",
    "href": "chapters/01_ch01.html#statements-propositions-predicates-and-connectives",
    "title": "Phase 1: Logic & Set Theory",
    "section": "1.1 Statements, Propositions, Predicates, and Connectives",
    "text": "1.1 Statements, Propositions, Predicates, and Connectives\nüí°Motivation\nLearning statements and connectives is like learning the alphabet of mathematics.\n- If you cannot distinguish valid statements, you cannot even start a proof.\nExample in causal inference:\n- \\(p\\): ‚ÄúTreatment is randomized.‚Äù\n- \\(q\\): ‚ÄúIgnorability holds.‚Äù\n- Then ‚ÄúIf treatment is randomized, then ignorability holds‚Äù is \\(p \\to q\\).\nWithout connectives, we‚Äôd stay in informal language. With them, we can formalize statements and reason rigorously about consequences, such as proving that a set of assumptions implies consistency of an estimator, showing that ignorability implies identification of a treatment effect, or demonstrating that conditional independence leads to factorization of a probability distribution into simpler components.\nA Statement (or Proposition) is a declarative sentence that is either true or false.\n\nExample: ‚Äú3 is even‚Äù (false), ‚ÄúBarcelona is in Spain‚Äù (true).\n\nA Predicate is like a ‚Äútemplate‚Äù for a statement: it depends on a variable and becomes a statement once you specify the value. For example:\n\\(P(x): x &gt; 0\\)\n- \\(P(2)\\) ‚Üí ‚Äú2 &gt; 0‚Äù (true).\n- \\(P(-1)\\) ‚Üí ‚Äú-1 &gt; 0‚Äù (false).\nConnectives let us combine assumptions systematically. Logical connectives let us build compound statements:\n\nNegation: \\(\\lnot p\\) (‚Äúnot \\(p\\)‚Äù)\n\nConjunction: \\(p \\land q\\) (‚Äú\\(p\\) and \\(q\\)‚Äù)\n\nDisjunction: \\(p \\lor q\\) (‚Äú\\(p\\) or \\(q\\)‚Äù)\n\nConditional: \\(p \\to q\\) (‚Äúif \\(p\\) then \\(q\\)‚Äù)\n\nBiconditional: \\(p \\leftrightarrow q\\) (‚Äú\\(p\\) if and only if \\(q\\)‚Äù)\n\nThus:\n- Predicate: general template (open sentence, truth depends on a variable) -&gt; becomes true or false only when a variable is given a value.\n- Proposition/statement: instance of that template (closed sentence, definite truth) -&gt; something that is already true or false.\n- Connectives: operators that take simple propositions and form compound propositions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html#truth-tables",
    "href": "chapters/01_ch01.html#truth-tables",
    "title": "Phase 1: Logic & Set Theory",
    "section": "1.2 Truth Tables",
    "text": "1.2 Truth Tables\nüí°Motivation Truth tables are the grammar checker of logic.\n- They allow us to test whether two statements are equivalent (so we can swap one for another in a proof, probably to make life easier).\n- They reveal tautologies (always true) and contradictions (always false).\n- They give a mechanical way to check validity of deductive arguments.\nExample in statistics:\n- ‚ÄúIf the data behave nicely (i.i.d. + finite variance), then the sample mean is reliable (it will converge to the true mean)‚Äù (\\(p \\to q\\)).\n- Equivalent contrapositive: ‚ÄúIf effect is not identifiable, then ignorability does not hold‚Äù (\\(\\lnot q \\to \\lnot p\\)).\n- Truth tables prove these are the same, so you can flip perspectives safely in a paper.\nA truth table shows how the truth value of a compound statement depends on its parts.\nExample: Prove that an implication (\\(p \\to q\\)) is equivalent (\\(\\equiv\\)) to its contrapositive (\\(\\lnot q \\to \\lnot p\\)).\nWe want to show: \\(p \\to q \\;\\equiv\\; \\lnot q \\to \\lnot p\\)\n\n\n\n\n\n\n\n\n\n\n\n\\(p\\)\n\\(q\\)\n\\(p \\to q\\)\n\\(\\lnot q\\)\n\\(\\lnot p\\)\n\\(\\lnot q \\to \\lnot p\\)\n\n\n\n\nT\nT\nT\nF\nF\nT\n\n\nT\nF\nF\nT\nF\nF\n\n\nF\nT\nT\nF\nT\nT\n\n\nF\nF\nT\nT\nT\nT\n\n\n\nSince the last two columns match, the implication is equivalent to its contrapositive.\nThe Basic Connectives\nLogical connectives are rules for combining simpler statements into compound ones.\nHere are the five most common and some intuition:\n\nNegation: \\(\\lnot p\\) (‚Äúnot \\(p\\)‚Äù)\n\nRule: Negation flips the truth value.\n\nIf \\(p\\) is true, \\(\\lnot p\\) is false.\n\nIf \\(p\\) is false, \\(\\lnot p\\) is true.\n\nExample:\n\\(p\\): ‚ÄúIt is raining.‚Äù\n\\(\\lnot p\\): ‚ÄúIt is not raining.‚Äù\n\n\nConjunction: \\(p \\land q\\) (‚Äú\\(p\\) and \\(q\\)‚Äù)\n\nRule: \\(p \\land q\\) is true only if both \\(p\\) and \\(q\\) are true.\nExample:\n\\(p\\): ‚ÄúIt is raining.‚Äù\n\\(q\\): ‚ÄúI am carrying an umbrella.‚Äù\n\\(p \\land q\\): ‚ÄúIt is raining and I am carrying an umbrella.‚Äù\nTruth check: If either part fails, the whole conjunction is false.\n\n\nDisjunction: \\(p \\lor q\\) (‚Äú\\(p\\) or \\(q\\)‚Äù)\n\nRule: \\(p \\lor q\\) is true if at least one of \\(p, q\\) is true.\n(This is the inclusive or used in logic.)\nExample:\n\\(p\\): ‚ÄúI will drink coffee.‚Äù\n\\(q\\): ‚ÄúI will drink tea.‚Äù\n\\(p \\lor q\\): ‚ÄúI will drink coffee or tea (or both).‚Äù\nNote: In everyday language, ‚Äúor‚Äù can be exclusive. Logic defaults to inclusive.\n\n\nConditional: \\(p \\to q\\) (‚Äúif \\(p\\) then \\(q\\)‚Äù)\n\nRule: An implication is false only when \\(p\\) is true and \\(q\\) is false.\nIn all other cases, it is true.\nEquivalent form:\n\\[\np \\to q \\;\\equiv\\; \\lnot p \\lor q\n\\]\nExample:\n\\(p\\): ‚ÄúIt rains.‚Äù\n\\(q\\): ‚ÄúThe ground is wet.‚Äù\n\\(p \\to q\\): ‚ÄúIf it rains, then the ground is wet.‚Äù\nCase analysis:¬†\n\nIf it rains and the ground is wet ‚Üí the statement ‚ÄúIf it rains, then the ground is wet‚Äù has been kept. Both the condition and the consequence hold, so the implication is true.\n\nIf it rains but the ground is not wet ‚Üí the statement has been broken. This is the only case where an implication is false: the condition was met but the promised result failed.\n\nIf it doesn‚Äôt rain ‚Üí the statement never gets a chance to be tested. We cannot accuse it of being false, because the condition (‚Äúit rains‚Äù) never happened. By definition, logic treats this as vacuously true: the promise has not been broken, since there was nothing to check.\n\nIf it doesn‚Äôt rain and the ground is wet ‚Üí still vacuously true. The implication didn‚Äôt say what should happen when it doesn‚Äôt rain; the ground being wet for other reasons (sprinklers, a bucket of water, etc.) doesn‚Äôt violate the promise.\n\n\nIntuition and goal of the conditional\nThe statement \\(p \\to q\\) is read ‚Äúif \\(p\\) holds then \\(q\\) holds‚Äù or even ‚Äúif \\(p\\) is true then \\(q\\) is true.‚Äù¬† At first glance, this seems strange because we must also handle cases when \\(p\\) or \\(q\\) are false.¬† Why not just say it means ‚Äúboth \\(p\\) and \\(q\\) are true‚Äù?\nThe key is that an implication is really a promise or rule:\n- ‚ÄúWhenever \\(p\\) happens, \\(q\\) must also happen.‚Äù\nSo we only judge the statement in the situations where the promise could actually be tested: when \\(p\\) is true.\n\nIf \\(p\\) is true and \\(q\\) is true ‚Üí the promise is kept ‚Üí the implication is true.\n\nIf \\(p\\) is true and \\(q\\) is false ‚Üí the promise is broken ‚Üí the implication is false.\n\nBut if \\(p\\) is false, the situation that was promised never arises. In those cases, the rule is not violated. By convention (and to make logical systems consistent), we treat the implication as vacuously true whenever \\(p\\) is false.\nThis explains why the truth table looks the way it does:\n\n\n\n\\(p\\)\n\\(q\\)\n\\(p \\to q\\)\nExplanation\n\n\n\n\nT\nT\nT\npromise kept\n\n\nT\nF\nF\npromise broken\n\n\nF\nT\nT\nvacuously true (condition never triggered)\n\n\nF\nF\nT\nvacuously true (condition never triggered)\n\n\n\nWhy is this important?\nUnderstanding the conditional matters because:\n\nIt allows us to formalize logical rules like the contrapositive:\n\\[\np \\to q \\;\\equiv\\; \\lnot q \\to \\lnot p\n\\] which is central in proofs.\nIt prevents confusion when reading theorems:\n\n‚ÄúIf a sequence converges, then it is bounded‚Äù (\\(p \\to q\\)).\n\nThis is not claiming that all bounded sequences converge; the truth table guarantees the direction of the promise is clear.\n\nIt highlights vacuous truth, which appears everywhere in math:\n\n‚ÄúAll unicorns have horns‚Äù is technically true, because there are no unicorns to provide a counterexample.\n\nSimilarly, in probability, if an event has probability zero, conditional statements given that event can be vacuously true.\n\n\nBy appreciating this structure, the reader sees why the conditional is defined with its somewhat surprising truth table: it captures the idea of a promise that can only be broken in one very specific case.\n\nBiconditional: \\(p \\leftrightarrow q\\) (‚Äú\\(p\\) if and only if \\(q\\)‚Äù)\n\nRule: \\(p \\leftrightarrow q\\) is true exactly when \\(p\\) and \\(q\\) have the same truth value\n(both true or both false).\nEquivalent form:\n\\[\np \\leftrightarrow q \\;\\equiv\\; (p \\to q) \\land (q \\to p)\n\\]\nExample:\n\\(p\\): ‚ÄúToday is Saturday.‚Äù\n\\(q\\): ‚ÄúTomorrow is Sunday.‚Äù\n\\(p \\leftrightarrow q\\): ‚ÄúToday is Saturday if and only if tomorrow is Sunday.‚Äù\nCase analysis:\n\nIf both \\(p\\) and \\(q\\) are true ‚Üí the biconditional is true (both directions of the promise hold).\n\nIf \\(p\\) is true but \\(q\\) is false ‚Üí false, because one direction of the ‚Äúif and only if‚Äù fails.\n\nIf \\(p\\) is false but \\(q\\) is true ‚Üí false, for the same reason.\n\nIf both \\(p\\) and \\(q\\) are false ‚Üí true, because they match in value (both false).\n\n\nThis explains why the truth table looks like this:\n\n\n\n\\(p\\)\n\\(q\\)\n\\(p \\leftrightarrow q\\)\nExplanation\n\n\n\n\nT\nT\nT\nboth true ‚Üí promise kept\n\n\nT\nF\nF\nmismatch ‚Üí one direction fails\n\n\nF\nT\nF\nmismatch ‚Üí one direction fails\n\n\nF\nF\nT\nboth false ‚Üí they match\n\n\n\nIntuition and goal of the biconditional\nThe biconditional expresses equivalence: \\(p\\) and \\(q\\) ‚Äústand or fall together.‚Äù\nIt is stronger than a one-way implication: both \\(p \\to q\\) and \\(q \\to p\\) must hold.\n\nIf you read \\(p \\leftrightarrow q\\) aloud, it means:\n‚Äú\\(p\\) is true exactly when \\(q\\) is true.‚Äù or ‚Äú\\(p\\) holds exactly when \\(q\\) holds.‚Äù\n\nThis is why mathematicians often use ‚Äúiff‚Äù (‚Äúif and only if‚Äù) in definitions and theorems:\n- It guarantees not only that \\(p\\) implies \\(q\\), but also that \\(q\\) implies \\(p\\).\n\nWhy is this important?\n\nIt formalizes definitions in mathematics.\n\nExample: ‚ÄúA number \\(n\\) is even iff \\(n = 2k\\) for some integer \\(k\\).‚Äù\n\nThis captures both directions: every even number has that form, and every number of that form is even.\n\nIt allows us to state equivalence theorems.\n\nExample: ‚ÄúA sequence is Cauchy iff it is convergent (in \\(\\mathbb{R}\\)).‚Äù\n\nThe biconditional captures the deep connection: each property implies the other.\n\nIt makes reasoning reversible.\n\nWith an implication, you can only go forward (\\(p \\to q\\)).\n\nWith a biconditional, you can go forward and backward: knowing either \\(p\\) or \\(q\\) tells you the other.\n\n\nBy mastering the biconditional, the reader understands why mathematicians love the phrase ‚Äúif and only if‚Äù: it‚Äôs the precise way of stating true equivalence between concepts.\nSummary Table of Connectives\n\n\n\n\n\n\n\n\nConnective\nSymbol\nRule (when true)\n\n\n\n\nNegation\n\\(\\lnot p\\)\nwhen \\(p\\) is false\n\n\nConjunction\n\\(p \\land q\\)\nwhen \\(p\\) and \\(q\\) are true\n\n\nDisjunction\n\\(p \\lor q\\)\nwhen at least one of \\(p, q\\) is true\n\n\nConditional\n\\(p \\to q\\)\nfalse only if \\(p\\) true and \\(q\\) false\n\n\nBiconditional\n\\(p \\leftrightarrow q\\)\nwhen \\(p\\) and \\(q\\) have same truth value",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html#tautologies-contradictions-and-logical-equivalence",
    "href": "chapters/01_ch01.html#tautologies-contradictions-and-logical-equivalence",
    "title": "Phase 1: Logic & Set Theory",
    "section": "1.3 Tautologies, Contradictions, and Logical Equivalence",
    "text": "1.3 Tautologies, Contradictions, and Logical Equivalence\nTautology\nDefinition:\nA tautology is a statement that is true in all possible cases.\nWhy it matters:\n- Tautologies act like universal truths: they don‚Äôt depend on data or assumptions.\n- They are often the ‚Äúglue‚Äù of proofs, showing that certain forms are always valid.\n- Many rules of inference (like modus ponens) are based on tautologies.\nExample (logic):\n\\[\n(p \\land q) \\to p\n\\]\nThis means: If both (p) and (q) are true, then (p) is true.\n- Always true, regardless of whether (p) or (q) are true or false.\nExample (statistics):\nThe Law of Total Probability is tautological:\n\\[\nP(A) = P(A \\cap B) + P(A \\cap \\lnot B).\n\\]\nThis identity always holds by construction, no matter what events (A) and (B) are.\n\nContradiction\nDefinition:\nA contradiction is a statement that is false in all possible cases.\nWhy it matters:\n- Contradictions are the engine of proof by contradiction.\n- If assuming something leads to a contradiction, then the assumption must be false.\n- They represent ‚Äúimpossible situations‚Äù in logic.\nExample (logic):\n\\[\np \\land \\lnot p\n\\]\nThis means: (p) is true and (p) is false at the same time.\n- Always false, no matter what truth value (p) has.\nExample (statistics):\nSuppose we assume:\n1. ‚ÄúThe variance of this distribution is finite.‚Äù\n2. ‚ÄúThe variance of this distribution is infinite.‚Äù\nTogether, these form a contradiction, so at least one assumption must be wrong.\n\nLogical Equivalence\nDefinition:\nTwo statements are logically equivalent if they have the same truth value in all possible cases.\nWhy it matters:\n- Logical equivalence lets us replace one statement with another in a proof.\n- Many powerful proof strategies rely on equivalence (contrapositive law, De Morgan‚Äôs laws, distributive laws).\n- Often the equivalent form is much easier to work with.\nExample (logic):\n\\[\np \\to q \\;\\equiv\\; \\lnot p \\lor q\n\\]\nThis means: ‚ÄúIf (p), then (q)‚Äù is the same as ‚ÄúEither not (p), or (q).‚Äù\n- This equivalence makes it easier to manipulate conditionals in proofs.\nExample (causal inference):\n\\[\n\\text{Ignorability} \\to \\text{Identifiability}\n\\]\nis logically equivalent to\n\\[\n\\lnot \\text{Identifiability} \\to \\lnot \\text{Ignorability}.\n\\]\nSwitching to the contrapositive often makes a proof or argument simpler.\n\nSummary:\n- Tautologies give us universal truths to rely on.\n- Contradictions allow us to eliminate false assumptions through contradiction proofs.\n- Logical equivalence lets us restate problems in easier forms without changing meaning.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html#predicates-and-quantifiers",
    "href": "chapters/01_ch01.html#predicates-and-quantifiers",
    "title": "Phase 1: Logic & Set Theory",
    "section": "2.1 Predicates and Quantifiers",
    "text": "2.1 Predicates and Quantifiers\nAs we saw above, a predicate is like a sentence with a ‚Äúblank‚Äù ‚Äî it becomes a full statement only once you plug in a value.\n\nExample: \\(P(x): x &gt; 0\\).\n\nIf \\(x = 2\\), then \\(P(2)\\) is the proposition ‚Äú2 &gt; 0‚Äù (true).\n\nIf \\(x = -3\\), then \\(P(-3)\\) is the proposition ‚Äú-3 &gt; 0‚Äù (false).\n\n\nWe use quantifiers to talk about how many elements satisfy a predicate:\n\nUniversal quantifier (\\(\\forall\\)):\n\\(\\forall x\\; P(x)\\) means ‚Äúfor all \\(x\\), \\(P(x)\\) is true.‚Äù\nExistential quantifier (\\(\\exists\\)):\n\\(\\exists x\\; P(x)\\) means ‚Äúthere exists at least one \\(x\\) such that \\(P(x)\\) is true.‚Äù\n\nExamples:\n- \\(\\forall x \\in \\mathbb{Z},\\; x^2 \\geq 0\\). (Every integer squared is nonnegative.)\n- \\(\\exists x \\in \\mathbb{Z},\\; x^2 = 9\\). (There exists an integer whose square is 9.)",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html#universe-of-discourse",
    "href": "chapters/01_ch01.html#universe-of-discourse",
    "title": "Phase 1: Logic & Set Theory",
    "section": "2.2 Universe of Discourse",
    "text": "2.2 Universe of Discourse\nThe universe of discourse is the set of objects we allow \\(x\\) to vary over.\nThe truth of a statement depends on it!\nExample:\n\n\\(\\forall x \\in \\mathbb{R},\\; x^2 \\geq 0\\) is true.\n\n\\(\\forall x \\in \\mathbb{Z},\\; x^2 = 2\\) is false (no integer squared equals 2).\n\nIf we didn‚Äôt specify whether \\(x\\) ranges over \\(\\mathbb{R}\\) or \\(\\mathbb{Z}\\) (both interpreted as the universe of discourse of each statement), the meaning would be ambiguous.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html#truth-of-quantified-statements",
    "href": "chapters/01_ch01.html#truth-of-quantified-statements",
    "title": "Phase 1: Logic & Set Theory",
    "section": "2.3 Truth of Quantified Statements",
    "text": "2.3 Truth of Quantified Statements\nHow to evaluate quantified statements:\n\n\\(\\forall x\\; P(x)\\) is true if every \\(x\\) in the universe makes \\(P(x)\\) true.\n\n\\(\\exists x\\; P(x)\\) is true if at least one \\(x\\) makes \\(P(x)\\) true.\n\nNegations of Quantifiers\nNegating quantified statements flips the quantifier:\n\\[\n\\lnot (\\forall x\\, P(x)) \\equiv \\exists x\\, \\lnot P(x)\n\\]\n\\[\n\\lnot (\\exists x\\, P(x)) \\equiv \\forall x\\, \\lnot P(x)\n\\]\nExamples:\n\n‚ÄúNot all students passed‚Äù means ‚ÄúThere exists a student who did not pass.‚Äù\n\n‚ÄúThere does not exist a unicorn‚Äù means ‚ÄúFor all \\(x\\), \\(x\\) is not a unicorn.‚Äù",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html#multiple-quantifiers",
    "href": "chapters/01_ch01.html#multiple-quantifiers",
    "title": "Phase 1: Logic & Set Theory",
    "section": "2.4 Multiple Quantifiers",
    "text": "2.4 Multiple Quantifiers\nOften statements involve more than one quantifier.\nThe order matters!\n\n\\(\\forall x \\in \\mathbb{R},\\; \\exists y \\in \\mathbb{R}: y &gt; x\\)\n‚Üí True, because for every real number \\(x\\), we can pick \\(y = x+1\\).\n\\(\\exists y \\in \\mathbb{R},\\; \\forall x \\in \\mathbb{R}: y &gt; x\\)\n‚Üí False, because no single real number is greater than all real numbers.\n\nTip: Think of quantifiers as a kind of game:\n- For \\(\\forall x\\), your opponent chooses the worst possible \\(x\\).\n- For \\(\\exists y\\), you get to respond by picking a suitable \\(y\\).\nThe order decides who gets to ‚Äúmove‚Äù first, and the outcome can change completely.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html#why-this-matters",
    "href": "chapters/01_ch01.html#why-this-matters",
    "title": "Phase 1: Logic & Set Theory",
    "section": "2.5 Why This Matters",
    "text": "2.5 Why This Matters\nQuantifiers appear in almost every mathematical theorem.\n\nAnalysis (limits):\n\\[\n\\forall \\epsilon &gt; 0,\\; \\exists \\delta &gt; 0:\\; |x - a| &lt; \\delta \\;\\to\\; |f(x) - L| &lt; \\epsilon\n\\]\n(‚ÄúFor every tolerance \\(\\epsilon\\), there exists a closeness \\(\\delta\\) that guarantees the function stays within that tolerance.‚Äù)\nStatistics:\n\n\\(\\forall n,\\; \\exists \\hat{\\theta}_n:\\; \\hat{\\theta}_n \\to \\theta\\) (There exists an estimator consistent for \\(\\theta\\).)\n\n\\(\\exists\\) an unbiased estimator of \\(\\mu\\) (the sample mean).\n\nCausal Inference:\n\n\\(\\forall\\) randomized experiments, \\(\\exists\\) an unbiased estimator of the treatment effect.\n\n\nQuantifiers are the way mathematics formalizes sweeping claims like ‚Äúalways‚Äù and ‚Äúsometimes,‚Äù which are at the heart of proofs and assumptions in Causal ML.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html#mathematical-interpretation",
    "href": "chapters/01_ch01.html#mathematical-interpretation",
    "title": "Phase 1: Logic & Set Theory",
    "section": "2.6 Mathematical Interpretation",
    "text": "2.6 Mathematical Interpretation\nQuantifiers can look intimidating at first, but the real skill is learning how to read them.\nEvery quantified statement has two parts:\n1. the quantifier (\\(\\forall\\) or \\(\\exists\\)), and\n2. the predicate (a property of \\(x\\) that is claimed to hold).\n\n2.6.1 Single Quantifier Examples\n\nUniversal (\\(\\forall\\)):\n\\[\n\\forall x \\in \\mathbb{R},\\; x^2 \\geq 0\n\\]\nRead: ‚ÄúFor every real number \\(x\\), the square of \\(x\\) is nonnegative.‚Äù\nInterpretation: This is true, because no real number squared gives a negative result.\nStatistics example:\n\\[\n\\forall n \\in \\mathbb{N},\\; \\operatorname{Var}(\\bar{X}_n) \\geq 0\n\\]\nRead: ‚ÄúFor every sample size \\(n\\), the variance of the sample mean is nonnegative.‚Äù\nInterpretation: Always true, because variances can never be negative.\n\n\n\nExistential (\\(\\exists\\)):\n\\[\n\\exists x \\in \\mathbb{Z},\\; x^2 = 4\n\\]\nRead: ‚ÄúThere exists an integer whose square is 4.‚Äù\nInterpretation: True, since \\(x = 2\\) and \\(x = -2\\) work.\nStatistics example:\n\\[\n\\exists \\;\\text{an estimator } \\hat{\\theta}\\; : \\; \\mathbb{E}[\\hat{\\theta}] = \\theta\n\\]\nRead: ‚ÄúThere exists an estimator whose expected value equals the true parameter.‚Äù\nInterpretation: This is the definition of an unbiased estimator (e.g., the sample mean for \\(\\mu\\)).\n\n\n2.6.2 Multiple Quantifier Examples\nWhen quantifiers are combined, order matters.\n\nExample 1:\n\\[\n\\forall x \\in \\mathbb{R},\\; \\exists y \\in \\mathbb{R}:\\; y &gt; x\n\\]\nRead: ‚ÄúFor every real number \\(x\\), there exists a real number \\(y\\) that is greater than \\(x\\).‚Äù\nTrue, because if someone hands you any \\(x\\), you can always respond with \\(y = x+1\\).\nStatistics example:\n\\[\n\\forall \\epsilon &gt; 0,\\; \\exists N \\in \\mathbb{N}:\\; n &gt; N \\;\\to\\; |\\bar{X}_n - \\mu| &lt; \\epsilon\n\\]\nRead: ‚ÄúFor every tolerance \\(\\epsilon\\), there exists a large enough sample size \\(N\\) such that if \\(n &gt; N\\), the sample mean is within \\(\\epsilon\\) of \\(\\mu\\).‚Äù\nInterpretation: This is the definition of consistency (Law of Large Numbers).\n\n\n\nExample 2:\n\\[\n\\exists y \\in \\mathbb{R},\\; \\forall x \\in \\mathbb{R}:\\; y &gt; x\n\\]\nRead: ‚ÄúThere exists a real number \\(y\\) such that \\(y\\) is greater than every real number \\(x\\).‚Äù\nFalse, because no single real number is larger than all others.\nStatistics example (false statement):\n\\[\n\\exists N \\in \\mathbb{N},\\; \\forall n &gt; N:\\; \\bar{X}_n = \\mu\n\\]\nRead: ‚ÄúThere exists a finite sample size \\(N\\) such that for all \\(n &gt; N\\), the sample mean equals the population mean exactly.‚Äù\nFalse, because sampling variation never completely disappears ‚Äî the sample mean only converges in probability, not with exact equality at some \\(N\\).\n\n\n2.6.3 How to Think About Multiple Quantifiers\nA useful way to think is as a game:\n\n\\(\\forall x\\) = your opponent picks a value of \\(x\\), possibly trying to make you fail.\n\n\\(\\exists y\\) = you get to respond by picking \\(y\\) to satisfy the condition.\n\nSo the statement\n\\[\n\\forall x \\in \\mathbb{R},\\; \\exists y \\in \\mathbb{R}:\\; y &gt; x\n\\]\nmeans: No matter what \\(x\\) your opponent picks, you can always respond with a suitable \\(y\\).\nBut the reverse order\n\\[\n\\exists y \\in \\mathbb{R},\\; \\forall x \\in \\mathbb{R}:\\; y &gt; x\n\\]\nmeans: You must pick one \\(y\\) that beats all possible \\(x\\). This is impossible, so the statement is false.\n\nWhy this section is important\nQuantifiers are everywhere in math, stats, and causal ML.\n\nUniversal quantifiers express generality:\n\n‚ÄúFor all sample sizes \\(n\\), \\(\\operatorname{Var}(\\bar{X}_n) \\geq 0\\).‚Äù\n\n‚ÄúFor all \\(\\epsilon &gt; 0\\), there exists an \\(N\\) such that ‚Ä¶‚Äù (limits, consistency).\n\nExistential quantifiers express possibility:\n\n‚ÄúThere exists an unbiased estimator of \\(\\mu\\).‚Äù\n\n‚ÄúThere exists a consistent estimator for every parameter.‚Äù\n\nWith multiple quantifiers, the order of ‚Äòwho chooses first‚Äô changes the meaning dramatically.\nThis interpretative skill is essential for reading theorems correctly and avoiding misinterpretation.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html#exercises",
    "href": "chapters/01_ch01.html#exercises",
    "title": "Phase 1: Logic & Set Theory",
    "section": "2.7 Exercises",
    "text": "2.7 Exercises\nGoal of these exercises:\n- Practice evaluating truth values.\n- Practice negating quantified statements.\n- Practice reading and interpreting symbolic logic in plain English.\nAt this stage, we are not proving statements ‚Äî only learning to understand and translate them correctly.\n\nTruth values (universe of discourse: \\(\\mathbb{Z}\\)):\nFor each statement, decide whether it is true or false and explain why in words.\n\n\\(\\forall x,\\; x^2 \\geq 0\\)\n\n\\(\\exists x,\\; x^2 = 2\\)\n\nHint: In the first, think: ‚ÄúIs there any integer whose square is negative?‚Äù\nIn the second, think: ‚ÄúIs there an integer whose square equals 2?‚Äù\n\n\n\nNegation practice:\nWrite the logical negation of each statement and simplify.\n\n\\(\\forall x \\in \\mathbb{R},\\; x^2 \\geq 0\\)\n\n\\(\\exists x \\in \\mathbb{N},\\; x^2 = 2\\)\n\nHint: Use the rules:\n\\[\n\\lnot (\\forall x\\, P(x)) \\equiv \\exists x\\, \\lnot P(x), \\qquad\n\\lnot (\\exists x\\, P(x)) \\equiv \\forall x\\, \\lnot P(x).\n\\]\n\n\n\nQuantifier order:\nCarefully interpret the following statements in plain English.\nAre they true or false?\n\n\\(\\forall x \\in \\mathbb{R},\\; \\exists y \\in \\mathbb{R}: y &gt; x\\)\n\n\\(\\exists y \\in \\mathbb{R},\\; \\forall x \\in \\mathbb{R}: y &gt; x\\)\n\n\n\n\nTranslate into symbols:\nExpress the following in logical notation.\n\n‚ÄúEvery dataset has at least one outlier.‚Äù\n\n‚ÄúThere exists a consistent estimator for every parameter.‚Äù\n\n\n\n\nInterpret the following statistical statements (no proof needed):\n\n\\(\\forall n \\in \\mathbb{N},\\; \\operatorname{Var}(\\bar{X}_n) \\geq 0\\)\n(For every sample size \\(n\\), the variance of the sample mean is nonnegative.)\n\\(\\exists n \\in \\mathbb{N},\\; \\forall \\epsilon &gt; 0:\\; |\\bar{X}_n - \\mu| &lt; \\epsilon\\)\n(There exists a fixed sample size \\(n\\) such that the sample mean is always arbitrarily close to \\(\\mu\\). Is this realistic?)\n\\(\\forall \\epsilon &gt; 0,\\; \\exists N \\in \\mathbb{N}:\\; n &gt; N \\;\\to\\; |\\bar{X}_n - \\mu| &lt; \\epsilon\\)\n(Interpretation: This is the formal definition of consistency / the Law of Large Numbers).",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html#references",
    "href": "chapters/01_ch01.html#references",
    "title": "Phase 1: Logic & Set Theory",
    "section": "References",
    "text": "References\n\nVelleman, D. J. (2006). How to Prove It: A Structured Approach.\n\nRosen, K. H. (2011). Discrete Mathematics and Its Applications.\n\nSpanos, A. (1999, 2010). Probability Theory and Statistical Inference.",
    "crumbs": [
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  }
]