[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Foundations of Data Science & Causal ML: A Mathematical Journey",
    "section": "",
    "text": "Journey Phases\nThis is my structured learning roadmap to prepare for research in causal machine learning with mathematical rigor.\nHere you can find both mathematical foundations and applications.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Journey Phases</span>"
    ]
  },
  {
    "objectID": "index.html#journey-phases",
    "href": "index.html#journey-phases",
    "title": "Foundations of Data Science & Causal ML: A Mathematical Journey",
    "section": "",
    "text": "Phase 0 – Logic & Set Theory\nGoal: Build comfort with the language of mathematics.\n- Proof techniques: direct, contrapositive, contradiction, induction.\n- Sets and families of sets, Cartesian products, power sets.\n- Functions: injective, surjective, bijective.\n- Relations: equivalence relations, partial orders.\n- Cardinality: countable vs. uncountable sets.\nTheory Output: Notes + proofs on quantifiers, induction, and countability.\nApplication Project: SQL/database operations as set theory (joins, unions, intersections).\nReferences:\n- Velleman – How to Prove It\n- Enderton – Set Theory\n\n\n\nPhase 1 – Real Analysis\nGoal: Rigorous calculus and convergence.\n- Sequences, series, limits.\n- Continuity, compactness, connectedness.\n- Differentiation, Riemann integration (with rigor).\n- Uniform convergence.\nTheory Output: ε–δ proofs, compactness in ℝ, uniform convergence examples.\nApplication Project: Gradient descent convergence demo; connect convexity to logistic regression loss.\nReferences:\n- Rudin – Principles of Mathematical Analysis (Baby Rudin)\n- Tao – Analysis I\n\n\n\nPhase 2 – Topology & Measure Theory\nGoal: Learn the structures that underlie probability theory.\n- Metric spaces, open/closed sets.\n- Compactness and product spaces.\n- σ-algebras, measurable functions.\n- Lebesgue measure and integration.\n- Convergence theorems: MCT, DCT.\nTheory Output: Worked examples of σ-algebras, Lebesgue integral, and convergence theorems.\nApplication Project: Fraud detection via Monte Carlo — rare events and measure-zero sets in anomaly detection.\nReferences:\n- Munkres – Topology\n- Schilling – Measures, Integrals and Martingales\n\n\n\nPhase 3 – Probability\nGoal: Define probability rigorously à la Kolmogorov.\n- Probability spaces and random variables as measurable functions.\n- Distributions, independence, product measures.\n- Conditional expectation as L² projection.\n- Laws of large numbers, central limit theorem.\n- Intro to martingales.\nTheory Output: Probability space construction, LLN/CLT proofs, conditional expectation as projection.\nApplication Project: A/B testing simulation — CLT and confidence intervals for conversion rates.\nReferences:\n- Durrett – Probability: Theory and Examples\n- Klenke – Probability Theory\n\n\n\nPhase 4 – Mathematical Statistics\nGoal: Connect probability → inference.\n- Point estimation: MLE, method of moments.\n- Properties: unbiasedness, consistency, efficiency.\n- Hypothesis testing and likelihood ratio tests.\n- Asymptotic results: convergence in probability/distribution, delta method.\nTheory Output: Consistency of MLE, hypothesis testing framework, asymptotic normality proofs.\nApplication Project: Logistic regression for churn prediction — prove Bernoulli MLE consistency, simulate convergence, apply to real dataset.\nReferences:\n- Casella & Berger – Statistical Inference\n\n\n\nPhase 5 – Causality\nGoal: Enter causal inference with strong mathematical foundations.\n- Pearl’s Structural Causal Models & do-calculus.\n- Rubin’s potential outcomes framework.\n- Invariant causal prediction (Peters, Janzing, Schölkopf).\n- Identifiability proofs.\n- Axiomatic frameworks (Park & Muandet).\nTheory Output: Worked proofs of identifiability, back-door/front-door criteria, do-calculus rules.\nApplication Project: Uplift modeling for churn retention OR reproduction of Chernozhukov’s Double Machine Learning estimator with Python.\nReferences:\n- Pearl – Causality\n- Peters, Janzing, Schölkopf – Elements of Causal Inference\n- Chernozhukov et al. – Causal Machine Learning papers",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Journey Phases</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html",
    "href": "chapters/01_ch01.html",
    "title": "Phase 0: Logic & Set Theory",
    "section": "",
    "text": "Motivation\nlorem ipslum…",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Phase 0: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html#references",
    "href": "chapters/01_ch01.html#references",
    "title": "Phase 0: Logic & Set Theory",
    "section": "References",
    "text": "References",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Phase 0: Logic & Set Theory</span>"
    ]
  }
]