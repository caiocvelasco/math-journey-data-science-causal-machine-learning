[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Foundations of Data Science & Causal ML: A Mathematical Journey",
    "section": "",
    "text": "Journey Phases\nThis is my structured learning roadmap to prepare for research in causal machine learning with mathematical rigor.\nHere you can find both mathematical foundations and applications.",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Journey Phases</span>"
    ]
  },
  {
    "objectID": "index.html#why-im-interested-in-causal-ml",
    "href": "index.html#why-im-interested-in-causal-ml",
    "title": "Foundations of Data Science & Causal ML: A Mathematical Journey",
    "section": "Why Iâ€™m Interested in Causal ML?",
    "text": "Why Iâ€™m Interested in Causal ML?\nGoal: Show the reader the contrasts between two â€œData Culturesâ€.\n- Statistics and econometrics build models on top of stochastic mechanisms (data-generating processes), aiming for explanation and inference.\n- Machine learning often ignores mechanisms and focuses on prediction accuracy, generalization, and performance.\n- Causal ML is a synthesis: it combines MLâ€™s flexibility with statisticsâ€™ concern for identification, adding a causal lens to reason about interventions and counterfactuals.",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Journey Phases</span>"
    ]
  },
  {
    "objectID": "index.html#phase-1-logic-set-theory",
    "href": "index.html#phase-1-logic-set-theory",
    "title": "Foundations of Data Science & Causal ML: A Mathematical Journey",
    "section": "Phase 1 â€“ Logic & Set Theory",
    "text": "Phase 1 â€“ Logic & Set Theory\nGoal: Build comfort with the language of mathematics.\n- Proof techniques: direct, contrapositive, contradiction, induction.\n- Sets and families of sets, Cartesian products, power sets.\n- Functions: injective, surjective, bijective.\n- Relations: equivalence relations, partial orders.\n- Cardinality: countable vs.Â uncountable sets.\nTheory Output: - Sentential logic (Velleman Ch. 1) - Predicate logic & quantifiers (Ch. 2) - Proof techniques (direct, contrapositive, contradiction, induction) (Ch. 3) - Sets, relations, functions (Ch. 4â€“6) - Countability, infinity (Ch. 9 selected, Taoâ€™s appendix)\nApplication Project: - SQL/database operations as set theory (joins, unions, intersections). - Prove/discuss equivalences (e.g., idempotency: SELECT DISTINCT twice = once)\nReferences:\n- Velleman â€“ How to Prove It\n- Enderton â€“ Set Theory",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Journey Phases</span>"
    ]
  },
  {
    "objectID": "index.html#phase-2-real-analysis",
    "href": "index.html#phase-2-real-analysis",
    "title": "Foundations of Data Science & Causal ML: A Mathematical Journey",
    "section": "Phase 2 â€“ Real Analysis",
    "text": "Phase 2 â€“ Real Analysis\nGoal: Rigorous calculus and convergence, revisiting classical calculus concepts with proofs.\n\nSequences, series, limits.\n\nContinuity, compactness, connectedness.\n\nDifferentiation: Mean Value Theorem, Taylor expansion.\n\nRiemann integration (rigorous foundation).\n\nUniform convergence.\n\nApplied Calculus Lens:\n- Multivariable calculus: partial derivatives, gradients, Jacobians, Hessians.\n- Convexity and optimization.\n- Taylor expansions for approximation.\n- Fundamental Theorem of Calculus as link to probability expectations.\nTheory Output: Îµâ€“Î´ proofs, compactness in â„, uniform convergence examples.\nApplication Project: Gradient descent convergence demo; connect convexity to logistic regression loss.\nReferences:\n- Rudin â€“ Principles of Mathematical Analysis (Baby Rudin)\n- Tao â€“ Analysis I",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Journey Phases</span>"
    ]
  },
  {
    "objectID": "index.html#phase-3-linear-algebra",
    "href": "index.html#phase-3-linear-algebra",
    "title": "Foundations of Data Science & Causal ML: A Mathematical Journey",
    "section": "Phase 3 â€“ Linear Algebra",
    "text": "Phase 3 â€“ Linear Algebra\nGoal: Move beyond computation to proofs and structure.\n- Vector spaces, subspaces, linear independence, bases, dimension.\n- Linear transformations and matrices.\n- Inner product spaces, orthogonality, Gramâ€“Schmidt.\n- Determinants, eigenvalues, eigenvectors, diagonalization.\n- Spectral theorem, singular value decomposition.\n- Matrix norms and conditioning.\nTheory Output: Proofs of rankâ€“nullity theorem, spectral theorem for symmetric matrices, SVD existence.\nApplication Project: PCA from first principles â€” prove orthogonal diagonalization, then implement PCA via SVD.\nReferences:\n- Axler â€“ Linear Algebra Done Right\n- Friedberg, Insel & Spence â€“ Linear Algebra\n- Trefethen & Bau â€“ Numerical Linear Algebra (for computational aspects)",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Journey Phases</span>"
    ]
  },
  {
    "objectID": "index.html#phase-4-functional-analysis-hilbert-spaces",
    "href": "index.html#phase-4-functional-analysis-hilbert-spaces",
    "title": "Foundations of Data Science & Causal ML: A Mathematical Journey",
    "section": "Phase 4 â€“ Functional Analysis & Hilbert Spaces",
    "text": "Phase 4 â€“ Functional Analysis & Hilbert Spaces\nGoal: Develop the tools to handle infinite-dimensional vector spaces, operators, and kernels.\n- Normed vector spaces, Banach spaces.\n- Hilbert spaces, orthogonality, projections.\n- Bounded linear operators.\n- Reproducing Kernel Hilbert Spaces (RKHS).\nTheory Output: Prove projection theorem in Hilbert spaces, examples of bounded/unbounded operators, RKHS construction.\nApplication Project: Kernelized regression and SVMs â€” connect functional analysis with machine learning models.\nReferences:\n- Kreyszig â€“ Introductory Functional Analysis with Applications\n- Conway â€“ A Course in Functional Analysis\n- Berlinet & Thomas-Agnan â€“ Reproducing Kernel Hilbert Spaces in Probability and Statistics",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Journey Phases</span>"
    ]
  },
  {
    "objectID": "index.html#phase-5-topology-measure-theory",
    "href": "index.html#phase-5-topology-measure-theory",
    "title": "Foundations of Data Science & Causal ML: A Mathematical Journey",
    "section": "Phase 5 â€“ Topology & Measure Theory",
    "text": "Phase 5 â€“ Topology & Measure Theory\nGoal: Learn the structures that underlie probability theory.\n- Metric spaces, open/closed sets.\n- Compactness and product spaces.\n- Ïƒ-algebras, measurable functions.\n- Lebesgue measure and integration.\n- Convergence theorems: MCT, DCT.\nTheory Output: Worked examples of Ïƒ-algebras, Lebesgue integral, and convergence theorems.\nApplication Project: Fraud detection via Monte Carlo â€” rare events and measure-zero sets in anomaly detection.\nReferences:\n- Munkres â€“ Topology\n- Schilling â€“ Measures, Integrals and Martingales",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Journey Phases</span>"
    ]
  },
  {
    "objectID": "index.html#phase-6-probability",
    "href": "index.html#phase-6-probability",
    "title": "Foundations of Data Science & Causal ML: A Mathematical Journey",
    "section": "Phase 6 â€“ Probability",
    "text": "Phase 6 â€“ Probability\nGoal: Define probability rigorously Ã  la Kolmogorov.\n- Probability spaces and random variables as measurable functions.\n- Distributions, independence, product measures.\n- Conditional expectation as LÂ² projection.\n- Laws of large numbers, central limit theorem.\n- Intro to martingales.\nTheory Output: Probability space construction, LLN/CLT proofs, conditional expectation as projection.\nApplication Project: A/B testing simulation â€” CLT and confidence intervals for conversion rates.\nReferences:\n- Durrett â€“ Probability: Theory and Examples\n- Klenke â€“ Probability Theory",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Journey Phases</span>"
    ]
  },
  {
    "objectID": "index.html#phase-7-mathematical-statistics",
    "href": "index.html#phase-7-mathematical-statistics",
    "title": "Foundations of Data Science & Causal ML: A Mathematical Journey",
    "section": "Phase 7 â€“ Mathematical Statistics",
    "text": "Phase 7 â€“ Mathematical Statistics\nGoal: Connect probability â†’ inference.\n- Point estimation: MLE, method of moments.\n- Properties: unbiasedness, consistency, efficiency.\n- Hypothesis testing and likelihood ratio tests.\n- Asymptotic results: convergence in probability/distribution, delta method.\nTheory Output: Consistency of MLE, hypothesis testing framework, asymptotic normality proofs.\nApplication Project: Logistic regression for churn prediction â€” prove Bernoulli MLE consistency, simulate convergence, apply to real dataset.\nReferences:\n- Casella & Berger â€“ Statistical Inference",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Journey Phases</span>"
    ]
  },
  {
    "objectID": "index.html#phase-8-statistical-learning-theory",
    "href": "index.html#phase-8-statistical-learning-theory",
    "title": "Foundations of Data Science & Causal ML: A Mathematical Journey",
    "section": "Phase 8 â€“ Statistical Learning Theory",
    "text": "Phase 8 â€“ Statistical Learning Theory\nGoal: Bridge inference and prediction by introducing the statistical foundations of machine learning.\nShow how algorithms that learn from data can be rigorously analyzed using probability and measure theoryâ€”without assuming a fixed data-generating model.\n\nLearning as an inference problem: i.i.d. samples ((X_i, Y_i) P(X, Y)).\n\nLoss functions and the notion of risk (R(f) = [L(Y, f(X))]).\n\nEmpirical Risk Minimization (ERM): minimize training loss (R_n(f)) as an estimator of true risk.\n\nGeneralization: understanding why (R_n(f)) â‰ˆ (R(f)) as (n ).\n\nUniform convergence and capacity control (VC dimension, Rademacher complexity).\n\nStructural Risk Minimization (SRM): balance fit and complexity with a penalty term.\n\nRegularization in RKHS: connect to Phase 4â€™s Hilbert-space framework via\n[ = _{f } [R_n(f) + |f|_{}^2]. ]\nBiasâ€“variance decomposition: interpret overfitting and underfitting statistically.\n\nContrast Breimanâ€™s two culturesâ€”data modeling vs.Â algorithmic modelingâ€”and show how Vapnikâ€™s learning theory formalizes the latter.\n\nTheory Output:\n- Derive a simple generalization bound using Hoeffdingâ€™s inequality and the union bound.\n- Compute VC dimension for linear separators.\n- Prove that L2 regularization corresponds to minimizing empirical risk under a bounded-norm constraint.\nApplication Project:\n- Simulate empirical vs.Â generalization error on synthetic data.\n- Train classifiers with varying complexity (polynomial degree or kernel width) and plot training/test errors.\n- Show visually how regularization reduces overfitting and aligns with theoretical bounds.\nReferences:\n- Vapnik â€“ The Nature of Statistical Learning Theory\n- Bousquet, Boucheron & Lugosi â€“ Introduction to Statistical Learning Theory\n- Hastie, Tibshirani & Friedman â€“ The Elements of Statistical Learning\n- Shalev-Shwartz & Ben-David â€“ Understanding Machine Learning: From Theory to Algorithms\n- Breiman â€“ Statistical Modeling: The Two Cultures",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Journey Phases</span>"
    ]
  },
  {
    "objectID": "index.html#phase-9-causality",
    "href": "index.html#phase-9-causality",
    "title": "Foundations of Data Science & Causal ML: A Mathematical Journey",
    "section": "Phase 9 â€“ Causality",
    "text": "Phase 9 â€“ Causality\nGoal: Enter causal inference with strong mathematical foundations.\n- Pearlâ€™s Structural Causal Models & do-calculus.\n- Rubinâ€™s potential outcomes framework.\n- Invariant causal prediction (Peters, Janzing, SchÃ¶lkopf).\n- Identifiability proofs.\n- Axiomatic frameworks (Park & Muandet).\nTheory Output: Worked proofs of identifiability, back-door/front-door criteria, do-calculus rules.\nApplication Project: Uplift modeling for churn retention OR reproduction of Chernozhukovâ€™s Double Machine Learning estimator with Python.\nReferences:\n- Pearl â€“ Causality\n- Peters, Janzing, SchÃ¶lkopf â€“ Elements of Causal Inference\n- Chernozhukov et al.Â â€“ Causal Machine Learning papers",
    "crumbs": [
      "<span class='chapter-number'>1</span>Â  <span class='chapter-title'>Journey Phases</span>"
    ]
  },
  {
    "objectID": "chapters/about.html",
    "href": "chapters/about.html",
    "title": "About Me",
    "section": "",
    "text": "What Iâ€™m doing now\nHi, Iâ€™m Caio Velasco ğŸ‘‹. Mechanical Engineer from Brazil with a Masterâ€™s in Economics & Public Policy (UCLA), thanks to a full scholarship from the Lemann Foundation. In 2021, I stepped away from a PhD in Economics in the Netherlands to support my family during the pandemic and redirected my focus fully toward Data Science.\nI work at the intersection of engineering, economics, and dataâ€”spanning Business Analysis, Consulting, Analytics Engineering, and Data Science across the US, UK, Spain, and Brazil. I enjoy turning messy, fragmented systems into robust end-to-end data platforms and pairing that with mathematical depth (statistics, econometrics, and causal ML) to drive real-world impact and avoid the correlation vs.Â causation dilemma.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>About Me</span>"
    ]
  },
  {
    "objectID": "chapters/about.html#what-im-doing-now",
    "href": "chapters/about.html#what-im-doing-now",
    "title": "About Me",
    "section": "",
    "text": "Building an open book: Foundations of Data Science & Causal ML: A Mathematical Journey (mathematical rigor + applications).\nHelping teams modernize their stacks (ingestion â†’ modeling with dbt â†’ warehouses like Snowflake/Redshift â†’ governance & Looker dashboards).\nDeepening foundations in real analysis, probability, statistics, and causal inference to support future research.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>About Me</span>"
    ]
  },
  {
    "objectID": "chapters/about.html#selected-highlights",
    "href": "chapters/about.html#selected-highlights",
    "title": "About Me",
    "section": "Selected highlights",
    "text": "Selected highlights\n\nAwards & scholarships from Yale University, UCLA, GE Foundation, Lemann Foundation, and The Club of Rome.\nEarly career: helped build Stone Payments (NASDAQ: STNE) in Brazil from scratch and founded MePrepara, an online math prep platform with 140+ videos supporting low-income Brazilian students.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>About Me</span>"
    ]
  },
  {
    "objectID": "chapters/about.html#focus-areas",
    "href": "chapters/about.html#focus-areas",
    "title": "About Me",
    "section": "Focus areas",
    "text": "Focus areas\n\nData & Analytics Engineering: Python Â· SQL Â· dbt Â· Snowflake/Redshift Â· AWS Â· Looker Â· best practices for modeling, tests, CI/CD.\nStat/ML & Causality: mathematical statistics, econometrics, causal inference.\nEducation: writing clear, didactic materials; mentoring; building bridges between theory and practice.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>About Me</span>"
    ]
  },
  {
    "objectID": "chapters/about.html#values-approach",
    "href": "chapters/about.html#values-approach",
    "title": "About Me",
    "section": "Values & approach",
    "text": "Values & approach\n\nRigor + practicality. Start from first principles, then ship value.\nClarity. Well-structured code and always documentating the process didacticaly.\nImpact. Data and technology as tools to expand opportunityâ€”because education changed my life.",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>About Me</span>"
    ]
  },
  {
    "objectID": "chapters/about.html#contact-links",
    "href": "chapters/about.html#contact-links",
    "title": "About Me",
    "section": "Contact & links",
    "text": "Contact & links\n\nPortfolio: https://caiocvelasco.github.io/\nGitHub: https://github.com/caiocvelasco\nLinkedIn: https://www.linkedin.com/in/caiocvelasco/",
    "crumbs": [
      "<span class='chapter-number'>2</span>Â  <span class='chapter-title'>About Me</span>"
    ]
  },
  {
    "objectID": "chapters/00_ch00.html",
    "href": "chapters/00_ch00.html",
    "title": "Why Iâ€™m Interested in Causal ML?",
    "section": "",
    "text": "Overview\nMy interest in Causal Machine Learning (Causal ML) arises from a long-standing divide in how different scientific and data communities think about data.\nStatistics and econometrics have traditionally aimed to explain the world, while machine learning has focused on predicting it. Causal ML is where these worlds meet â€” combining the interpretability and rigor of econometrics with the flexibility and scalability of machine learning.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Why I'm Interested in Causal ML?</span>"
    ]
  },
  {
    "objectID": "chapters/00_ch00.html#the-two-worlds-at-a-glance",
    "href": "chapters/00_ch00.html#the-two-worlds-at-a-glance",
    "title": "Why Iâ€™m Interested in Causal ML?",
    "section": "The Two Worlds (at a Glance)",
    "text": "The Two Worlds (at a Glance)\nIn his influential essay Statistical Modeling: The Two Cultures (2001), Leo Breiman described two very different traditions.\n\n\n\n\n\n\n\n\n\nLens\nWhat you assume\nWhat you optimize\nTypical outputs\n\n\n\n\nData Modeling (Statistics/Econometrics)\nA stochastic data-generating process (DGP)\nValid inference under assumptions\nParameters, confidence intervals, p-values\n\n\nAlgorithmic Modeling (Machine Learning)\nDGP is unknown; data are i.i.d.\nPredictive accuracy & generalization\nPredictions, cross-validation scores, error curves\n\n\n\nBoth cultures ask: what can we learn from data? â€” but their answers differ profoundly.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Why I'm Interested in Causal ML?</span>"
    ]
  },
  {
    "objectID": "chapters/00_ch00.html#world-1-the-statistics-and-econometrics-tradition",
    "href": "chapters/00_ch00.html#world-1-the-statistics-and-econometrics-tradition",
    "title": "Why Iâ€™m Interested in Causal ML?",
    "section": "World 1 â€” The Statistics and Econometrics Tradition",
    "text": "World 1 â€” The Statistics and Econometrics Tradition\n(Model â†’ Inference)\nDid you know that when you run a regression, you are implicitly making a strong claim about how the world works?\nIn this tradition, everything begins with the data-generating process (DGP) â€” what Aris Spanos in Probability Theory and Statistical Inference calls a stochastic mechanism. Imagine it as a hidden machine tossing probabilistic dice, producing the data we observe.\nA statistical model is not just a tool; itâ€™s a story about that hidden mechanism â€” a simplified mathematical narrative about how outcomes arise.\nExamples:\n- A linear regression assumes wages, test scores, or prices are generated as a straight-line combination of inputs plus random noise.\n- A Poisson model assumes counts follow a specific probabilistic law.\n- A time-series AR(1) assumes todayâ€™s value depends on yesterdayâ€™s, plus a random shock.\nIf the DGP is hidden, what do we do? â€” We try to uncover its truth.\nThatâ€™s why introductory statistics begins with the idea of a Population and then introduces Samples. Since we canâ€™t observe the whole population, we rely on probabilistic laws like the Law of Large Numbers (LLN) and the Central Limit Theorem (CLT) to argue that sample averages approximate population truths.\nFrom here arises Inference:\n&gt; If I trust my assumptions about the DGP, I can use my sample to make representative claims about the population.\nThis is the heart of classical statistics and econometrics:\n1. Model the hidden process.\n2. Estimate its parameters.\n3. Test whether your story holds up against data.\nBut remember: the DGP is always an assumption. If itâ€™s wrong, inference can mislead.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Why I'm Interested in Causal ML?</span>"
    ]
  },
  {
    "objectID": "chapters/00_ch00.html#world-2-the-machine-learning-tradition",
    "href": "chapters/00_ch00.html#world-2-the-machine-learning-tradition",
    "title": "Why Iâ€™m Interested in Causal ML?",
    "section": "World 2 â€” The Machine Learning Tradition",
    "text": "World 2 â€” The Machine Learning Tradition\n(Data â†’ Prediction)\nNow comes the other world â€” the algorithmic modeling culture.\nHere, the hidden mechanism is ignored. No one asks whether wages are â€œtruly linearâ€ or if prices follow a Poisson law. The question is brutally pragmatic:\n\nCan I predict well?\n\nThe central challenge here is generalization â€” ensuring that a model trained on one dataset (the training set) performs well on unseen data (the test set).\nKey trade-offs: - Underfitting: model too simple â†’ high bias, low accuracy.\n- Overfitting: model too complex â†’ captures noise, poor generalization.\nSuccess is measured by predictive accuracy, not by unbiasedness or efficiency.\nInterpretability is optional; performance is everything.\nBut this comes with risks â€” strong predictions may fail when the underlying data distribution shifts.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Why I'm Interested in Causal ML?</span>"
    ]
  },
  {
    "objectID": "chapters/00_ch00.html#when-each-culture-wins-and-loses",
    "href": "chapters/00_ch00.html#when-each-culture-wins-and-loses",
    "title": "Why Iâ€™m Interested in Causal ML?",
    "section": "When Each Culture Wins â€” and Loses",
    "text": "When Each Culture Wins â€” and Loses\nLetâ€™s make this distinction concrete.\n\nExample 1 â€” Estimating the causal effect of education on wages\nIn econometrics, Iâ€™d model wages as a function of education, experience, and other covariates to isolate the causal effect of education.\nA pure ML model might predict wages well, but it wouldnâ€™t reveal whether education causes higher wages â€” maybe itâ€™s just correlated with family wealth or job networks.\nML excels at prediction, but fails at interpretation and identification.\n\n\nExample 2 â€” Predicting house prices\nAn econometrician might specify a linear model with a few variables and strong assumptions â€” too simplistic to capture reality.\nML models like gradient boosting can handle thousands of variables and complex nonlinearities, achieving far higher predictive accuracy â€” even if they canâ€™t â€œexplainâ€ why.\nSo:\n- Statistics shines when we seek mechanistic understanding.\n- ML shines when we need accurate prediction.\n- Each fails where the other succeeds.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Why I'm Interested in Causal ML?</span>"
    ]
  },
  {
    "objectID": "chapters/00_ch00.html#why-causal-ml-is-the-synthesis",
    "href": "chapters/00_ch00.html#why-causal-ml-is-the-synthesis",
    "title": "Why Iâ€™m Interested in Causal ML?",
    "section": "Why Causal ML Is the Synthesis",
    "text": "Why Causal ML Is the Synthesis\nâœ¨ This is why Causal Machine Learning excites me.\nItâ€™s the bridge â€” combining MLâ€™s flexibility with econometricsâ€™ discipline of identification.\nResearchers like Victor Chernozhukov, Christian Hansen, Nathan Kallus, Martin Spindler, and Vasilis Syrgkanis show that we can merge both worlds:\n- From machine learning, borrow flexible algorithms for complex patterns.\n- From econometrics, borrow identification strategies to isolate causality.\n- From statistics, borrow inference tools for uncertainty and robustness.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Why I'm Interested in Causal ML?</span>"
    ]
  },
  {
    "objectID": "chapters/00_ch00.html#a-real-world-case-online-advertising",
    "href": "chapters/00_ch00.html#a-real-world-case-online-advertising",
    "title": "Why Iâ€™m Interested in Causal ML?",
    "section": "A Real-World Case â€” Online Advertising",
    "text": "A Real-World Case â€” Online Advertising\n\nA standard supervised ML model might predict that showing more ads increases purchases, because ads and purchases are positively correlated.\nBut randomized experiments revealed a much smaller â€” sometimes zero â€” causal effect.\nWhy? Because users likely to purchase were already targeted with more ads.\n\nPrediction looked strong.\nCausality revealed the truth.\nThis illustrates why prediction â‰  causation â€” and why combining both is powerful.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Why I'm Interested in Causal ML?</span>"
    ]
  },
  {
    "objectID": "chapters/00_ch00.html#how-causal-ml-works-intuition-first",
    "href": "chapters/00_ch00.html#how-causal-ml-works-intuition-first",
    "title": "Why Iâ€™m Interested in Causal ML?",
    "section": "How Causal ML Works (Intuition First)",
    "text": "How Causal ML Works (Intuition First)\nCausal ML methods are sophisticated, but their core intuitions are simple.\n\nSample splitting / cross-fitting: train nuisance models on one part of the data, test causal relations on another â€” prevents â€œreusingâ€ the same information and overfitting.\n\nOrthogonalization (Double ML): construct estimators that remain unbiased even if nuisance models are imperfect.\n\nHeterogeneous treatment effects: use trees, forests, or boosting to uncover who benefits most.\n\nBeyond IID: Causal inference helps reason about distribution shifts, interventions, and counterfactuals â€” what if tomorrow doesnâ€™t look like today?\n\nTogether, these ideas unify prediction and inference under one theoretical umbrella.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Why I'm Interested in Causal ML?</span>"
    ]
  },
  {
    "objectID": "chapters/00_ch00.html#focus-of-this-project",
    "href": "chapters/00_ch00.html#focus-of-this-project",
    "title": "Why Iâ€™m Interested in Causal ML?",
    "section": "Focus of This Project",
    "text": "Focus of This Project\nMy main focus is to master the mathematical foundations that underpin Causal ML â€” to move beyond using tools, and toward understanding why they work.\nMathematical depth transforms intuition into clarity. Too often, methods are applied skillfully but without true comprehension. This project aims to bridge that gap.\n\nâ€œMathematical maturity is not about memorizing formulas,\nbut about seeing how different ideas connect.â€",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Why I'm Interested in Causal ML?</span>"
    ]
  },
  {
    "objectID": "chapters/00_ch00.html#the-foundational-path-the-journey-ahead",
    "href": "chapters/00_ch00.html#the-foundational-path-the-journey-ahead",
    "title": "Why Iâ€™m Interested in Causal ML?",
    "section": "The Foundational Path (The Journey Ahead)",
    "text": "The Foundational Path (The Journey Ahead)\nTo understand Causal ML properly, weâ€™ll build the entire mathematical backbone step by step:\n\nLogic & Set Theory â€” precision and proof techniques.\n\nReal Analysis â€” limits, continuity, optimization.\n\nLinear Algebra â€” geometry of models and data.\n\nFunctional Analysis & Hilbert Spaces â€” kernels, projections, and RKHS.\n\nTopology & Measure Theory â€” Ïƒ-algebras and integration.\n\nProbability â€” rigorous uncertainty, LLN/CLT, expectations.\n\nMathematical Statistics â€” estimation, hypothesis testing, asymptotics.\n\nStatistical Learning Theory (Interlude) â€” generalization, VC dimension, ERM.\n\nCausality â€” SCMs, potential outcomes, identifiability, and Causal ML methods.\n\nEach phase builds a bridge â€” from logic to learning, from inference to causation.\n\n\nCausal ML is not just a toolkit.\nItâ€™s a synthesis â€” the meeting point of mathematical rigor, algorithmic flexibility, and causal reasoning.\nUnderstanding its foundations is the first step toward using it wisely.",
    "crumbs": [
      "<span class='chapter-number'>3</span>Â  <span class='chapter-title'>Why I'm Interested in Causal ML?</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html",
    "href": "chapters/01_ch01.html",
    "title": "Phase 1: Logic & Set Theory",
    "section": "",
    "text": "0. Mathematics, Axioms, and Science\nMathematics begins with axioms â€” assumptions we agree to accept without proof â€” and builds everything else from them using precise rules of logic.\nThis process of axiomatization is what gives mathematics its unique clarity: starting from simple, explicit principles, we construct entire theories.\nWhy is this important?\n- Axioms are not â€œtrueâ€ in an absolute sense. They are starting points.\n- What matters is whether the consequences drawn from them are logically consistent and useful for understanding the world.\n- In practice, this makes mathematics a language in which science can express and test ideas.\nMathematics provides the rigorous framework for turning assumptions into predictions.\nFor example, in Euclidean geometry we assume as an axiom:\nBut this depends on the â€œspaceâ€ we are working in.\nIntuitively:\nNow consider two points on the equator that are exactly opposite each other (say Rio de Janeiro and Jakarta).\nThe key idea is that â€œstraight lineâ€ really means shortest possible path given the geometry of the space.\nOn a plane, thatâ€™s the familiar straight line; on a sphere, itâ€™s a great circle.\nThe notion of a geodesic generalizes this idea: whatever the space, it tells us the natural way to connect two points as efficiently as possible.\nThis illustrates a key point:\nThey are conventions that define the system we are working in.\nWithin their proper context (e.g., Euclidean geometry), their consequences are logically consistent (no contradictions arise) and useful (for designing buildings, maps, and bridges). But in other contexts (e.g., spherical geometry), we need a different set of axioms.\nIn the same way, in probability we may assume:\nReal-world data often violates this (dependence in time series, non-identical distributions in heterogeneous populations).\nBut within the i.i.d. framework, the consequences are logically consistent and extremely useful: they give us fundamental results like the Law of Large Numbers and the Central Limit Theorem.\nThe analogy with geometry is this:\nSimilarly, in probability:\nWhy we begin with Logic and Set Theory\nTogether, Logic and Set Theory form the grammar and vocabulary of mathematics.\nThey are not yet â€œabout the real worldâ€ â€” but they give us the precise tools to state assumptions and derive consequences.\nIntuition for research\nWhen you see theoretical work in physics, economics, or causal machine learning, it often starts by axiomatizing the problem:\n- Define the objects (e.g.Â random variables, causal spaces).\n- Specify assumptions as axioms (e.g.Â independence, stability, interventions).\n- Derive results rigorously from there.\nBy starting here, we are learning how to speak the language of mathematics before applying it to inference, probability, and causality.\nThus, before diving into analysis and probability, we establish a foundation in logic and set theory. This is important to formalize assumptions, express mathematical objects precisely, and build proofs with rigor.\nLogic gives us the language to connect premises and conclusions, while Set Theory gives us the structure to define universes of discourse, events, functions, probability spaces, etc.\nTherefore: Yes, it would be essential to learn Logic and Set Theory!",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html#statements-propositions-predicates-and-connectives",
    "href": "chapters/01_ch01.html#statements-propositions-predicates-and-connectives",
    "title": "Phase 1: Logic & Set Theory",
    "section": "1.1 Statements, Propositions, Predicates, and Connectives",
    "text": "1.1 Statements, Propositions, Predicates, and Connectives\nğŸ’¡Motivation\nLearning statements and connectives is like learning the alphabet of mathematics.\n- If you cannot distinguish valid statements, you cannot even start a proof.\nExample in causal inference:\n- \\(p\\): â€œThe model includes all relevant confounders.â€\n- \\(q\\): â€œThe conditional independence assumption holds.â€\n- Then â€œIf the model includes all relevant confounders, then the conditional independence assumption holdsâ€ is \\(p \\to q\\).\nWithout connectives, weâ€™d stay in informal language. With them, we can formalize statements and reason rigorously about consequences, such as proving that a set of assumptions implies consistency of an estimator, showing that ignorability implies identification of a treatment effect, or demonstrating that conditional independence leads to factorization of a probability distribution into simpler components.\nA Statement (or Proposition) is a declarative sentence that is either true or false.\n\nExample: â€œ3 is evenâ€ (false), â€œBarcelona is in Spainâ€ (true).\n\nA Predicate is like a â€œtemplateâ€ for a statement: it depends on a variable and becomes a statement once you specify the value. For example:\n\\(P(x): x &gt; 0\\) (this is a Predicate or a â€œtemplateâ€)\n- \\(P(2)\\) â†’ â€œ2 &gt; 0â€ (This is a Statement, with logic value â€œtrueâ€).\n- \\(P(-1)\\) â†’ â€œ-1 &gt; 0â€ (This is a Statement, with logic value â€œfalseâ€).\nConnectives are logical â€œoperatorsâ€ that allow us to combine simple statements into more complex ones.\nThey let us reason systematically instead of relying on vague language.\nBut why do we want to combine simple statements into more complex ones?\nBecause in mathematics (and especially in statistics), we rarely deal with isolated facts.\nWe deal with assumptions and want to know what conclusions they imply.\nConnectives give us the structure to move from â€œpieces of informationâ€ to a logical argument.\nExample from statistics:\n\n\\(p\\): â€œThe sample is i.i.d.â€\n\n\\(q\\): â€œThe variance of the sample is finite.â€\n\n\\(r\\): â€œThe sample mean converges to the true mean (Law of Large Numbers).â€\n\nIn words:\nâ€œIf the sample is i.i.d. and the variance is finite, then the sample mean converges.â€\nSymbolically:\n\\[\n(p \\land q) \\to r\n\\]\nThis is exactly how many theorems are written: conditions (joined with \\(\\land\\)) imply a result (\\(\\to\\)).\nSo connectives are not just about making compound sentences longer.\nThey are about giving us a precise language of assumptions and consequences.\n\nThis is the foundation of mathematical reasoning: once we can formalize, we can prove.\n\nLetâ€™s see them one by one with examples:\n\nNegation (\\(\\lnot p\\))\nMeaning: â€œnot \\(p\\)â€\n\nIf \\(p\\) is: â€œThe number 5 is evenâ€ (false),\n\nThen \\(\\lnot p\\) is: â€œThe number 5 is not evenâ€ (true).\n\n\n\n\nConjunction (\\(p \\land q\\))\nMeaning: â€œ\\(p\\) and \\(q\\)â€ â†’ true only when both are true.\n\n\\(p\\): â€œ2 is evenâ€ (true)\n\n\\(q\\): â€œ3 is primeâ€ (true)\n\n\\(p \\land q\\): â€œ2 is even and 3 is primeâ€ (true).\n\nIf either part were false, the whole conjunction would be false.\n\n\n\n\nDisjunction (\\(p \\lor q\\))\nMeaning: â€œ\\(p\\) or \\(q\\)â€ â†’ true if at least one is true.\n\n\\(p\\): â€œBarcelona is in Spainâ€ (true)\n\n\\(q\\): â€œLisbon is in Brazilâ€ (false)\n\n\\(p \\lor q\\): â€œBarcelona is in Spain or Lisbon is in Brazilâ€ (true).\n\n(Note: In logic, â€œorâ€ usually means inclusive or â€” at least one true, possibly both.)\n\n\n\nConditional (\\(p \\to q\\))\nMeaning: â€œif \\(p\\) then \\(q\\)â€ â†’ false only if \\(p\\) is true and \\(q\\) is false.\n\n\\(p\\): â€œIt rains.â€\n\n\\(q\\): â€œThe ground is wet.â€\n\n\\(p \\to q\\): â€œIf it rains, then the ground is wet.â€\n\nTrue if it rains and the ground is wet (promise kept).\n\nFalse if it rains but the ground is not wet (promise broken).\n\nTrue if it doesnâ€™t rain (vacuously true).\n\n\n\n\n\nBiconditional (\\(p \\leftrightarrow q\\))\nMeaning: â€œ\\(p\\) if and only if \\(q\\)â€ â†’ true when \\(p\\) and \\(q\\) have the same truth value.\n\n\\(p\\): â€œToday is Saturday.â€\n\n\\(q\\): â€œTomorrow is Sunday.â€\n\n\\(p \\leftrightarrow q\\): â€œToday is Saturday if and only if tomorrow is Sunday.â€ â†’ true.\n\nIf one part is true and the other false, the biconditional is false.\n\n\nThus:\n- Predicate: general template (open sentence, truth depends on a variable) -&gt; becomes true or false only when a variable is given a value.\n- Proposition/statement: instance of that template (closed sentence, definite truth) -&gt; something that is already true or false.\n- Connectives: operators that take simple propositions and form compound propositions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html#truth-tables",
    "href": "chapters/01_ch01.html#truth-tables",
    "title": "Phase 1: Logic & Set Theory",
    "section": "1.2 Truth Tables",
    "text": "1.2 Truth Tables\nğŸ’¡Motivation\nNow that we know connectives allow us to combine assumptions and conclusions into structured arguments, we need a way to check the consistency and behavior of these arguments.\nWhy? Because once statements get more complex, our intuition alone isnâ€™t reliable.\nFor example, the conditional \\(p \\to q\\) often confuses beginners. Why? Because in everyday language, â€œifâ€¦ thenâ€¦â€ feels different from how logic defines it.\nThe Concept of â€œPromiseâ€\n\nWe naturally read a conditional like \\(p \\to q\\) as: â€œIf it rains, then the ground gets wet.â€\nThink of \\(p \\to q\\) as a promise:\n\n\\(p\\): It rains.\n\n\\(q\\): The ground gets wet.\n\nSo \\(p \\to q\\) means: â€œI promise that if it rains, then the ground will get wet.â€\nWe also naturally think that:\n1. If it rains and the ground is wet â†’ âœ… promise kept.\n2. If it rains and the ground is not wet â†’ âŒ promise broken.\nSo far, this matches intuition.\nBut what if it doesnâ€™t rain?\n- Logic says: the conditional is still true, no matter whether the ground is wet or dry.\n- Why? Because the promise â€œif it rains, then the ground gets wetâ€ has not been broken â€” the condition (\\(p\\)) never happened in the first place.\n- This is what we call vacuous truth.\nThis feels counterintuitive, and thatâ€™s exactly why we need a systematic tool.\nWhy Truth Tables?\nTruth tables are the grammar checker of logic. They resolve this confusion:\n\nThey let us mechanically verify whether compound statements are valid, equivalent, tautological, or contradictory.\n\nThey are a first step toward building formal proofs, because they allow us to test the â€œmechanicsâ€ of logical structure before tackling deeper arguments.\n\nThink of them as a microscope for logical statements:\nwhen connectives weave sentences together, truth tables let us zoom in and see all possible truth scenarios at once.\nThey also let us test whether two statements are logically equivalent. For example:\n\nâ€œIf not \\(p\\), then not \\(q\\).â€\n\nâ€œIf \\(q\\), then \\(p\\).â€\n\nAre these equivalent? Intuition may fail, but a Truth Table makes it crystal clear!\nExample in statistics:\n\nIf the data behave nicely (i.i.d. + finite variance), then the sample mean is reliable (it will converge to the true mean) (\\(p \\to q\\)).\n\nContrapositive: If the sample mean does not converge to the true mean, then the data were not i.i.d. with finite variance (\\(\\lnot q \\to \\lnot p\\)).\n\nA truth table shows how the truth value of a compound statement depends on its parts.\nExample: Prove that an implication (\\(p \\to q\\)) is equivalent (\\(\\equiv\\)) to its contrapositive (\\(\\lnot q \\to \\lnot p\\)).\nWe want to show: \\(p \\to q \\;\\equiv\\; \\lnot q \\to \\lnot p\\)\n\n\n\n\n\n\n\n\n\n\n\n\\(p\\)\n\\(q\\)\n\\(p \\to q\\)\n\\(\\lnot q\\)\n\\(\\lnot p\\)\n\\(\\lnot q \\to \\lnot p\\)\n\n\n\n\nT\nT\nT\nF\nF\nT\n\n\nT\nF\nF\nT\nF\nF\n\n\nF\nT\nT\nF\nT\nT\n\n\nF\nF\nT\nT\nT\nT\n\n\n\nSince the last two columns match, the implication is equivalent to its contrapositive.\nExample: Analysis of a Conditional\nTake \\(p\\): It rains, and \\(q\\): The ground gets wet.\nThe statement \\(p \\to q\\) is a promise: â€œIf it rains, then the ground gets wet.â€\n\n\n\n\\(p\\)\n\\(q\\)\n\\(p \\to q\\)\nExplanation\n\n\n\n\nT\nT\nT\nâœ… promise kept\n\n\nT\nF\nF\nâŒ promise broken\n\n\nF\nT\nT\nvacuously true (condition never triggered)\n\n\nF\nF\nT\nvacuously true (condition never triggered)\n\n\n\nKey intuition:\n- An implication is only false when the condition happens but the promised result fails.\n- In all other cases, the promise has not been broken, so the implication is true.\nThis is why statements like â€œAll unicorns have hornsâ€ are technically true: since there are no unicorns, the condition never triggers, and the statement cannot be falsified.\n\n\nExercises\n\nExercise 1. Show that \\(p \\to q\\) is equivalent to \\(\\lnot p \\lor q\\).\nStep 1. Write the two statements side by side: \\(p \\to q\\) and \\(\\lnot p \\lor q\\).\nStep 2. Build a truth table with columns for \\(p\\), \\(q\\), \\(\\lnot p\\), \\(p \\to q\\), and \\(\\lnot p \\lor q\\).\nStep 3. Compare the last two columns.\nStep 4. Conclude: if they match in all rows, the statements are logically equivalent.\n\nSolution\n\nStep 1: Write the two statements side by side: \\(p \\to q\\) and \\(\\lnot p \\lor q\\).\nWhat we want to check is whether these two always mean the same thing.\n- \\(p \\to q\\): â€œIf \\(p\\), then \\(q\\).â€\n- \\(\\lnot p \\lor q\\): â€œEither not \\(p\\), or \\(q\\).â€\nAt first sight they look different, but we suspect they are really saying the same thing.\nTo confirm, we will compare their truth tables row by row.\n\nStep 2: Build a truth table with columns for \\(p\\), \\(q\\), \\(\\lnot p\\), \\(p \\to q\\), and \\(\\lnot p \\lor q\\).\n(Didactic hint: to fill this correctly, recall the â€œgrammarâ€ of each connective.)\n- Negation \\(\\lnot p\\): flips the truth value (Tâ†’F, Fâ†’T).\n- Disjunction \\(A \\lor B\\): true if at least one of \\(A,B\\) is true.\n- Conditional \\(p \\to q\\): think â€œpromiseâ€ â€” it is false only when the promise is broken (i.e., \\(p\\) is T but \\(q\\) is F); otherwise true (including when \\(p\\) is F: vacuous truth).\n\n\n\n\\(p\\)\n\\(q\\)\n\\(\\lnot p\\)\n\\(p \\to q\\)\n\\(\\lnot p \\lor q\\)\n\n\n\n\nT\nT\nF\nT\nT\n\n\nT\nF\nF\nF\nF\n\n\nF\nT\nT\nT\nT\n\n\nF\nF\nT\nT\nT\n\n\n\nHow we filled it:\n- \\(\\lnot p\\) is just the flip of \\(p\\).\n- \\(p \\to q\\) is only F in the row \\((p,q)=(T,F)\\) (promise broken).\n- \\(\\lnot p \\lor q\\) is true when either \\(\\lnot p\\) is T or \\(q\\) is T.\n\nStep 3: Compare the last two columns.\nThe columns for \\(p \\to q\\) and \\(\\lnot p \\lor q\\) are identical (T, F, T, T).\nTherefore, the statements have the same truth value in every case.\n\nStep 4: Conclude.\nWe conclude:\n\\(p \\to q \\equiv \\lnot p \\lor q\\)\nIn words: an implication â€œif \\(p\\), then \\(q\\)â€ always carries the same meaning as saying â€œeither \\(p\\) does not happen, or \\(q\\) happens.â€\n\n\nExercise 2. Analyze the statement: â€œIf a number is divisible by 4, then it is even.â€\nStep 1. Define \\(p\\): â€œnumber is divisible by 4â€, and \\(q\\): â€œnumber is even.â€\nStep 2. Translate into logic: \\(p \\to q\\).\nStep 3. Think through cases:\n- A number divisible by 4 (say 12) â†’ it is even â†’ promise kept.\n- A number divisible by 4 but not even â†’ impossible case â†’ promise broken.\n- A number not divisible by 4 (say 9) â†’ nothing promised, implication vacuously true.\nStep 4. Verify with a truth table if needed.\nStep 5. Conclude: the statement holds logically.\n\nBiconditional: \\(p \\leftrightarrow q\\) (â€œ\\(p\\) if and only if \\(q\\)â€)\n\nRule: \\(p \\leftrightarrow q\\) is true exactly when \\(p\\) and \\(q\\) have the same truth value\n(both true or both false).\nEquivalent form:\n\\[\np \\leftrightarrow q \\;\\equiv\\; (p \\to q) \\land (q \\to p)\n\\]\nExample:\n\\(p\\): â€œToday is Saturday.â€\n\\(q\\): â€œTomorrow is Sunday.â€\n\\(p \\leftrightarrow q\\): â€œToday is Saturday if and only if tomorrow is Sunday.â€\nCase analysis:\n\nIf both \\(p\\) and \\(q\\) are true â†’ the biconditional is true (both directions of the promise hold).\n\nIf \\(p\\) is true but \\(q\\) is false â†’ false, because one direction of the â€œif and only ifâ€ fails.\n\nIf \\(p\\) is false but \\(q\\) is true â†’ false, for the same reason.\n\nIf both \\(p\\) and \\(q\\) are false â†’ true, because they match in value (both false).\n\n\nThis explains why the truth table looks like this:\n\n\n\n\\(p\\)\n\\(q\\)\n\\(p \\leftrightarrow q\\)\nExplanation\n\n\n\n\nT\nT\nT\nboth true â†’ promise kept\n\n\nT\nF\nF\nmismatch â†’ one direction fails\n\n\nF\nT\nF\nmismatch â†’ one direction fails\n\n\nF\nF\nT\nboth false â†’ they match\n\n\n\nIntuition and goal of the biconditional\nThe biconditional expresses equivalence: \\(p\\) and \\(q\\) â€œstand or fall together.â€\nIt is stronger than a one-way implication: both \\(p \\to q\\) and \\(q \\to p\\) must hold.\n\nIf you read \\(p \\leftrightarrow q\\) aloud, it means:\nâ€œ\\(p\\) is true exactly when \\(q\\) is true.â€ or â€œ\\(p\\) holds exactly when \\(q\\) holds.â€\n\nThis is why mathematicians often use â€œiffâ€ (â€œif and only ifâ€) in definitions and theorems:\n- It guarantees not only that \\(p\\) implies \\(q\\), but also that \\(q\\) implies \\(p\\).\n\nWhy is this important?\n\nIt formalizes definitions in mathematics.\n\nExample: â€œA number \\(n\\) is even iff \\(n = 2k\\) for some integer \\(k\\).â€\n\nThis captures both directions: every even number has that form, and every number of that form is even.\n\nIt allows us to state equivalence theorems.\n\nExample: â€œA sequence is Cauchy iff it is convergent (in \\(\\mathbb{R}\\)).â€\n\nThe biconditional captures the deep connection: each property implies the other.\n\nIt makes reasoning reversible.\n\nWith an implication, you can only go forward (\\(p \\to q\\)).\n\nWith a biconditional, you can go forward and backward: knowing either \\(p\\) or \\(q\\) tells you the other.\n\n\nBy mastering the biconditional, the reader understands why mathematicians love the phrase â€œif and only ifâ€: itâ€™s the precise way of stating true equivalence between concepts.\nSummary Table of Connectives\n\n\n\n\n\n\n\n\nConnective\nSymbol\nRule (when true)\n\n\n\n\nNegation\n\\(\\lnot p\\)\nwhen \\(p\\) is false\n\n\nConjunction\n\\(p \\land q\\)\nwhen \\(p\\) and \\(q\\) are true\n\n\nDisjunction\n\\(p \\lor q\\)\nwhen at least one of \\(p, q\\) is true\n\n\nConditional\n\\(p \\to q\\)\nfalse only if \\(p\\) true and \\(q\\) false\n\n\nBiconditional\n\\(p \\leftrightarrow q\\)\nwhen \\(p\\) and \\(q\\) have same truth value",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html#tautologies-contradictions-and-logical-equivalence",
    "href": "chapters/01_ch01.html#tautologies-contradictions-and-logical-equivalence",
    "title": "Phase 1: Logic & Set Theory",
    "section": "1.3 Tautologies, Contradictions, and Logical Equivalence",
    "text": "1.3 Tautologies, Contradictions, and Logical Equivalence\nTautology\nDefinition:\nA tautology is a statement that is true in all possible cases.\nWhy it matters:\n- Tautologies act like universal truths: they donâ€™t depend on data or assumptions.\n- They are often the â€œglueâ€ of proofs, showing that certain forms are always valid.\n- Many rules of inference (like modus ponens) are based on tautologies.\nExample (logic):\n\\[\n(p \\land q) \\to p\n\\]\nThis means: If both (p) and (q) are true, then (p) is true.\n- Always true, regardless of whether (p) or (q) are true or false.\nExample (statistics):\nThe Law of Total Probability is tautological:\n\\[\nP(A) = P(A \\cap B) + P(A \\cap \\lnot B).\n\\]\nThis identity always holds by construction, no matter what events (A) and (B) are.\n\nContradiction\nDefinition:\nA contradiction is a statement that is false in all possible cases.\nWhy it matters:\n- Contradictions are the engine of proof by contradiction.\n- If assuming something leads to a contradiction, then the assumption must be false.\n- They represent â€œimpossible situationsâ€ in logic.\nExample (logic):\n\\[\np \\land \\lnot p\n\\]\nThis means: (p) is true and (p) is false at the same time.\n- Always false, no matter what truth value (p) has.\nExample (statistics):\nSuppose we assume:\n1. â€œThe variance of this distribution is finite.â€\n2. â€œThe variance of this distribution is infinite.â€\nTogether, these form a contradiction, so at least one assumption must be wrong.\n\nLogical Equivalence\nDefinition:\nTwo statements are logically equivalent if they have the same truth value in all possible cases.\nWhy it matters:\n- Logical equivalence lets us replace one statement with another in a proof.\n- Many powerful proof strategies rely on equivalence (contrapositive law, De Morganâ€™s laws, distributive laws).\n- Often the equivalent form is much easier to work with.\nExample (logic):\n\\[\np \\to q \\;\\equiv\\; \\lnot p \\lor q\n\\]\nThis means: â€œIf (p), then (q)â€ is the same as â€œEither not (p), or (q).â€\n- This equivalence makes it easier to manipulate conditionals in proofs.\nExample (causal inference):\n\\[\n\\text{Ignorability} \\to \\text{Identifiability}\n\\]\nis logically equivalent to\n\\[\n\\lnot \\text{Identifiability} \\to \\lnot \\text{Ignorability}.\n\\]\nSwitching to the contrapositive often makes a proof or argument simpler.\n\nSummary:\n- Tautologies give us universal truths to rely on.\n- Contradictions allow us to eliminate false assumptions through contradiction proofs.\n- Logical equivalence lets us restate problems in easier forms without changing meaning.",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html#predicates-and-quantifiers",
    "href": "chapters/01_ch01.html#predicates-and-quantifiers",
    "title": "Phase 1: Logic & Set Theory",
    "section": "2.1 Predicates and Quantifiers",
    "text": "2.1 Predicates and Quantifiers\nAs we saw above, a predicate is like a sentence with a â€œblankâ€ â€” it becomes a full statement only once you plug in a value.\n\nExample: \\(P(x): x &gt; 0\\).\n\nIf \\(x = 2\\), then \\(P(2)\\) is the proposition â€œ2 &gt; 0â€ (true).\n\nIf \\(x = -3\\), then \\(P(-3)\\) is the proposition â€œ-3 &gt; 0â€ (false).\n\n\nWe use quantifiers to talk about how many elements satisfy a predicate:\n\nUniversal quantifier (\\(\\forall\\)):\n\\(\\forall x\\; P(x)\\) means â€œfor all \\(x\\), \\(P(x)\\) is true.â€\nExistential quantifier (\\(\\exists\\)):\n\\(\\exists x\\; P(x)\\) means â€œthere exists at least one \\(x\\) such that \\(P(x)\\) is true.â€\n\nExamples:\n- \\(\\forall x \\in \\mathbb{Z},\\; x^2 \\geq 0\\). (Every integer squared is nonnegative.)\n- \\(\\exists x \\in \\mathbb{Z},\\; x^2 = 9\\). (There exists an integer whose square is 9.)",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html#universe-of-discourse",
    "href": "chapters/01_ch01.html#universe-of-discourse",
    "title": "Phase 1: Logic & Set Theory",
    "section": "2.2 Universe of Discourse",
    "text": "2.2 Universe of Discourse\nThe universe of discourse is the set of objects we allow \\(x\\) to vary over.\nThe truth of a statement depends on it!\nExample:\n\n\\(\\forall x \\in \\mathbb{R},\\; x^2 \\geq 0\\) is true.\n\n\\(\\forall x \\in \\mathbb{Z},\\; x^2 = 2\\) is false (no integer squared equals 2).\n\nIf we didnâ€™t specify whether \\(x\\) ranges over \\(\\mathbb{R}\\) or \\(\\mathbb{Z}\\) (both interpreted as the universe of discourse of each statement), the meaning would be ambiguous.",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html#truth-of-quantified-statements",
    "href": "chapters/01_ch01.html#truth-of-quantified-statements",
    "title": "Phase 1: Logic & Set Theory",
    "section": "2.3 Truth of Quantified Statements",
    "text": "2.3 Truth of Quantified Statements\nHow to evaluate quantified statements:\n\n\\(\\forall x\\; P(x)\\) is true if every \\(x\\) in the universe makes \\(P(x)\\) true.\n\n\\(\\exists x\\; P(x)\\) is true if at least one \\(x\\) makes \\(P(x)\\) true.\n\nNegations of Quantifiers\nNegating quantified statements flips the quantifier:\n\\[\n\\lnot (\\forall x\\, P(x)) \\equiv \\exists x\\, \\lnot P(x)\n\\]\n\\[\n\\lnot (\\exists x\\, P(x)) \\equiv \\forall x\\, \\lnot P(x)\n\\]\nExamples:\n\nâ€œNot all students passedâ€ means â€œThere exists a student who did not pass.â€\n\nâ€œThere does not exist a unicornâ€ means â€œFor all \\(x\\), \\(x\\) is not a unicorn.â€",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html#multiple-quantifiers",
    "href": "chapters/01_ch01.html#multiple-quantifiers",
    "title": "Phase 1: Logic & Set Theory",
    "section": "2.4 Multiple Quantifiers",
    "text": "2.4 Multiple Quantifiers\nOften statements involve more than one quantifier.\nThe order matters!\n\n\\(\\forall x \\in \\mathbb{R},\\; \\exists y \\in \\mathbb{R}: y &gt; x\\)\nâ†’ True, because for every real number \\(x\\), we can pick \\(y = x+1\\).\n\\(\\exists y \\in \\mathbb{R},\\; \\forall x \\in \\mathbb{R}: y &gt; x\\)\nâ†’ False, because no single real number is greater than all real numbers.\n\nTip: Think of quantifiers as a kind of game:\n- For \\(\\forall x\\), your opponent chooses the worst possible \\(x\\).\n- For \\(\\exists y\\), you get to respond by picking a suitable \\(y\\).\nThe order decides who gets to â€œmoveâ€ first, and the outcome can change completely.",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html#why-this-matters",
    "href": "chapters/01_ch01.html#why-this-matters",
    "title": "Phase 1: Logic & Set Theory",
    "section": "2.5 Why This Matters",
    "text": "2.5 Why This Matters\nQuantifiers appear in almost every mathematical theorem.\n\nAnalysis (limits):\n\\[\n\\forall \\epsilon &gt; 0,\\; \\exists \\delta &gt; 0:\\; |x - a| &lt; \\delta \\;\\to\\; |f(x) - L| &lt; \\epsilon\n\\]\n(â€œFor every tolerance \\(\\epsilon\\), there exists a closeness \\(\\delta\\) that guarantees the function stays within that tolerance.â€)\nStatistics:\n\n\\(\\forall n,\\; \\exists \\hat{\\theta}_n:\\; \\hat{\\theta}_n \\to \\theta\\) (There exists an estimator consistent for \\(\\theta\\).)\n\n\\(\\exists\\) an unbiased estimator of \\(\\mu\\) (the sample mean).\n\nCausal Inference:\n\n\\(\\forall\\) randomized experiments, \\(\\exists\\) an unbiased estimator of the treatment effect.\n\n\nQuantifiers are the way mathematics formalizes sweeping claims like â€œalwaysâ€ and â€œsometimes,â€ which are at the heart of proofs and assumptions in Causal ML.",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html#mathematical-interpretation",
    "href": "chapters/01_ch01.html#mathematical-interpretation",
    "title": "Phase 1: Logic & Set Theory",
    "section": "2.6 Mathematical Interpretation",
    "text": "2.6 Mathematical Interpretation\nQuantifiers can look intimidating at first, but the real skill is learning how to read them.\nEvery quantified statement has two parts:\n1. the quantifier (\\(\\forall\\) or \\(\\exists\\)), and\n2. the predicate (a property of \\(x\\) that is claimed to hold).\n\n2.6.1 Single Quantifier Examples\n\nUniversal (\\(\\forall\\)):\n\\[\n\\forall x \\in \\mathbb{R},\\; x^2 \\geq 0\n\\]\nRead: â€œFor every real number \\(x\\), the square of \\(x\\) is nonnegative.â€\nInterpretation: This is true, because no real number squared gives a negative result.\nStatistics example:\n\\[\n\\forall n \\in \\mathbb{N},\\; \\operatorname{Var}(\\bar{X}_n) \\geq 0\n\\]\nRead: â€œFor every sample size \\(n\\), the variance of the sample mean is nonnegative.â€\nInterpretation: Always true, because variances can never be negative.\n\n\n\nExistential (\\(\\exists\\)):\n\\[\n\\exists x \\in \\mathbb{Z},\\; x^2 = 4\n\\]\nRead: â€œThere exists an integer whose square is 4.â€\nInterpretation: True, since \\(x = 2\\) and \\(x = -2\\) work.\nStatistics example:\n\\[\n\\exists \\;\\text{an estimator } \\hat{\\theta}\\; : \\; \\mathbb{E}[\\hat{\\theta}] = \\theta\n\\]\nRead: â€œThere exists an estimator whose expected value equals the true parameter.â€\nInterpretation: This is the definition of an unbiased estimator (e.g., the sample mean for \\(\\mu\\)).\n\n\n2.6.2 Multiple Quantifier Examples\nWhen quantifiers are combined, order matters.\n\nExample 1:\n\\[\n\\forall x \\in \\mathbb{R},\\; \\exists y \\in \\mathbb{R}:\\; y &gt; x\n\\]\nRead: â€œFor every real number \\(x\\), there exists a real number \\(y\\) that is greater than \\(x\\).â€\nTrue, because if someone hands you any \\(x\\), you can always respond with \\(y = x+1\\).\nStatistics example:\n\\[\n\\forall \\epsilon &gt; 0,\\; \\exists N \\in \\mathbb{N}:\\; n &gt; N \\;\\to\\; |\\bar{X}_n - \\mu| &lt; \\epsilon\n\\]\nRead: â€œFor every tolerance \\(\\epsilon\\), there exists a large enough sample size \\(N\\) such that if \\(n &gt; N\\), the sample mean is within \\(\\epsilon\\) of \\(\\mu\\).â€\nInterpretation: This is the definition of consistency (Law of Large Numbers).\n\n\n\nExample 2:\n\\[\n\\exists y \\in \\mathbb{R},\\; \\forall x \\in \\mathbb{R}:\\; y &gt; x\n\\]\nRead: â€œThere exists a real number \\(y\\) such that \\(y\\) is greater than every real number \\(x\\).â€\nFalse, because no single real number is larger than all others.\nStatistics example (false statement):\n\\[\n\\exists N \\in \\mathbb{N},\\; \\forall n &gt; N:\\; \\bar{X}_n = \\mu\n\\]\nRead: â€œThere exists a finite sample size \\(N\\) such that for all \\(n &gt; N\\), the sample mean equals the population mean exactly.â€\nFalse, because sampling variation never completely disappears â€” the sample mean only converges in probability, not with exact equality at some \\(N\\).\n\n\n2.6.3 How to Think About Multiple Quantifiers\nA useful way to think is as a game:\n\n\\(\\forall x\\) = your opponent picks a value of \\(x\\), possibly trying to make you fail.\n\n\\(\\exists y\\) = you get to respond by picking \\(y\\) to satisfy the condition.\n\nSo the statement\n\\[\n\\forall x \\in \\mathbb{R},\\; \\exists y \\in \\mathbb{R}:\\; y &gt; x\n\\]\nmeans: No matter what \\(x\\) your opponent picks, you can always respond with a suitable \\(y\\).\nBut the reverse order\n\\[\n\\exists y \\in \\mathbb{R},\\; \\forall x \\in \\mathbb{R}:\\; y &gt; x\n\\]\nmeans: You must pick one \\(y\\) that beats all possible \\(x\\). This is impossible, so the statement is false.\n\nWhy this section is important\nQuantifiers are everywhere in math, stats, and causal ML.\n\nUniversal quantifiers express generality:\n\nâ€œFor all sample sizes \\(n\\), \\(\\operatorname{Var}(\\bar{X}_n) \\geq 0\\).â€\n\nâ€œFor all \\(\\epsilon &gt; 0\\), there exists an \\(N\\) such that â€¦â€ (limits, consistency).\n\nExistential quantifiers express possibility:\n\nâ€œThere exists an unbiased estimator of \\(\\mu\\).â€\n\nâ€œThere exists a consistent estimator for every parameter.â€\n\nWith multiple quantifiers, the order of â€˜who chooses firstâ€™ changes the meaning dramatically.\nThis interpretative skill is essential for reading theorems correctly and avoiding misinterpretation.",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html#exercises-1",
    "href": "chapters/01_ch01.html#exercises-1",
    "title": "Phase 1: Logic & Set Theory",
    "section": "2.7 Exercises",
    "text": "2.7 Exercises\nGoal of these exercises:\n- Practice evaluating truth values.\n- Practice negating quantified statements.\n- Practice reading and interpreting symbolic logic in plain English.\nAt this stage, we are not proving statements â€” only learning to understand and translate them correctly.\n\nTruth values (universe of discourse: \\(\\mathbb{Z}\\)):\nFor each statement, decide whether it is true or false and explain why in words.\n\n\\(\\forall x,\\; x^2 \\geq 0\\)\n\n\\(\\exists x,\\; x^2 = 2\\)\n\nHint: In the first, think: â€œIs there any integer whose square is negative?â€\nIn the second, think: â€œIs there an integer whose square equals 2?â€\n\n\n\nNegation practice:\nWrite the logical negation of each statement and simplify.\n\n\\(\\forall x \\in \\mathbb{R},\\; x^2 \\geq 0\\)\n\n\\(\\exists x \\in \\mathbb{N},\\; x^2 = 2\\)\n\nHint: Use the rules:\n\\[\n\\lnot (\\forall x\\, P(x)) \\equiv \\exists x\\, \\lnot P(x), \\qquad\n\\lnot (\\exists x\\, P(x)) \\equiv \\forall x\\, \\lnot P(x).\n\\]\n\n\n\nQuantifier order:\nCarefully interpret the following statements in plain English.\nAre they true or false?\n\n\\(\\forall x \\in \\mathbb{R},\\; \\exists y \\in \\mathbb{R}: y &gt; x\\)\n\n\\(\\exists y \\in \\mathbb{R},\\; \\forall x \\in \\mathbb{R}: y &gt; x\\)\n\n\n\n\nTranslate into symbols:\nExpress the following in logical notation.\n\nâ€œEvery dataset has at least one outlier.â€\n\nâ€œThere exists a consistent estimator for every parameter.â€\n\n\n\n\nInterpret the following statistical statements (no proof needed):\n\n\\(\\forall n \\in \\mathbb{N},\\; \\operatorname{Var}(\\bar{X}_n) \\geq 0\\)\n(For every sample size \\(n\\), the variance of the sample mean is nonnegative.)\n\\(\\exists n \\in \\mathbb{N},\\; \\forall \\epsilon &gt; 0:\\; |\\bar{X}_n - \\mu| &lt; \\epsilon\\)\n(There exists a fixed sample size \\(n\\) such that the sample mean is always arbitrarily close to \\(\\mu\\). Is this realistic?)\n\\(\\forall \\epsilon &gt; 0,\\; \\exists N \\in \\mathbb{N}:\\; n &gt; N \\;\\to\\; |\\bar{X}_n - \\mu| &lt; \\epsilon\\)\n(Interpretation: This is the formal definition of consistency / the Law of Large Numbers).",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html#references",
    "href": "chapters/01_ch01.html#references",
    "title": "Phase 1: Logic & Set Theory",
    "section": "References",
    "text": "References\n\nVelleman, D. J. (2006). How to Prove It: A Structured Approach.\n\nRosen, K. H. (2011). Discrete Mathematics and Its Applications.\n\nSpanos, A. (1999, 2010). Probability Theory and Statistical Inference.",
    "crumbs": [
      "<span class='chapter-number'>4</span>Â  <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/99_ex01.html",
    "href": "chapters/99_ex01.html",
    "title": "Phase 4: The Mathematical Truth: Linear vs Logistic Regression",
    "section": "",
    "text": "How to beautifully explain the mathematical truth behind Linear and Logistic regression\n(Still under construction)",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Phase 4: The Mathematical Truth: Linear vs Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/99_ex01.html#the-theoretical-world-the-full-data-generating-process",
    "href": "chapters/99_ex01.html#the-theoretical-world-the-full-data-generating-process",
    "title": "Phase 4: The Mathematical Truth: Linear vs Logistic Regression",
    "section": "1. The Theoretical World: The Full Data-Generating Process",
    "text": "1. The Theoretical World: The Full Data-Generating Process\nIn the idealized mathematical world, we assume the existence of a Data-Generating Process (DGP) â€”\na mechanism that maps all causal inputs in the universe to the variable of interest \\(Y\\):\n\\[\nY = f(X_1, X_2, X_3, \\ldots)\n\\]\nEach \\(X_j\\) represents a different characteristic or influence (education, experience, intelligence, etc.),\nand together they fully determine the value of \\(Y\\).\nIf we had complete access to this \\(f(\\cdot)\\) and to all relevant \\(X_j\\)â€™s, the process would be deterministic:\ngiven the inputs, we could predict \\(Y\\) exactly.\nFormally, the joint distribution of all variables is\n\\[\n(Y, X_1, X_2, X_3, \\ldots) \\sim F_{Y, X_1, X_2, X_3, \\ldots}\n\\]\nand the DGP defines the conditional distribution\n\\[\nF_{Y \\mid X_1, X_2, X_3, \\ldots}(y \\mid x_1, x_2, x_3, \\ldots)\n\\]\nwhich would be perfectly degenerate (a Dirac delta) if the world were truly deterministic.",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Phase 4: The Mathematical Truth: Linear vs Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/99_ex01.html#the-linear-regression-case-deterministic-dgp-incomplete-knowledge",
    "href": "chapters/99_ex01.html#the-linear-regression-case-deterministic-dgp-incomplete-knowledge",
    "title": "Phase 4: The Mathematical Truth: Linear vs Logistic Regression",
    "section": "2. The Linear Regression Case: Deterministic DGP, Incomplete Knowledge",
    "text": "2. The Linear Regression Case: Deterministic DGP, Incomplete Knowledge\n\n2.1. From Theory to Practice\nIn the real world, we rarely observe all the variables that influence \\(Y\\).\nSuppose the true DGP is\n\\[\nY = 20 + 3X_1 + 2X_2\n\\]\nwhere \\(X_1\\) is years of education and \\(X_2\\) is years of experience.\nIf we knew both \\(X_1\\) and \\(X_2\\), the model would be deterministic:\nfor \\(X_1=10, X_2=5\\), we know \\(Y=20+3(10)+2(5)=50\\).\nBut imagine we only measure \\(X_1\\).\nWe must then approximate the conditional behavior of \\(Y\\) given \\(X_1\\):\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\varepsilon\n\\]\nThe term \\(\\varepsilon\\) now captures the influence of \\(X_2\\) (and any other unobserved factors):\n\\[\n\\varepsilon = 2X_2\n\\]\nand since \\(X_2\\) varies randomly in the population, \\(\\varepsilon\\) is random to us.\nThis motivates the probabilistic model\n\\[\nY \\mid X_1 = x_1 \\sim \\mathcal{N}(\\beta_0 + \\beta_1 x_1, \\sigma^2)\n\\]\nwhere the conditional variance \\(\\sigma^2\\) reflects epistemic randomness â€” randomness due to our ignorance about unobserved determinants.\nIn other words, the world might be deterministic, but our model is not.\n\n\n\n2.2. Conceptual Picture: â€œSlicingâ€ the Joint Distribution\nThe true DGP defines a high-dimensional surface in the space of all variables.\nBy conditioning on only one variable (\\(X_1\\)), we take a slice through that surface:\n\\[\nF_{Y \\mid X_1}(y \\mid x_1)\n\\]\nThis slice still contains variation from the omitted axes (\\(X_2, X_3, \\ldots\\)).\nThat variation appears as the random scatter of points around the regression line.\nHence, in linear regression:\n\nRandomness arises because our model conditions on an incomplete subset of the true DGP.\n\nIf we had access to all the relevant \\(X_j\\)â€™s and the true functional form \\(f\\),\nthe process would become deterministic and \\(Y\\) would be known with certainty.",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Phase 4: The Mathematical Truth: Linear vs Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/99_ex01.html#the-logistic-regression-case-a-probabilistic-dgp",
    "href": "chapters/99_ex01.html#the-logistic-regression-case-a-probabilistic-dgp",
    "title": "Phase 4: The Mathematical Truth: Linear vs Logistic Regression",
    "section": "3. The Logistic Regression Case: A Probabilistic DGP",
    "text": "3. The Logistic Regression Case: A Probabilistic DGP\nNow consider a fundamentally different scenario.\nLet \\(Y\\) represent whether a person gets hired (\\(Y=1\\)) or not hired (\\(Y=0\\)).\nWe still have predictors such as education \\(X_1\\) and experience \\(X_2\\).\nEven if we could measure every possible factor, two people with identical \\((X_1, X_2)\\) might still face different outcomes.\nThe process of hiring, medical recovery, or clicking an ad is inherently random at the individual level.\nHere, the DGP itself is stochastic:\n\\[\nY \\mid X_1, X_2 \\sim \\text{Bernoulli}(p(X_1, X_2))\n\\]\nwhere\n\\[\np(X_1, X_2) = P(Y=1 \\mid X_1, X_2)\n\\]\nThis means that the theoretical DGP is not a deterministic function \\(f(\\cdot)\\) but a probability law describing how likely each outcome is.\nWe specify the probability function \\(p(\\cdot)\\) using a logistic form:\n\\[\np(x_1, x_2) = \\frac{e^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2}}{1 + e^{\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2}}\n\\]\n\n\n3.1. Example: The Hiring Probability\nLet\n\\[\n\\beta_0 = -8, \\quad \\beta_1 = 0.5, \\quad \\beta_2 = 0.2\n\\]\nFor a candidate with 12 years of education (\\(X_1=12\\)) and 5 years of experience (\\(X_2=5\\)):\n\\[\np = \\frac{e^{-8 + 0.5(12) + 0.2(5)}}{1 + e^{-8 + 0.5(12) + 0.2(5)}}\n   = \\frac{e^{-1}}{1 + e^{-1}} \\approx 0.27\n\\]\nThus, even knowing all the predictors and parameters,\n\\[\nP(Y=1 \\mid X_1=12, X_2=5) = 0.27\n\\]\nmeans that among 100 identical candidates (same \\(X_1, X_2\\)),\naround 27 would be hired and 73 would not â€” but we cannot know which ones.\nEach individual outcome is generated by an independent Bernoulli(0.27) draw.\n\n\n\n\n\n\n\n\n\n\nCandidate\nEducation \\(X_1\\)\nExperience \\(X_2\\)\n\\(p_i\\)\nRealized \\(Y_i\\)\n\n\n\n\n1\n12\n5\n0.27\n0\n\n\n2\n12\n5\n0.27\n1\n\n\n3\n12\n5\n0.27\n0\n\n\n4\n12\n5\n0.27\n0\n\n\n5\n12\n5\n0.27\n1\n\n\n\nEven with complete knowledge of the DGP, \\(Y\\) remains random.\nThis randomness is ontological â€” it comes from the probabilistic nature of the DGP itself,\nnot from ignorance about missing variables.",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Phase 4: The Mathematical Truth: Linear vs Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/99_ex01.html#the-fundamental-distinction",
    "href": "chapters/99_ex01.html#the-fundamental-distinction",
    "title": "Phase 4: The Mathematical Truth: Linear vs Logistic Regression",
    "section": "4. The Fundamental Distinction",
    "text": "4. The Fundamental Distinction\n\n\n\n\n\n\n\n\nFeature\nLinear Regression\nLogistic Regression\n\n\n\n\nNature of DGP\nDeterministic function of all relevant \\(X_j\\)\nIntrinsically probabilistic (Bernoulli process)\n\n\nConditional distribution\n\\(Y \\mid X \\sim \\mathcal{N}(\\mu(X), \\sigma^2)\\)\n\\(Y \\mid X \\sim \\text{Bernoulli}(p(X))\\)\n\n\nRandomness arises from\nOmitted or unobserved variables (epistemic)\nThe event-generation mechanism itself (ontological)\n\n\nWhat the model approximates\nConditional mean \\(E[Y\\mid X] = \\mu(X)\\)\nSuccess probability \\(P(Y=1\\mid X) = p(X)\\)\n\n\nIf we knew all of \\(X\\) and the true DGP\n\\(Y\\) becomes deterministic\n\\(Y\\) remains random\n\n\nLimiting case (\\(p=0\\) or \\(1\\))\nPerfect prediction possible\nBecomes deterministic, but no longer logistic\n\n\nConceptual analogy\nâ€œThe world is deterministic but we canâ€™t see it all.â€\nâ€œThe world itself flips a weighted coin.â€\n\n\n\nHence:\n\nğŸ”¹ Linear regression models a deterministic world we only partially see.\nğŸ”¹ Logistic regression models a probabilistic world whose outcomes remain random even when all causes are known.",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Phase 4: The Mathematical Truth: Linear vs Logistic Regression</span>"
    ]
  },
  {
    "objectID": "chapters/99_ex01.html#appendix-conditioning-as-projection-on-subspaces-optional",
    "href": "chapters/99_ex01.html#appendix-conditioning-as-projection-on-subspaces-optional",
    "title": "Phase 4: The Mathematical Truth: Linear vs Logistic Regression",
    "section": "5. Appendix â€” Conditioning as Projection on Subspaces (Optional)",
    "text": "5. Appendix â€” Conditioning as Projection on Subspaces (Optional)\nIn the theoretical world, the joint distribution \\(F_{Y,X_1,X_2,\\ldots}\\) defines a high-dimensional space of relationships between all variables. Conditioning on a subset of them â€” for instance, only \\(X_1\\) â€” corresponds to taking a projection of that space onto a lower-dimensional subspace.\nIn linear algebraic terms, if we represent \\(Y\\) and the \\(X_j\\)â€™s as elements of a vector space of random variables with an inner product \\(\\langle A,B \\rangle = E[AB]\\), then the conditional expectation \\(E[Y \\mid X_1]\\) is precisely the orthogonal projection of \\(Y\\) onto the subspace spanned by \\(X_1\\).\nThe residual \\(\\varepsilon = Y - E[Y \\mid X_1]\\) is orthogonal to that subspace:\n\\[\nE[\\varepsilon X_1] = 0\n\\]\nThis provides a geometric interpretation of â€œtaking a sliceâ€ of the DGP: we restrict our attention to one axis (one subspace) of the full theoretical space, and the remaining variation appears as randomness in the orthogonal complement.\n\n\nSummary Insight:\nLinear regression transforms deterministic complexity into stochastic simplicity\nby conditioning on incomplete information.\nLogistic regression models inherent probabilistic events,\nwhere randomness persists even when all causes are known.",
    "crumbs": [
      "<span class='chapter-number'>5</span>Â  <span class='chapter-title'>Phase 4: The Mathematical Truth: Linear vs Logistic Regression</span>"
    ]
  }
]