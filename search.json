[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Foundations of Data Science & Causal ML: A Mathematical Journey",
    "section": "",
    "text": "Journey Phases\nThis is my structured learning roadmap to prepare for research in causal machine learning with mathematical rigor.\nHere you can find both mathematical foundations and applications.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Journey Phases</span>"
    ]
  },
  {
    "objectID": "index.html#why-im-interested-in-causal-ml",
    "href": "index.html#why-im-interested-in-causal-ml",
    "title": "Foundations of Data Science & Causal ML: A Mathematical Journey",
    "section": "Why I’m Interested in Causal ML?",
    "text": "Why I’m Interested in Causal ML?\nGoal: Show the reader the contrasts between two “Data Cultures”.\n- Statistics and econometrics build models on top of stochastic mechanisms (data-generating processes), aiming for explanation and inference.\n- Machine learning often ignores mechanisms and focuses on prediction accuracy, generalization, and performance.\n- Causal ML is a synthesis: it combines ML’s flexibility with statistics’ concern for identification, adding a causal lens to reason about interventions and counterfactuals.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Journey Phases</span>"
    ]
  },
  {
    "objectID": "index.html#phase-1-logic-set-theory",
    "href": "index.html#phase-1-logic-set-theory",
    "title": "Foundations of Data Science & Causal ML: A Mathematical Journey",
    "section": "Phase 1 – Logic & Set Theory",
    "text": "Phase 1 – Logic & Set Theory\nGoal: Build comfort with the language of mathematics.\n- Proof techniques: direct, contrapositive, contradiction, induction.\n- Sets and families of sets, Cartesian products, power sets.\n- Functions: injective, surjective, bijective.\n- Relations: equivalence relations, partial orders.\n- Cardinality: countable vs. uncountable sets.\nTheory Output: - Sentential logic (Velleman Ch. 1) - Predicate logic & quantifiers (Ch. 2) - Proof techniques (direct, contrapositive, contradiction, induction) (Ch. 3) - Sets, relations, functions (Ch. 4–6) - Countability, infinity (Ch. 9 selected, Tao’s appendix)\nApplication Project: - SQL/database operations as set theory (joins, unions, intersections). - Prove/discuss equivalences (e.g., idempotency: SELECT DISTINCT twice = once)\nReferences:\n- Velleman – How to Prove It\n- Enderton – Set Theory",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Journey Phases</span>"
    ]
  },
  {
    "objectID": "index.html#phase-2-real-analysis",
    "href": "index.html#phase-2-real-analysis",
    "title": "Foundations of Data Science & Causal ML: A Mathematical Journey",
    "section": "Phase 2 – Real Analysis",
    "text": "Phase 2 – Real Analysis\nGoal: Rigorous calculus and convergence, revisiting classical calculus concepts with proofs.\n\nSequences, series, limits.\n\nContinuity, compactness, connectedness.\n\nDifferentiation: Mean Value Theorem, Taylor expansion.\n\nRiemann integration (rigorous foundation).\n\nUniform convergence.\n\nApplied Calculus Lens:\n- Multivariable calculus: partial derivatives, gradients, Jacobians, Hessians.\n- Convexity and optimization.\n- Taylor expansions for approximation.\n- Fundamental Theorem of Calculus as link to probability expectations.\nTheory Output: ε–δ proofs, compactness in ℝ, uniform convergence examples.\nApplication Project: Gradient descent convergence demo; connect convexity to logistic regression loss.\nReferences:\n- Rudin – Principles of Mathematical Analysis (Baby Rudin)\n- Tao – Analysis I",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Journey Phases</span>"
    ]
  },
  {
    "objectID": "index.html#phase-3-linear-algebra",
    "href": "index.html#phase-3-linear-algebra",
    "title": "Foundations of Data Science & Causal ML: A Mathematical Journey",
    "section": "Phase 3 – Linear Algebra",
    "text": "Phase 3 – Linear Algebra\nGoal: Move beyond computation to proofs and structure.\n- Vector spaces, subspaces, linear independence, bases, dimension.\n- Linear transformations and matrices.\n- Inner product spaces, orthogonality, Gram–Schmidt.\n- Determinants, eigenvalues, eigenvectors, diagonalization.\n- Spectral theorem, singular value decomposition.\n- Matrix norms and conditioning.\nTheory Output: Proofs of rank–nullity theorem, spectral theorem for symmetric matrices, SVD existence.\nApplication Project: PCA from first principles — prove orthogonal diagonalization, then implement PCA via SVD.\nReferences:\n- Axler – Linear Algebra Done Right\n- Friedberg, Insel & Spence – Linear Algebra\n- Trefethen & Bau – Numerical Linear Algebra (for computational aspects)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Journey Phases</span>"
    ]
  },
  {
    "objectID": "index.html#phase-4-functional-analysis-hilbert-spaces",
    "href": "index.html#phase-4-functional-analysis-hilbert-spaces",
    "title": "Foundations of Data Science & Causal ML: A Mathematical Journey",
    "section": "Phase 4 – Functional Analysis & Hilbert Spaces",
    "text": "Phase 4 – Functional Analysis & Hilbert Spaces\nGoal: Develop the tools to handle infinite-dimensional vector spaces, operators, and kernels.\n- Normed vector spaces, Banach spaces.\n- Hilbert spaces, orthogonality, projections.\n- Bounded linear operators.\n- Reproducing Kernel Hilbert Spaces (RKHS).\nTheory Output: Prove projection theorem in Hilbert spaces, examples of bounded/unbounded operators, RKHS construction.\nApplication Project: Kernelized regression and SVMs — connect functional analysis with machine learning models.\nReferences:\n- Kreyszig – Introductory Functional Analysis with Applications\n- Conway – A Course in Functional Analysis\n- Berlinet & Thomas-Agnan – Reproducing Kernel Hilbert Spaces in Probability and Statistics",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Journey Phases</span>"
    ]
  },
  {
    "objectID": "index.html#phase-5-topology-measure-theory",
    "href": "index.html#phase-5-topology-measure-theory",
    "title": "Foundations of Data Science & Causal ML: A Mathematical Journey",
    "section": "Phase 5 – Topology & Measure Theory",
    "text": "Phase 5 – Topology & Measure Theory\nGoal: Learn the structures that underlie probability theory.\n- Metric spaces, open/closed sets.\n- Compactness and product spaces.\n- σ-algebras, measurable functions.\n- Lebesgue measure and integration.\n- Convergence theorems: MCT, DCT.\nTheory Output: Worked examples of σ-algebras, Lebesgue integral, and convergence theorems.\nApplication Project: Fraud detection via Monte Carlo — rare events and measure-zero sets in anomaly detection.\nReferences:\n- Munkres – Topology\n- Schilling – Measures, Integrals and Martingales",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Journey Phases</span>"
    ]
  },
  {
    "objectID": "index.html#phase-6-probability",
    "href": "index.html#phase-6-probability",
    "title": "Foundations of Data Science & Causal ML: A Mathematical Journey",
    "section": "Phase 6 – Probability",
    "text": "Phase 6 – Probability\nGoal: Define probability rigorously à la Kolmogorov.\n- Probability spaces and random variables as measurable functions.\n- Distributions, independence, product measures.\n- Conditional expectation as L² projection.\n- Laws of large numbers, central limit theorem.\n- Intro to martingales.\nTheory Output: Probability space construction, LLN/CLT proofs, conditional expectation as projection.\nApplication Project: A/B testing simulation — CLT and confidence intervals for conversion rates.\nReferences:\n- Durrett – Probability: Theory and Examples\n- Klenke – Probability Theory",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Journey Phases</span>"
    ]
  },
  {
    "objectID": "index.html#phase-7-mathematical-statistics",
    "href": "index.html#phase-7-mathematical-statistics",
    "title": "Foundations of Data Science & Causal ML: A Mathematical Journey",
    "section": "Phase 7 – Mathematical Statistics",
    "text": "Phase 7 – Mathematical Statistics\nGoal: Connect probability → inference.\n- Point estimation: MLE, method of moments.\n- Properties: unbiasedness, consistency, efficiency.\n- Hypothesis testing and likelihood ratio tests.\n- Asymptotic results: convergence in probability/distribution, delta method.\nTheory Output: Consistency of MLE, hypothesis testing framework, asymptotic normality proofs.\nApplication Project: Logistic regression for churn prediction — prove Bernoulli MLE consistency, simulate convergence, apply to real dataset.\nReferences:\n- Casella & Berger – Statistical Inference",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Journey Phases</span>"
    ]
  },
  {
    "objectID": "index.html#phase-8-causality",
    "href": "index.html#phase-8-causality",
    "title": "Foundations of Data Science & Causal ML: A Mathematical Journey",
    "section": "Phase 8 – Causality",
    "text": "Phase 8 – Causality\nGoal: Enter causal inference with strong mathematical foundations.\n- Pearl’s Structural Causal Models & do-calculus.\n- Rubin’s potential outcomes framework.\n- Invariant causal prediction (Peters, Janzing, Schölkopf).\n- Identifiability proofs.\n- Axiomatic frameworks (Park & Muandet).\nTheory Output: Worked proofs of identifiability, back-door/front-door criteria, do-calculus rules.\nApplication Project: Uplift modeling for churn retention OR reproduction of Chernozhukov’s Double Machine Learning estimator with Python.\nReferences:\n- Pearl – Causality\n- Peters, Janzing, Schölkopf – Elements of Causal Inference\n- Chernozhukov et al. – Causal Machine Learning papers",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Journey Phases</span>"
    ]
  },
  {
    "objectID": "chapters/about.html",
    "href": "chapters/about.html",
    "title": "About Me",
    "section": "",
    "text": "What I’m doing now\nHi, I’m Caio Velasco 👋. Mechanical Engineer from Brazil with a Master’s in Economics & Public Policy (UCLA), thanks to a full scholarship from the Lemann Foundation. In 2021, I stepped away from a PhD in Economics in the Netherlands to support my family during the pandemic and redirected my focus fully toward Data Science.\nI work at the intersection of engineering, economics, and data—spanning Business Analysis, Consulting, Analytics Engineering, and Data Science across the US, UK, Spain, and Brazil. I enjoy turning messy, fragmented systems into robust end-to-end data platforms and pairing that with mathematical depth (statistics, econometrics, and causal ML) to drive real-world impact and avoid the correlation vs. causation dilemma.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>About Me</span>"
    ]
  },
  {
    "objectID": "chapters/about.html#what-im-doing-now",
    "href": "chapters/about.html#what-im-doing-now",
    "title": "About Me",
    "section": "",
    "text": "Building an open book: Foundations of Data Science & Causal ML: A Mathematical Journey (mathematical rigor + applications).\nHelping teams modernize their stacks (ingestion → modeling with dbt → warehouses like Snowflake/Redshift → governance & Looker dashboards).\nDeepening foundations in real analysis, probability, statistics, and causal inference to support future research.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>About Me</span>"
    ]
  },
  {
    "objectID": "chapters/about.html#selected-highlights",
    "href": "chapters/about.html#selected-highlights",
    "title": "About Me",
    "section": "Selected highlights",
    "text": "Selected highlights\n\nAwards & scholarships from Yale University, UCLA, GE Foundation, Lemann Foundation, and The Club of Rome.\nEarly career: helped build Stone Payments (NASDAQ: STNE) in Brazil from scratch and founded MePrepara, an online math prep platform with 140+ videos supporting low-income Brazilian students.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>About Me</span>"
    ]
  },
  {
    "objectID": "chapters/about.html#focus-areas",
    "href": "chapters/about.html#focus-areas",
    "title": "About Me",
    "section": "Focus areas",
    "text": "Focus areas\n\nData & Analytics Engineering: Python · SQL · dbt · Snowflake/Redshift · AWS · Looker · best practices for modeling, tests, CI/CD.\nStat/ML & Causality: mathematical statistics, econometrics, causal inference.\nEducation: writing clear, didactic materials; mentoring; building bridges between theory and practice.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>About Me</span>"
    ]
  },
  {
    "objectID": "chapters/about.html#values-approach",
    "href": "chapters/about.html#values-approach",
    "title": "About Me",
    "section": "Values & approach",
    "text": "Values & approach\n\nRigor + practicality. Start from first principles, then ship value.\nClarity. Well-structured code and always documentating the process didacticaly.\nImpact. Data and technology as tools to expand opportunity—because education changed my life.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>About Me</span>"
    ]
  },
  {
    "objectID": "chapters/about.html#contact-links",
    "href": "chapters/about.html#contact-links",
    "title": "About Me",
    "section": "Contact & links",
    "text": "Contact & links\n\nPortfolio: https://caiocvelasco.github.io/\nGitHub: https://github.com/caiocvelasco\nLinkedIn: https://www.linkedin.com/in/caiocvelasco/",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>About Me</span>"
    ]
  },
  {
    "objectID": "chapters/00_ch00.html",
    "href": "chapters/00_ch00.html",
    "title": "Why I’m Interested in Causal ML?",
    "section": "",
    "text": "The 2 Worlds\nMy interest in Causal Machine Learning arises from a long-standing divide in the way different worlds think about data.\nIn his influential essay Statistical Modeling: The Two Cultures (2001), Leo Breiman described two very different traditions.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Why I'm Interested in Causal ML?</span>"
    ]
  },
  {
    "objectID": "chapters/00_ch00.html#the-2-worlds",
    "href": "chapters/00_ch00.html#the-2-worlds",
    "title": "Why I’m Interested in Causal ML?",
    "section": "",
    "text": "Data Modeling culture (the dominant one in Statistics at the time): assumes data are generated by a stochastic model with explicit assumptions (e.g., linear regression, logistic regression). The focus is on inference and interpretability.\nAlgorithmic Modeling culture (rising in Machine Learning): does not assume a specific stochastic DGP, but uses flexible algorithms (e.g., random forests, neural networks) to directly optimize predictive accuracy.\n\n\nThe Statistics (and Econometrics) World\nDid you know that when you run a regression, you are implicitly making a very strong claim about how the world works?\nIn this tradition, everything begins with the data-generating process (DGP) — what Aris Spanos in his book Probability Theory and Statistical Inference: Econometric Modeling with Observational Data calls a stochastic mechanism. Imagine it as a hidden machine tossing probabilistic dice, producing the data we observe. A statistical model is not just a tool; it is a story about that hidden mechanism that is producing the outcomes we observe.\nExamples:\n- A linear regression assumes wages, test scores, or prices are generated as a straight-line combination of inputs plus random noise.\n- A Poisson model assumes counts come from a specific probabilistic law.\n- A time-series AR(1) model assumes today’s value depends on yesterday’s value with some random shock added in.\nIf the DGP is hidden, what do you think we want to do? To uncover its truth.\nThat is why introductory statistics courses begin with the notion of a Population and then introduce Samples. Since we cannot observe the entire population, we study samples and rely on probabilistic laws — the Law of Large Numbers (LLN), the Central Limit Theorem (CLT) — to argue that repeated samples will “stabilize” around the truth about some population parameters (mean, median, variance…).\nFrom there comes Inference:\n&gt; If I trust my assumptions about the DGP, then I can use my sample to make representative claims about the population.\nThis is the heart of statistics and econometrics: model the hidden process, estimate its parameters, and test whether your story holds up against data. But — and this is crucial — the DGP is always an assumption. If it is wrong, the inference may be misleading.\n\n\nThe Machine Learning World\nNow comes the other world. The algorithmic modeling culture. Here, the hidden machine is ignored. No one asks whether wages are “truly linear” in education, or whether prices follow a Poisson law, or whether the dice is probabilistic. Instead, the question is brutally pragmatic: Can I predict well?\nIn this world, the central drama is not about whether the stochastic story is correct, but about whether a model trained on one dataset (training set) generalizes to unseen data (the test set). This is the training vs. testing problem.\nThe classical core risks are:\n- Underfitting: the model is too simple, missing patterns (high bias).\n- Overfitting: the model is too complex, capturing noise instead of signal (high variance).\nSuccess here is measured by predictive accuracy, not by unbiasedness or efficiency of estimators, like in the Statistics world. Interpretability is optional (if you don’t know what is inside a black box, you cannot “read” it and “interpret” it); performance is everything. Predictions may work well in stable environments but fail when the underlying data distribution shifts.\n\nA concrete example makes the distinction clearer — and shows how each culture fails where the other succeeds:\n\nEstimating the causal effect of education on wages.\nIn econometrics and statistics, I would model wages as generated by a stochastic mechanism involving education, experience, and other covariates. My goal is to isolate the effect of education from confounders. If I only used a machine learning model, it might predict wages very well — but it would not tell me the causal role of education (maybe it is not education itself that raises wages, but family wealth that enables both schooling and better jobs), because prediction does not separate correlation from causation. ML excels at prediction, but fails to deliver interpretability and identification here.\nPredicting house prices.\nEconometrics might specify a linear model with a few chosen variables and assumptions about errors. But this DGP is likely too simplistic: house prices depend on thousands of complex, nonlinear factors (location, amenities, school ratings, neighborhood trends). An econometric model here may fail badly in predictive accuracy. By contrast, ML models like gradient boosting can capture these nonlinearities and achieve much higher predictive power — even though they may not explain why prices are high.\n\nSo each culture shines in one domain and struggles in the other. Together, they suggest the need for a synthesis.\n\nCausal ML in Action\nA famous example comes from Online Advertising.\n- A standard supervised ML model might predict that showing more ads increases purchases, because ads and purchases are positively correlated in the data.\n- But when economists ran randomized experiments, they found the causal effect was often much smaller — sometimes zero. Why? Because people who were already likely to buy were also the ones most likely to be targeted with ads.\nThis is a case where prediction looked strong, but causality revealed the true relationship and new decisions had to be made.\n\n\n\n\nWhy Causal ML Is a Synthesis\n✨ This is why Causal Machine Learning excites me.\nBy combining machine learning with the discipline of causal inference, researchers such as Victor Chernozhukov, Christian Hansen, Nathan Kallus, Martin Spindler, and Vasilis Syrgkanis — co-authors of Causal ML: Applied Causal Inference Powered by ML and AI — show that we can take the best of both worlds.\nThe basic idea is simple:\n- From machine learning, we borrow flexible algorithms that can handle many variables and complex patterns.\n- From statistics and econometrics, we borrow tools that check whether we are uncovering cause-and-effect rather than just correlations.\nTechniques like sample splitting and cross-fitting might sound technical, but the intuition is clear: we let the algorithm “learn” patterns on one part of the data, and we keep another part untouched to see whether those patterns still hold when drawing causal conclusions. This helps us avoid fooling ourselves with spurious results.\nCausal thinking also pushes us beyond the tidy assumption that all observations are IID (independent and identically distributed). Real life changes: policies shift, markets move, and people adapt. Causal inference gives us language to reason about such changes — about interventions (“what if we changed X?”), counterfactuals (“what would have happened otherwise?”), and distribution shifts (“what if tomorrow doesn’t look like yesterday?”).\nCausal ML suggests a synthesis:\n- From statistics and econometrics, we inherit the concern with stochastic mechanisms, identification, and rigorous inference.\n- From machine learning, we gain flexibility, robustness, and predictive accuracy.\n- From causality, we add the language to generalize beyond observation, to interventions and explanations.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Why I'm Interested in Causal ML?</span>"
    ]
  },
  {
    "objectID": "chapters/00_ch00.html#focus",
    "href": "chapters/00_ch00.html#focus",
    "title": "Why I’m Interested in Causal ML?",
    "section": "Focus",
    "text": "Focus\nMy main focus in this project is to learn — and become comfortable with — the mathematical foundations that underpin Causal Machine Learning.\nWhile Causal ML is an exciting and active field (and may well be my future research direction), the broader point is that mathematical depth makes a difference. Too often, methods are applied skillfully but without a full grasp of what is happening underneath.\nBy working through the logical and mathematical foundations step by step, I aim to develop the clarity that allows not only the application of tools, but a genuine understanding of them. The goal is to bridge the cultures of explanation and prediction — and, in some small way, contribute to the ongoing effort to unify them.\nWhat “foundations” means here\n- Logic & Set Theory — the language of proofs and structures.\n- Real Analysis — rigor of limits, continuity, convergence, integration.\n- Linear Algebra — vector spaces, eigen-structure, decompositions.\n- Functional Analysis & Hilbert Spaces — norms, projections, RKHS.\n- Topology & Measure Theory — σ-algebras, Lebesgue integration, convergence theorems.\n- Probability — Kolmogorov framework, LLN/CLT, conditional expectation.\n- Mathematical Statistics — estimation, tests, asymptotics.\n- Causality — SCMs, potential outcomes, identifiability, modern Causal ML.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Why I'm Interested in Causal ML?</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html",
    "href": "chapters/01_ch01.html",
    "title": "Phase 1: Logic & Set Theory",
    "section": "",
    "text": "0. Mathematics, Axioms, and Science\nMathematics begins with axioms — assumptions we agree to accept without proof — and builds everything else from them using precise rules of logic.\nThis process of axiomatization is what gives mathematics its unique clarity: starting from simple, explicit principles, we construct entire theories.\nWhy is this important?\n- Axioms are not “true” in an absolute sense. They are starting points.\n- What matters is whether the consequences drawn from them are logically consistent and useful for understanding the world.\n- In practice, this makes mathematics a language in which science can express and test ideas.\nMathematics provides the rigorous framework for turning assumptions into predictions.\nFor example, in Euclidean geometry we assume as an axiom:\nBut this depends on the “space” we are working in.\nIntuitively:\nNow consider two points on the equator that are exactly opposite each other (say Rio de Janeiro and Jakarta).\nThe key idea is that “straight line” really means shortest possible path given the geometry of the space.\nOn a plane, that’s the familiar straight line; on a sphere, it’s a great circle.\nThe notion of a geodesic generalizes this idea: whatever the space, it tells us the natural way to connect two points as efficiently as possible.\nThis illustrates a key point:\nThey are conventions that define the system we are working in.\nWithin their proper context (e.g., Euclidean geometry), their consequences are logically consistent (no contradictions arise) and useful (for designing buildings, maps, and bridges). But in other contexts (e.g., spherical geometry), we need a different set of axioms.\nIn the same way, in probability we may assume:\nReal-world data often violates this (dependence in time series, non-identical distributions in heterogeneous populations).\nBut within the i.i.d. framework, the consequences are logically consistent and extremely useful: they give us fundamental results like the Law of Large Numbers and the Central Limit Theorem.\nThe analogy with geometry is this:\nSimilarly, in probability:\nWhy we begin with Logic and Set Theory\nTogether, Logic and Set Theory form the grammar and vocabulary of mathematics.\nThey are not yet “about the real world” — but they give us the precise tools to state assumptions and derive consequences.\nIntuition for research\nWhen you see theoretical work in physics, economics, or causal machine learning, it often starts by axiomatizing the problem:\n- Define the objects (e.g. random variables, causal spaces).\n- Specify assumptions as axioms (e.g. independence, stability, interventions).\n- Derive results rigorously from there.\nBy starting here, we are learning how to speak the language of mathematics before applying it to inference, probability, and causality.\nThus, before diving into analysis and probability, we establish a foundation in logic and set theory. This is important to formalize assumptions, express mathematical objects precisely, and build proofs with rigor.\nLogic gives us the language to connect premises and conclusions, while Set Theory gives us the structure to define universes of discourse, events, functions, probability spaces, etc.\nTherefore: Yes, it would be essential to learn Logic and Set Theory!",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html#statements-propositions-predicates-and-connectives",
    "href": "chapters/01_ch01.html#statements-propositions-predicates-and-connectives",
    "title": "Phase 1: Logic & Set Theory",
    "section": "1.1 Statements, Propositions, Predicates, and Connectives",
    "text": "1.1 Statements, Propositions, Predicates, and Connectives\n💡Motivation\nLearning statements and connectives is like learning the alphabet of mathematics.\n- If you cannot distinguish valid statements, you cannot even start a proof.\nExample in causal inference:\n- \\(p\\): “The model includes all relevant confounders.”\n- \\(q\\): “The conditional independence assumption holds.”\n- Then “If the model includes all relevant confounders, then the conditional independence assumption holds” is \\(p \\to q\\).\nWithout connectives, we’d stay in informal language. With them, we can formalize statements and reason rigorously about consequences, such as proving that a set of assumptions implies consistency of an estimator, showing that ignorability implies identification of a treatment effect, or demonstrating that conditional independence leads to factorization of a probability distribution into simpler components.\nA Statement (or Proposition) is a declarative sentence that is either true or false.\n\nExample: “3 is even” (false), “Barcelona is in Spain” (true).\n\nA Predicate is like a “template” for a statement: it depends on a variable and becomes a statement once you specify the value. For example:\n\\(P(x): x &gt; 0\\) (this is a Predicate or a “template”)\n- \\(P(2)\\) → “2 &gt; 0” (This is a Statement, with logic value “true”).\n- \\(P(-1)\\) → “-1 &gt; 0” (This is a Statement, with logic value “false”).\nConnectives are logical “operators” that allow us to combine simple statements into more complex ones.\nThey let us reason systematically instead of relying on vague language.\nBut why do we want to combine simple statements into more complex ones?\nBecause in mathematics (and especially in statistics), we rarely deal with isolated facts.\nWe deal with assumptions and want to know what conclusions they imply.\nConnectives give us the structure to move from “pieces of information” to a logical argument.\nExample from statistics:\n\n\\(p\\): “The sample is i.i.d.”\n\n\\(q\\): “The variance of the sample is finite.”\n\n\\(r\\): “The sample mean converges to the true mean (Law of Large Numbers).”\n\nIn words:\n“If the sample is i.i.d. and the variance is finite, then the sample mean converges.”\nSymbolically:\n\\[\n(p \\land q) \\to r\n\\]\nThis is exactly how many theorems are written: conditions (joined with \\(\\land\\)) imply a result (\\(\\to\\)).\nSo connectives are not just about making compound sentences longer.\nThey are about giving us a precise language of assumptions and consequences.\n\nThis is the foundation of mathematical reasoning: once we can formalize, we can prove.\n\nLet’s see them one by one with examples:\n\nNegation (\\(\\lnot p\\))\nMeaning: “not \\(p\\)”\n\nIf \\(p\\) is: “The number 5 is even” (false),\n\nThen \\(\\lnot p\\) is: “The number 5 is not even” (true).\n\n\n\n\nConjunction (\\(p \\land q\\))\nMeaning: “\\(p\\) and \\(q\\)” → true only when both are true.\n\n\\(p\\): “2 is even” (true)\n\n\\(q\\): “3 is prime” (true)\n\n\\(p \\land q\\): “2 is even and 3 is prime” (true).\n\nIf either part were false, the whole conjunction would be false.\n\n\n\n\nDisjunction (\\(p \\lor q\\))\nMeaning: “\\(p\\) or \\(q\\)” → true if at least one is true.\n\n\\(p\\): “Barcelona is in Spain” (true)\n\n\\(q\\): “Lisbon is in Brazil” (false)\n\n\\(p \\lor q\\): “Barcelona is in Spain or Lisbon is in Brazil” (true).\n\n(Note: In logic, “or” usually means inclusive or — at least one true, possibly both.)\n\n\n\nConditional (\\(p \\to q\\))\nMeaning: “if \\(p\\) then \\(q\\)” → false only if \\(p\\) is true and \\(q\\) is false.\n\n\\(p\\): “It rains.”\n\n\\(q\\): “The ground is wet.”\n\n\\(p \\to q\\): “If it rains, then the ground is wet.”\n\nTrue if it rains and the ground is wet (promise kept).\n\nFalse if it rains but the ground is not wet (promise broken).\n\nTrue if it doesn’t rain (vacuously true).\n\n\n\n\n\nBiconditional (\\(p \\leftrightarrow q\\))\nMeaning: “\\(p\\) if and only if \\(q\\)” → true when \\(p\\) and \\(q\\) have the same truth value.\n\n\\(p\\): “Today is Saturday.”\n\n\\(q\\): “Tomorrow is Sunday.”\n\n\\(p \\leftrightarrow q\\): “Today is Saturday if and only if tomorrow is Sunday.” → true.\n\nIf one part is true and the other false, the biconditional is false.\n\n\nThus:\n- Predicate: general template (open sentence, truth depends on a variable) -&gt; becomes true or false only when a variable is given a value.\n- Proposition/statement: instance of that template (closed sentence, definite truth) -&gt; something that is already true or false.\n- Connectives: operators that take simple propositions and form compound propositions.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html#truth-tables",
    "href": "chapters/01_ch01.html#truth-tables",
    "title": "Phase 1: Logic & Set Theory",
    "section": "1.2 Truth Tables",
    "text": "1.2 Truth Tables\n💡Motivation\nNow that we know connectives allow us to combine assumptions and conclusions into structured arguments, we need a way to check the consistency and behavior of these arguments.\nWhy? Because once statements get more complex, our intuition alone isn’t reliable.\nFor example, the conditional \\(p \\to q\\) often confuses beginners. Why? Because in everyday language, “if… then…” feels different from how logic defines it.\nThe Concept of “Promise”\n\nWe naturally read a conditional like \\(p \\to q\\) as: “If it rains, then the ground gets wet.”\nThink of \\(p \\to q\\) as a promise:\n\n\\(p\\): It rains.\n\n\\(q\\): The ground gets wet.\n\nSo \\(p \\to q\\) means: “I promise that if it rains, then the ground will get wet.”\nWe also naturally think that:\n1. If it rains and the ground is wet → ✅ promise kept.\n2. If it rains and the ground is not wet → ❌ promise broken.\nSo far, this matches intuition.\nBut what if it doesn’t rain?\n- Logic says: the conditional is still true, no matter whether the ground is wet or dry.\n- Why? Because the promise “if it rains, then the ground gets wet” has not been broken — the condition (\\(p\\)) never happened in the first place.\n- This is what we call vacuous truth.\nThis feels counterintuitive, and that’s exactly why we need a systematic tool.\nWhy Truth Tables?\nTruth tables are the grammar checker of logic. They resolve this confusion:\n\nThey let us mechanically verify whether compound statements are valid, equivalent, tautological, or contradictory.\n\nThey are a first step toward building formal proofs, because they allow us to test the “mechanics” of logical structure before tackling deeper arguments.\n\nThink of them as a microscope for logical statements:\nwhen connectives weave sentences together, truth tables let us zoom in and see all possible truth scenarios at once.\nThey also let us test whether two statements are logically equivalent. For example:\n\n“If not \\(p\\), then not \\(q\\).”\n\n“If \\(q\\), then \\(p\\).”\n\nAre these equivalent? Intuition may fail, but a Truth Table makes it crystal clear!\nExample in statistics:\n\nIf the data behave nicely (i.i.d. + finite variance), then the sample mean is reliable (it will converge to the true mean) (\\(p \\to q\\)).\n\nContrapositive: If the sample mean does not converge to the true mean, then the data were not i.i.d. with finite variance (\\(\\lnot q \\to \\lnot p\\)).\n\nA truth table shows how the truth value of a compound statement depends on its parts.\nExample: Prove that an implication (\\(p \\to q\\)) is equivalent (\\(\\equiv\\)) to its contrapositive (\\(\\lnot q \\to \\lnot p\\)).\nWe want to show: \\(p \\to q \\;\\equiv\\; \\lnot q \\to \\lnot p\\)\n\n\n\n\n\n\n\n\n\n\n\n\\(p\\)\n\\(q\\)\n\\(p \\to q\\)\n\\(\\lnot q\\)\n\\(\\lnot p\\)\n\\(\\lnot q \\to \\lnot p\\)\n\n\n\n\nT\nT\nT\nF\nF\nT\n\n\nT\nF\nF\nT\nF\nF\n\n\nF\nT\nT\nF\nT\nT\n\n\nF\nF\nT\nT\nT\nT\n\n\n\nSince the last two columns match, the implication is equivalent to its contrapositive.\nExample: Analysis of a Conditional\nTake \\(p\\): It rains, and \\(q\\): The ground gets wet.\nThe statement \\(p \\to q\\) is a promise: “If it rains, then the ground gets wet.”\n\n\n\n\\(p\\)\n\\(q\\)\n\\(p \\to q\\)\nExplanation\n\n\n\n\nT\nT\nT\n✅ promise kept\n\n\nT\nF\nF\n❌ promise broken\n\n\nF\nT\nT\nvacuously true (condition never triggered)\n\n\nF\nF\nT\nvacuously true (condition never triggered)\n\n\n\nKey intuition:\n- An implication is only false when the condition happens but the promised result fails.\n- In all other cases, the promise has not been broken, so the implication is true.\nThis is why statements like “All unicorns have horns” are technically true: since there are no unicorns, the condition never triggers, and the statement cannot be falsified.\n\n\nExercises\n\nExercise 1. Show that \\(p \\to q\\) is equivalent to \\(\\lnot p \\lor q\\).\nStep 1. Write the two statements side by side: \\(p \\to q\\) and \\(\\lnot p \\lor q\\).\nStep 2. Build a truth table with columns for \\(p\\), \\(q\\), \\(\\lnot p\\), \\(p \\to q\\), and \\(\\lnot p \\lor q\\).\nStep 3. Compare the last two columns.\nStep 4. Conclude: if they match in all rows, the statements are logically equivalent.\n\nSolution\n\nStep 1: Write the two statements side by side: \\(p \\to q\\) and \\(\\lnot p \\lor q\\).\nWhat we want to check is whether these two always mean the same thing.\n- \\(p \\to q\\): “If \\(p\\), then \\(q\\).”\n- \\(\\lnot p \\lor q\\): “Either not \\(p\\), or \\(q\\).”\nAt first sight they look different, but we suspect they are really saying the same thing.\nTo confirm, we will compare their truth tables row by row.\n\nStep 2: Build a truth table with columns for \\(p\\), \\(q\\), \\(\\lnot p\\), \\(p \\to q\\), and \\(\\lnot p \\lor q\\).\n(Didactic hint: to fill this correctly, recall the “grammar” of each connective.)\n- Negation \\(\\lnot p\\): flips the truth value (T→F, F→T).\n- Disjunction \\(A \\lor B\\): true if at least one of \\(A,B\\) is true.\n- Conditional \\(p \\to q\\): think “promise” — it is false only when the promise is broken (i.e., \\(p\\) is T but \\(q\\) is F); otherwise true (including when \\(p\\) is F: vacuous truth).\n\n\n\n\\(p\\)\n\\(q\\)\n\\(\\lnot p\\)\n\\(p \\to q\\)\n\\(\\lnot p \\lor q\\)\n\n\n\n\nT\nT\nF\nT\nT\n\n\nT\nF\nF\nF\nF\n\n\nF\nT\nT\nT\nT\n\n\nF\nF\nT\nT\nT\n\n\n\nHow we filled it:\n- \\(\\lnot p\\) is just the flip of \\(p\\).\n- \\(p \\to q\\) is only F in the row \\((p,q)=(T,F)\\) (promise broken).\n- \\(\\lnot p \\lor q\\) is true when either \\(\\lnot p\\) is T or \\(q\\) is T.\n\nStep 3: Compare the last two columns.\nThe columns for \\(p \\to q\\) and \\(\\lnot p \\lor q\\) are identical (T, F, T, T).\nTherefore, the statements have the same truth value in every case.\n\nStep 4: Conclude.\nWe conclude:\n\\(p \\to q \\equiv \\lnot p \\lor q\\)\nIn words: an implication “if \\(p\\), then \\(q\\)” always carries the same meaning as saying “either \\(p\\) does not happen, or \\(q\\) happens.”\n\n\nExercise 2. Analyze the statement: “If a number is divisible by 4, then it is even.”\nStep 1. Define \\(p\\): “number is divisible by 4”, and \\(q\\): “number is even.”\nStep 2. Translate into logic: \\(p \\to q\\).\nStep 3. Think through cases:\n- A number divisible by 4 (say 12) → it is even → promise kept.\n- A number divisible by 4 but not even → impossible case → promise broken.\n- A number not divisible by 4 (say 9) → nothing promised, implication vacuously true.\nStep 4. Verify with a truth table if needed.\nStep 5. Conclude: the statement holds logically.\n\nBiconditional: \\(p \\leftrightarrow q\\) (“\\(p\\) if and only if \\(q\\)”)\n\nRule: \\(p \\leftrightarrow q\\) is true exactly when \\(p\\) and \\(q\\) have the same truth value\n(both true or both false).\nEquivalent form:\n\\[\np \\leftrightarrow q \\;\\equiv\\; (p \\to q) \\land (q \\to p)\n\\]\nExample:\n\\(p\\): “Today is Saturday.”\n\\(q\\): “Tomorrow is Sunday.”\n\\(p \\leftrightarrow q\\): “Today is Saturday if and only if tomorrow is Sunday.”\nCase analysis:\n\nIf both \\(p\\) and \\(q\\) are true → the biconditional is true (both directions of the promise hold).\n\nIf \\(p\\) is true but \\(q\\) is false → false, because one direction of the “if and only if” fails.\n\nIf \\(p\\) is false but \\(q\\) is true → false, for the same reason.\n\nIf both \\(p\\) and \\(q\\) are false → true, because they match in value (both false).\n\n\nThis explains why the truth table looks like this:\n\n\n\n\\(p\\)\n\\(q\\)\n\\(p \\leftrightarrow q\\)\nExplanation\n\n\n\n\nT\nT\nT\nboth true → promise kept\n\n\nT\nF\nF\nmismatch → one direction fails\n\n\nF\nT\nF\nmismatch → one direction fails\n\n\nF\nF\nT\nboth false → they match\n\n\n\nIntuition and goal of the biconditional\nThe biconditional expresses equivalence: \\(p\\) and \\(q\\) “stand or fall together.”\nIt is stronger than a one-way implication: both \\(p \\to q\\) and \\(q \\to p\\) must hold.\n\nIf you read \\(p \\leftrightarrow q\\) aloud, it means:\n“\\(p\\) is true exactly when \\(q\\) is true.” or “\\(p\\) holds exactly when \\(q\\) holds.”\n\nThis is why mathematicians often use “iff” (“if and only if”) in definitions and theorems:\n- It guarantees not only that \\(p\\) implies \\(q\\), but also that \\(q\\) implies \\(p\\).\n\nWhy is this important?\n\nIt formalizes definitions in mathematics.\n\nExample: “A number \\(n\\) is even iff \\(n = 2k\\) for some integer \\(k\\).”\n\nThis captures both directions: every even number has that form, and every number of that form is even.\n\nIt allows us to state equivalence theorems.\n\nExample: “A sequence is Cauchy iff it is convergent (in \\(\\mathbb{R}\\)).”\n\nThe biconditional captures the deep connection: each property implies the other.\n\nIt makes reasoning reversible.\n\nWith an implication, you can only go forward (\\(p \\to q\\)).\n\nWith a biconditional, you can go forward and backward: knowing either \\(p\\) or \\(q\\) tells you the other.\n\n\nBy mastering the biconditional, the reader understands why mathematicians love the phrase “if and only if”: it’s the precise way of stating true equivalence between concepts.\nSummary Table of Connectives\n\n\n\n\n\n\n\n\nConnective\nSymbol\nRule (when true)\n\n\n\n\nNegation\n\\(\\lnot p\\)\nwhen \\(p\\) is false\n\n\nConjunction\n\\(p \\land q\\)\nwhen \\(p\\) and \\(q\\) are true\n\n\nDisjunction\n\\(p \\lor q\\)\nwhen at least one of \\(p, q\\) is true\n\n\nConditional\n\\(p \\to q\\)\nfalse only if \\(p\\) true and \\(q\\) false\n\n\nBiconditional\n\\(p \\leftrightarrow q\\)\nwhen \\(p\\) and \\(q\\) have same truth value",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html#tautologies-contradictions-and-logical-equivalence",
    "href": "chapters/01_ch01.html#tautologies-contradictions-and-logical-equivalence",
    "title": "Phase 1: Logic & Set Theory",
    "section": "1.3 Tautologies, Contradictions, and Logical Equivalence",
    "text": "1.3 Tautologies, Contradictions, and Logical Equivalence\nTautology\nDefinition:\nA tautology is a statement that is true in all possible cases.\nWhy it matters:\n- Tautologies act like universal truths: they don’t depend on data or assumptions.\n- They are often the “glue” of proofs, showing that certain forms are always valid.\n- Many rules of inference (like modus ponens) are based on tautologies.\nExample (logic):\n\\[\n(p \\land q) \\to p\n\\]\nThis means: If both (p) and (q) are true, then (p) is true.\n- Always true, regardless of whether (p) or (q) are true or false.\nExample (statistics):\nThe Law of Total Probability is tautological:\n\\[\nP(A) = P(A \\cap B) + P(A \\cap \\lnot B).\n\\]\nThis identity always holds by construction, no matter what events (A) and (B) are.\n\nContradiction\nDefinition:\nA contradiction is a statement that is false in all possible cases.\nWhy it matters:\n- Contradictions are the engine of proof by contradiction.\n- If assuming something leads to a contradiction, then the assumption must be false.\n- They represent “impossible situations” in logic.\nExample (logic):\n\\[\np \\land \\lnot p\n\\]\nThis means: (p) is true and (p) is false at the same time.\n- Always false, no matter what truth value (p) has.\nExample (statistics):\nSuppose we assume:\n1. “The variance of this distribution is finite.”\n2. “The variance of this distribution is infinite.”\nTogether, these form a contradiction, so at least one assumption must be wrong.\n\nLogical Equivalence\nDefinition:\nTwo statements are logically equivalent if they have the same truth value in all possible cases.\nWhy it matters:\n- Logical equivalence lets us replace one statement with another in a proof.\n- Many powerful proof strategies rely on equivalence (contrapositive law, De Morgan’s laws, distributive laws).\n- Often the equivalent form is much easier to work with.\nExample (logic):\n\\[\np \\to q \\;\\equiv\\; \\lnot p \\lor q\n\\]\nThis means: “If (p), then (q)” is the same as “Either not (p), or (q).”\n- This equivalence makes it easier to manipulate conditionals in proofs.\nExample (causal inference):\n\\[\n\\text{Ignorability} \\to \\text{Identifiability}\n\\]\nis logically equivalent to\n\\[\n\\lnot \\text{Identifiability} \\to \\lnot \\text{Ignorability}.\n\\]\nSwitching to the contrapositive often makes a proof or argument simpler.\n\nSummary:\n- Tautologies give us universal truths to rely on.\n- Contradictions allow us to eliminate false assumptions through contradiction proofs.\n- Logical equivalence lets us restate problems in easier forms without changing meaning.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html#predicates-and-quantifiers",
    "href": "chapters/01_ch01.html#predicates-and-quantifiers",
    "title": "Phase 1: Logic & Set Theory",
    "section": "2.1 Predicates and Quantifiers",
    "text": "2.1 Predicates and Quantifiers\nAs we saw above, a predicate is like a sentence with a “blank” — it becomes a full statement only once you plug in a value.\n\nExample: \\(P(x): x &gt; 0\\).\n\nIf \\(x = 2\\), then \\(P(2)\\) is the proposition “2 &gt; 0” (true).\n\nIf \\(x = -3\\), then \\(P(-3)\\) is the proposition “-3 &gt; 0” (false).\n\n\nWe use quantifiers to talk about how many elements satisfy a predicate:\n\nUniversal quantifier (\\(\\forall\\)):\n\\(\\forall x\\; P(x)\\) means “for all \\(x\\), \\(P(x)\\) is true.”\nExistential quantifier (\\(\\exists\\)):\n\\(\\exists x\\; P(x)\\) means “there exists at least one \\(x\\) such that \\(P(x)\\) is true.”\n\nExamples:\n- \\(\\forall x \\in \\mathbb{Z},\\; x^2 \\geq 0\\). (Every integer squared is nonnegative.)\n- \\(\\exists x \\in \\mathbb{Z},\\; x^2 = 9\\). (There exists an integer whose square is 9.)",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html#universe-of-discourse",
    "href": "chapters/01_ch01.html#universe-of-discourse",
    "title": "Phase 1: Logic & Set Theory",
    "section": "2.2 Universe of Discourse",
    "text": "2.2 Universe of Discourse\nThe universe of discourse is the set of objects we allow \\(x\\) to vary over.\nThe truth of a statement depends on it!\nExample:\n\n\\(\\forall x \\in \\mathbb{R},\\; x^2 \\geq 0\\) is true.\n\n\\(\\forall x \\in \\mathbb{Z},\\; x^2 = 2\\) is false (no integer squared equals 2).\n\nIf we didn’t specify whether \\(x\\) ranges over \\(\\mathbb{R}\\) or \\(\\mathbb{Z}\\) (both interpreted as the universe of discourse of each statement), the meaning would be ambiguous.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html#truth-of-quantified-statements",
    "href": "chapters/01_ch01.html#truth-of-quantified-statements",
    "title": "Phase 1: Logic & Set Theory",
    "section": "2.3 Truth of Quantified Statements",
    "text": "2.3 Truth of Quantified Statements\nHow to evaluate quantified statements:\n\n\\(\\forall x\\; P(x)\\) is true if every \\(x\\) in the universe makes \\(P(x)\\) true.\n\n\\(\\exists x\\; P(x)\\) is true if at least one \\(x\\) makes \\(P(x)\\) true.\n\nNegations of Quantifiers\nNegating quantified statements flips the quantifier:\n\\[\n\\lnot (\\forall x\\, P(x)) \\equiv \\exists x\\, \\lnot P(x)\n\\]\n\\[\n\\lnot (\\exists x\\, P(x)) \\equiv \\forall x\\, \\lnot P(x)\n\\]\nExamples:\n\n“Not all students passed” means “There exists a student who did not pass.”\n\n“There does not exist a unicorn” means “For all \\(x\\), \\(x\\) is not a unicorn.”",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html#multiple-quantifiers",
    "href": "chapters/01_ch01.html#multiple-quantifiers",
    "title": "Phase 1: Logic & Set Theory",
    "section": "2.4 Multiple Quantifiers",
    "text": "2.4 Multiple Quantifiers\nOften statements involve more than one quantifier.\nThe order matters!\n\n\\(\\forall x \\in \\mathbb{R},\\; \\exists y \\in \\mathbb{R}: y &gt; x\\)\n→ True, because for every real number \\(x\\), we can pick \\(y = x+1\\).\n\\(\\exists y \\in \\mathbb{R},\\; \\forall x \\in \\mathbb{R}: y &gt; x\\)\n→ False, because no single real number is greater than all real numbers.\n\nTip: Think of quantifiers as a kind of game:\n- For \\(\\forall x\\), your opponent chooses the worst possible \\(x\\).\n- For \\(\\exists y\\), you get to respond by picking a suitable \\(y\\).\nThe order decides who gets to “move” first, and the outcome can change completely.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html#why-this-matters",
    "href": "chapters/01_ch01.html#why-this-matters",
    "title": "Phase 1: Logic & Set Theory",
    "section": "2.5 Why This Matters",
    "text": "2.5 Why This Matters\nQuantifiers appear in almost every mathematical theorem.\n\nAnalysis (limits):\n\\[\n\\forall \\epsilon &gt; 0,\\; \\exists \\delta &gt; 0:\\; |x - a| &lt; \\delta \\;\\to\\; |f(x) - L| &lt; \\epsilon\n\\]\n(“For every tolerance \\(\\epsilon\\), there exists a closeness \\(\\delta\\) that guarantees the function stays within that tolerance.”)\nStatistics:\n\n\\(\\forall n,\\; \\exists \\hat{\\theta}_n:\\; \\hat{\\theta}_n \\to \\theta\\) (There exists an estimator consistent for \\(\\theta\\).)\n\n\\(\\exists\\) an unbiased estimator of \\(\\mu\\) (the sample mean).\n\nCausal Inference:\n\n\\(\\forall\\) randomized experiments, \\(\\exists\\) an unbiased estimator of the treatment effect.\n\n\nQuantifiers are the way mathematics formalizes sweeping claims like “always” and “sometimes,” which are at the heart of proofs and assumptions in Causal ML.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html#mathematical-interpretation",
    "href": "chapters/01_ch01.html#mathematical-interpretation",
    "title": "Phase 1: Logic & Set Theory",
    "section": "2.6 Mathematical Interpretation",
    "text": "2.6 Mathematical Interpretation\nQuantifiers can look intimidating at first, but the real skill is learning how to read them.\nEvery quantified statement has two parts:\n1. the quantifier (\\(\\forall\\) or \\(\\exists\\)), and\n2. the predicate (a property of \\(x\\) that is claimed to hold).\n\n2.6.1 Single Quantifier Examples\n\nUniversal (\\(\\forall\\)):\n\\[\n\\forall x \\in \\mathbb{R},\\; x^2 \\geq 0\n\\]\nRead: “For every real number \\(x\\), the square of \\(x\\) is nonnegative.”\nInterpretation: This is true, because no real number squared gives a negative result.\nStatistics example:\n\\[\n\\forall n \\in \\mathbb{N},\\; \\operatorname{Var}(\\bar{X}_n) \\geq 0\n\\]\nRead: “For every sample size \\(n\\), the variance of the sample mean is nonnegative.”\nInterpretation: Always true, because variances can never be negative.\n\n\n\nExistential (\\(\\exists\\)):\n\\[\n\\exists x \\in \\mathbb{Z},\\; x^2 = 4\n\\]\nRead: “There exists an integer whose square is 4.”\nInterpretation: True, since \\(x = 2\\) and \\(x = -2\\) work.\nStatistics example:\n\\[\n\\exists \\;\\text{an estimator } \\hat{\\theta}\\; : \\; \\mathbb{E}[\\hat{\\theta}] = \\theta\n\\]\nRead: “There exists an estimator whose expected value equals the true parameter.”\nInterpretation: This is the definition of an unbiased estimator (e.g., the sample mean for \\(\\mu\\)).\n\n\n2.6.2 Multiple Quantifier Examples\nWhen quantifiers are combined, order matters.\n\nExample 1:\n\\[\n\\forall x \\in \\mathbb{R},\\; \\exists y \\in \\mathbb{R}:\\; y &gt; x\n\\]\nRead: “For every real number \\(x\\), there exists a real number \\(y\\) that is greater than \\(x\\).”\nTrue, because if someone hands you any \\(x\\), you can always respond with \\(y = x+1\\).\nStatistics example:\n\\[\n\\forall \\epsilon &gt; 0,\\; \\exists N \\in \\mathbb{N}:\\; n &gt; N \\;\\to\\; |\\bar{X}_n - \\mu| &lt; \\epsilon\n\\]\nRead: “For every tolerance \\(\\epsilon\\), there exists a large enough sample size \\(N\\) such that if \\(n &gt; N\\), the sample mean is within \\(\\epsilon\\) of \\(\\mu\\).”\nInterpretation: This is the definition of consistency (Law of Large Numbers).\n\n\n\nExample 2:\n\\[\n\\exists y \\in \\mathbb{R},\\; \\forall x \\in \\mathbb{R}:\\; y &gt; x\n\\]\nRead: “There exists a real number \\(y\\) such that \\(y\\) is greater than every real number \\(x\\).”\nFalse, because no single real number is larger than all others.\nStatistics example (false statement):\n\\[\n\\exists N \\in \\mathbb{N},\\; \\forall n &gt; N:\\; \\bar{X}_n = \\mu\n\\]\nRead: “There exists a finite sample size \\(N\\) such that for all \\(n &gt; N\\), the sample mean equals the population mean exactly.”\nFalse, because sampling variation never completely disappears — the sample mean only converges in probability, not with exact equality at some \\(N\\).\n\n\n2.6.3 How to Think About Multiple Quantifiers\nA useful way to think is as a game:\n\n\\(\\forall x\\) = your opponent picks a value of \\(x\\), possibly trying to make you fail.\n\n\\(\\exists y\\) = you get to respond by picking \\(y\\) to satisfy the condition.\n\nSo the statement\n\\[\n\\forall x \\in \\mathbb{R},\\; \\exists y \\in \\mathbb{R}:\\; y &gt; x\n\\]\nmeans: No matter what \\(x\\) your opponent picks, you can always respond with a suitable \\(y\\).\nBut the reverse order\n\\[\n\\exists y \\in \\mathbb{R},\\; \\forall x \\in \\mathbb{R}:\\; y &gt; x\n\\]\nmeans: You must pick one \\(y\\) that beats all possible \\(x\\). This is impossible, so the statement is false.\n\nWhy this section is important\nQuantifiers are everywhere in math, stats, and causal ML.\n\nUniversal quantifiers express generality:\n\n“For all sample sizes \\(n\\), \\(\\operatorname{Var}(\\bar{X}_n) \\geq 0\\).”\n\n“For all \\(\\epsilon &gt; 0\\), there exists an \\(N\\) such that …” (limits, consistency).\n\nExistential quantifiers express possibility:\n\n“There exists an unbiased estimator of \\(\\mu\\).”\n\n“There exists a consistent estimator for every parameter.”\n\nWith multiple quantifiers, the order of ‘who chooses first’ changes the meaning dramatically.\nThis interpretative skill is essential for reading theorems correctly and avoiding misinterpretation.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html#exercises-1",
    "href": "chapters/01_ch01.html#exercises-1",
    "title": "Phase 1: Logic & Set Theory",
    "section": "2.7 Exercises",
    "text": "2.7 Exercises\nGoal of these exercises:\n- Practice evaluating truth values.\n- Practice negating quantified statements.\n- Practice reading and interpreting symbolic logic in plain English.\nAt this stage, we are not proving statements — only learning to understand and translate them correctly.\n\nTruth values (universe of discourse: \\(\\mathbb{Z}\\)):\nFor each statement, decide whether it is true or false and explain why in words.\n\n\\(\\forall x,\\; x^2 \\geq 0\\)\n\n\\(\\exists x,\\; x^2 = 2\\)\n\nHint: In the first, think: “Is there any integer whose square is negative?”\nIn the second, think: “Is there an integer whose square equals 2?”\n\n\n\nNegation practice:\nWrite the logical negation of each statement and simplify.\n\n\\(\\forall x \\in \\mathbb{R},\\; x^2 \\geq 0\\)\n\n\\(\\exists x \\in \\mathbb{N},\\; x^2 = 2\\)\n\nHint: Use the rules:\n\\[\n\\lnot (\\forall x\\, P(x)) \\equiv \\exists x\\, \\lnot P(x), \\qquad\n\\lnot (\\exists x\\, P(x)) \\equiv \\forall x\\, \\lnot P(x).\n\\]\n\n\n\nQuantifier order:\nCarefully interpret the following statements in plain English.\nAre they true or false?\n\n\\(\\forall x \\in \\mathbb{R},\\; \\exists y \\in \\mathbb{R}: y &gt; x\\)\n\n\\(\\exists y \\in \\mathbb{R},\\; \\forall x \\in \\mathbb{R}: y &gt; x\\)\n\n\n\n\nTranslate into symbols:\nExpress the following in logical notation.\n\n“Every dataset has at least one outlier.”\n\n“There exists a consistent estimator for every parameter.”\n\n\n\n\nInterpret the following statistical statements (no proof needed):\n\n\\(\\forall n \\in \\mathbb{N},\\; \\operatorname{Var}(\\bar{X}_n) \\geq 0\\)\n(For every sample size \\(n\\), the variance of the sample mean is nonnegative.)\n\\(\\exists n \\in \\mathbb{N},\\; \\forall \\epsilon &gt; 0:\\; |\\bar{X}_n - \\mu| &lt; \\epsilon\\)\n(There exists a fixed sample size \\(n\\) such that the sample mean is always arbitrarily close to \\(\\mu\\). Is this realistic?)\n\\(\\forall \\epsilon &gt; 0,\\; \\exists N \\in \\mathbb{N}:\\; n &gt; N \\;\\to\\; |\\bar{X}_n - \\mu| &lt; \\epsilon\\)\n(Interpretation: This is the formal definition of consistency / the Law of Large Numbers).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  },
  {
    "objectID": "chapters/01_ch01.html#references",
    "href": "chapters/01_ch01.html#references",
    "title": "Phase 1: Logic & Set Theory",
    "section": "References",
    "text": "References\n\nVelleman, D. J. (2006). How to Prove It: A Structured Approach.\n\nRosen, K. H. (2011). Discrete Mathematics and Its Applications.\n\nSpanos, A. (1999, 2010). Probability Theory and Statistical Inference.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Phase 1: Logic & Set Theory</span>"
    ]
  }
]